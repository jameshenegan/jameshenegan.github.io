<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1 id="summary-of-kaggle-courses">Summary of Kaggle Courses</h1>
<h2 id="intro-to-machine-learning">Intro to Machine Learning</h2>
<h3 id="section-1-how-models-work">Section 1: How Models Work</h3>
<p>The introduction provides an overview of how machine learning models
work, focusing on decision trees as an entry point. It begins by
explaining the scenario in which you’ll use machine learning to predict
house prices. Your cousin, who has made a fortune in real estate using
intuition, partners with you to leverage your data science skills. The
idea is to use machine learning, specifically decision trees, to predict
housing prices based on patterns in data, similar to how your cousin has
used patterns in the past.</p>
<p>A <strong>decision tree</strong> is a simple model that splits data
into two categories and predicts the price of a house based on
historical averages. The process of identifying these patterns in data
is called “fitting” or “training” the model, and the data used is called
“training data.”</p>
<p>The introduction explains that deeper decision trees, which include
more factors such as the number of bedrooms, bathrooms, and lot size,
lead to better predictions. The depth of the tree refers to the number
of splits, with each split capturing additional factors that affect
house prices. The prediction process involves tracing through the tree
according to a house’s characteristics, with the final prediction
happening at the tree’s leaf, which is the point where a decision is
made based on the data.</p>
<p>The course aims to guide you through working with this type of model,
using real data to improve predictions.</p>
<h3 id="section-2-basic-data-exploration">Section 2: Basic Data
Exploration</h3>
<p>The section provides an introduction to using the
<strong>Pandas</strong> library to explore and familiarize yourself with
data, which is the first step in any machine learning project.
<strong>Pandas</strong> is a key tool used by data scientists to handle
and manipulate tabular data (like an Excel sheet or SQL table), and its
primary data structure is the <strong>DataFrame</strong>.</p>
<p>The section explains how to load and inspect a dataset using Pandas.
In the example, housing data from Melbourne, Australia, is loaded using
the <code>pd.read_csv()</code> function, which reads the data into a
DataFrame. The <strong>describe()</strong> function is used to generate
summary statistics of the data, which includes key metrics such as
<strong>count</strong>, <strong>mean</strong>, <strong>standard
deviation (std)</strong>, <strong>minimum (min)</strong>, and percentile
values like the <strong>25th, 50th, and 75th percentiles</strong>, along
with the <strong>maximum (max)</strong> value for each column.</p>
<p>These statistics help understand the dataset by showing how spread
out the values are and providing insight into possible missing data,
like in the case of homes missing details for certain features (e.g.,
second bedroom size for one-bedroom homes). Key metrics like
<strong>mean</strong> (average) and <strong>standard deviation</strong>
(variation in data) give a sense of the dataset’s central tendencies and
variability. The percentiles allow you to gauge how data is distributed,
with the <strong>min</strong> and <strong>max</strong> marking the
extremes.</p>
<p>This process is essential for understanding the data structure and
identifying any missing or unusual values before building machine
learning models.</p>
<h3 id="section-3-your-first-machine-learning-model">Section 3: Your
First Machine Learning Model</h3>
<p>This section outlines the process of selecting data for building
machine learning models using the <strong>Pandas</strong> and
<strong>scikit-learn</strong> libraries. Here’s a summary of the key
points:</p>
<ol type="1">
<li><p><strong>Familiarizing with Data</strong>: Start by reviewing the
dataset’s columns using Pandas. In this case, Melbourne housing data is
explored. Since some data has missing values, these are removed using
the <code>dropna()</code> function for simplicity.</p></li>
<li><p><strong>Selecting Prediction Target</strong>: The
<strong>prediction target</strong> is the variable you want to predict,
which is stored as a Series using dot notation. In this example, house
prices (<code>Price</code>) are selected as the prediction target and
stored in the variable <code>y</code>.</p></li>
<li><p><strong>Choosing Features</strong>: Features are the columns used
to make predictions. A subset of features (e.g., <code>Rooms</code>,
<code>Bathroom</code>, <code>Landsize</code>, <code>Lattitude</code>,
and <code>Longtitude</code>) is selected using a list of column names.
The selected features are stored in a DataFrame <code>X</code>, which is
used as input for the model.</p></li>
<li><p><strong>Building the Model</strong>: Using the
<strong>scikit-learn</strong> library, a decision tree model is defined
and trained (fitted) with the selected features (<code>X</code>) and the
target variable (<code>y</code>). The model captures patterns in the
data to make predictions.</p></li>
<li><p><strong>Making Predictions</strong>: After fitting the model, it
can be used to predict prices for new data. In this case, predictions
are made for the first five rows of the training data using the
<code>predict()</code> function, and the predicted prices are
displayed.</p></li>
<li><p><strong>Model Evaluation</strong>: Though not discussed in detail
here, evaluating the model’s accuracy is a crucial step after making
predictions. The model in this example is fitted with a specified
<code>random_state</code> to ensure reproducible results.</p></li>
</ol>
<p>This process forms the basic workflow of selecting features, building
a model, and making predictions in machine learning.</p>
<h3 id="section-4-model-validation">Section 4: Model Validation</h3>
<p>This lesson introduces the concept of <strong>model
validation</strong>, which is essential for assessing how good a machine
learning model is. It emphasizes that model quality is typically
measured by <strong>predictive accuracy</strong>—how close the model’s
predictions are to actual outcomes.</p>
<h3 id="key-points">Key Points:</h3>
<ol type="1">
<li><p><strong>Model Validation</strong>: To evaluate the quality of a
model, you must measure how well it predicts new, unseen data, rather
than testing it on the data it was trained with. Many make the mistake
of evaluating models using the same data they trained on, which can lead
to over-optimistic results.</p></li>
<li><p><strong>Mean Absolute Error (MAE)</strong>: A common metric for
summarizing model accuracy. The <strong>error</strong> is the difference
between the actual and predicted values. The MAE takes the absolute
value of these errors and calculates the average, summarizing the
model’s quality in a single number (e.g., “on average, our predictions
are off by X”).</p></li>
<li><p><strong>In-Sample vs. Out-of-Sample</strong>: If you measure
accuracy using the same data used to build the model
(<strong>in-sample</strong>), you risk overfitting. This occurs when the
model captures random patterns or anomalies specific to the training
data that don’t generalize to new data. For instance, the model may
learn irrelevant patterns (like homes with green doors being more
expensive), which leads to poor performance on unseen data.</p></li>
<li><p><strong>Validation Data</strong>: To avoid overfitting, you need
to test the model on data it hasn’t seen before. This is done by
splitting the dataset into <strong>training data</strong> (used to build
the model) and <strong>validation data</strong> (used to evaluate the
model).</p></li>
<li><p><strong>Using <code>train_test_split</code></strong>: In
practice, the data is split into two sets using
<strong>scikit-learn’s</strong> <code>train_test_split</code> function.
The model is then trained on one part and evaluated on the other. The
example provided shows a drastic difference between the in-sample error
(approximately $500) and out-of-sample error (approximately $250,000),
demonstrating how misleading in-sample scores can be.</p></li>
<li><p><strong>Improvement</strong>: The lesson concludes by
highlighting that the model can be improved by refining features,
choosing better models, or experimenting with different
approaches.</p></li>
</ol>
<p>This lesson underscores the importance of testing models on unseen
data to ensure their accuracy and usefulness in real-world
scenarios.</p>
<h3 id="section-5-underfitting-and-overfitting">Section 5: Underfitting
and Overfitting</h3>
<p>This section introduces the concepts of <strong>underfitting</strong>
and <strong>overfitting</strong> in machine learning models,
specifically using decision trees, and explains how to balance these two
to improve model accuracy.</p>
<h4 id="key-points-1">Key Points:</h4>
<ol type="1">
<li><p><strong>Experimenting with Models</strong>: Now that we can
measure model accuracy, we can experiment with different models. For
decision trees, the most important factor influencing performance is the
tree’s <strong>depth</strong>, which is controlled by the number of
splits (leaves) it makes.</p></li>
<li><p><strong>Tree Depth and Overfitting/Underfitting</strong>:</p>
<ul>
<li><strong>Overfitting</strong> happens when a model is too complex,
meaning it fits the training data almost perfectly by learning specific
patterns, even noise. However, it performs poorly on new, unseen
data.</li>
<li><strong>Underfitting</strong> occurs when the model is too simple
and doesn’t capture important distinctions in the data. It performs
poorly on both training and validation data.</li>
<li>Finding the “sweet spot” between these extremes is key for model
accuracy, particularly on new data.</li>
</ul></li>
<li><p><strong>Controlling Overfitting with Max Leaf Nodes</strong>: One
way to control the tree’s complexity is by limiting the number of
<strong>leaf nodes</strong> (the endpoints of the tree where predictions
are made). The <code>max_leaf_nodes</code> parameter in a decision tree
helps manage overfitting by reducing the number of splits.</p></li>
<li><p><strong>Evaluating Models with MAE</strong>: By using the
<strong>Mean Absolute Error (MAE)</strong> to evaluate models with
different values of <code>max_leaf_nodes</code>, you can find the
optimal complexity for your model. A loop is used to test different
values (e.g., 5, 50, 500, and 5000 leaf nodes) and calculate MAE for
each.</p>
<ul>
<li>In the example provided, <strong>500 leaf nodes</strong> yielded the
best MAE score, balancing between underfitting and overfitting.</li>
</ul></li>
<li><p><strong>Conclusion</strong>:</p>
<ul>
<li><strong>Overfitting</strong>: The model captures patterns that are
too specific to the training data, causing poor performance on new
data.</li>
<li><strong>Underfitting</strong>: The model fails to capture key
patterns in the data, leading to poor performance even on training
data.</li>
<li><strong>Validation data</strong> helps measure the accuracy of
models on unseen data, allowing you to select the best model by testing
multiple configurations.</li>
</ul></li>
</ol>
<p>In summary, understanding and controlling overfitting and
underfitting is crucial for building accurate models, and validation
data is essential for assessing model performance.</p>
<h3 id="section-6-random-forests">Section 6: Random Forests</h3>
<p>This introduction explains the trade-off between
<strong>overfitting</strong> and <strong>underfitting</strong> in
decision trees, which often makes it difficult to balance model
complexity and accuracy. A <strong>deep decision tree</strong> may
overfit the training data by relying too heavily on small subsets of
data, while a <strong>shallow tree</strong> might underfit by not
capturing enough of the data’s nuances.</p>
<p>The section introduces the <strong>random forest</strong> model as a
solution to this problem. A random forest is a collection of multiple
decision trees that make predictions by averaging the outputs of
individual trees. This approach improves predictive accuracy and reduces
the risk of overfitting compared to a single decision tree. It performs
well even with default parameters, making it a robust choice for many
applications.</p>
<h4 id="example">Example:</h4>
<p>Using <strong>scikit-learn</strong>, a random forest model is created
with the <code>RandomForestRegressor</code> class. The random forest
model, applied to the same training and validation data, produced a
significantly better <strong>Mean Absolute Error (MAE)</strong> of
around <strong>191,670</strong>, compared to the best decision tree MAE
of 250,000.</p>
<h4 id="conclusion">Conclusion:</h4>
<p>While the random forest’s performance could be further optimized by
tuning parameters, one of its key advantages is that it often works well
without needing much fine-tuning.</p>
<h2 id="intermediate-machine-learning">Intermediate Machine
Learning</h2>
<h3 id="section-1-introduction">Section 1: Introduction</h3>
<p>This introduction to Kaggle’s <strong>Intermediate Machine
Learning</strong> course outlines what participants will learn and the
skills they will develop. The course is designed for those with some
background in machine learning who want to improve the quality of their
models quickly.</p>
<h4 id="key-topics-covered">Key Topics Covered:</h4>
<ol type="1">
<li><strong>Handling real-world data issues</strong> like missing values
and categorical variables.</li>
<li><strong>Designing pipelines</strong> to write cleaner and more
efficient machine learning code.</li>
<li><strong>Advanced model validation</strong> techniques, such as
cross-validation.</li>
<li><strong>Building advanced models</strong>, including XGBoost, a
popular tool for Kaggle competitions.</li>
<li><strong>Avoiding common data science pitfalls</strong>, such as data
leakage.</li>
</ol>
<p>The course includes <strong>hands-on exercises</strong> using data
from the Housing Prices Competition, where learners will work with 79
variables to predict home prices. Learners can track their progress by
submitting predictions to a competition leaderboard.</p>
<h4 id="prerequisites">Prerequisites:</h4>
<p>Participants should have prior experience building machine learning
models and understanding concepts like model validation, overfitting,
underfitting, and random forests. Those new to machine learning are
encouraged to complete the Intro to Machine Learning course first.</p>
<h3 id="section-2-missing-values">Section 2: Missing Values</h3>
<p>This tutorial introduces three methods for handling missing values in
datasets and compares their effectiveness using a real-world dataset.
Here’s a summary:</p>
<h4 id="introduction">Introduction:</h4>
<p>Missing values are common in datasets, and machine learning libraries
like scikit-learn require these values to be handled before building
models. Three common approaches to managing missing data are
outlined:</p>
<h4 id="three-approaches">Three Approaches:</h4>
<ol type="1">
<li><p><strong>Drop Columns with Missing Values</strong>: The simplest
solution is to drop columns with missing values. However, this can lead
to a significant loss of potentially useful information, especially if
only a few values are missing in key columns.</p></li>
<li><p><strong>Imputation</strong>: This approach fills missing values
with a statistic like the mean. Though imputed values may not be exact,
imputation typically leads to more accurate models compared to dropping
columns entirely.</p></li>
<li><p><strong>Extended Imputation</strong>: This method builds on
standard imputation by also creating a new column indicating which
values were originally missing. This approach can sometimes improve
results by capturing unique patterns related to the missing
data.</p></li>
</ol>
<h4 id="example-1">Example:</h4>
<p>The tutorial uses the <strong>Melbourne Housing</strong> dataset to
test these approaches. A function (<code>score_dataset()</code>)
calculates the model’s performance using <strong>Mean Absolute Error
(MAE)</strong>.</p>
<h3 id="results">Results:</h3>
<ul>
<li><strong>Approach 1 (Drop Columns)</strong>: MAE = 183,550</li>
<li><strong>Approach 2 (Imputation)</strong>: MAE = 178,166</li>
<li><strong>Approach 3 (Extended Imputation)</strong>: MAE =
178,927</li>
</ul>
<h4 id="conclusion-1">Conclusion:</h4>
<ul>
<li><strong>Imputation</strong> (Approach 2) performed better than
simply dropping columns with missing values (Approach 1).</li>
<li>The <strong>Extended Imputation</strong> (Approach 3) did not offer
significant improvement over standard imputation in this case.</li>
<li>Imputation worked better because dropping columns removed a lot of
useful information, especially since only a few entries were missing in
certain columns.</li>
</ul>
<h3 id="section-3-categorical-variables">Section 3: Categorical
Variables</h3>
<p>This tutorial explains <strong>categorical variables</strong> and
presents three approaches for handling them in machine learning models.
Categorical variables take a limited number of values, like survey
responses or car brands, and they must be preprocessed before being used
in models.</p>
<h4 id="three-approaches-1">Three Approaches:</h4>
<ol type="1">
<li><p><strong>Drop Categorical Variables</strong>:</p>
<ul>
<li>The simplest approach is to drop columns with categorical data.
However, this often leads to loss of useful information and should only
be used if the categorical data is not important.</li>
</ul></li>
<li><p><strong>Ordinal Encoding</strong>:</p>
<ul>
<li>Assigns each unique value in a categorical variable to an integer.
This works well for ordinal variables, where there is a clear ordering
(e.g., “Never” &lt; “Rarely” &lt; “Most days” &lt; “Every day”). It’s
particularly effective for tree-based models like decision trees and
random forests.</li>
</ul></li>
<li><p><strong>One-Hot Encoding</strong>:</p>
<ul>
<li>Creates new binary columns for each category in a categorical
variable. For example, a “Color” column with values like “Red”,
“Yellow”, and “Green” would create three new columns, each indicating
the presence or absence of these values. This is useful for nominal
variables (no inherent ordering) but can become inefficient when there
are many categories.</li>
</ul></li>
</ol>
<h4 id="example-2">Example:</h4>
<p>Using the <strong>Melbourne Housing</strong> dataset, the tutorial
demonstrates these approaches, comparing them based on their
<strong>Mean Absolute Error (MAE)</strong> when used in a random forest
model.</p>
<h4 id="results-1">Results:</h4>
<ul>
<li><strong>Dropping categorical variables (Approach 1)</strong> had the
highest MAE (worst performance).</li>
<li><strong>Ordinal Encoding (Approach 2)</strong> and <strong>One-Hot
Encoding (Approach 3)</strong> performed similarly, with slightly better
results from ordinal encoding in this case.</li>
</ul>
<h4 id="conclusion-2">Conclusion:</h4>
<ul>
<li><strong>One-Hot Encoding</strong> typically works best for nominal
variables, while <strong>Ordinal Encoding</strong> is more appropriate
for ordinal variables.</li>
<li><strong>Dropping categorical variables</strong> usually results in
the worst performance. However, the best approach can vary depending on
the dataset.</li>
</ul>
<p>This tutorial highlights the importance of handling categorical data
effectively to improve model accuracy.</p>
<h3 id="section-4-pipelines">Section 4: Pipelines</h3>
<p>This tutorial introduces <strong>pipelines</strong>, a tool for
organizing data preprocessing and modeling in machine learning projects.
Pipelines bundle multiple steps—such as preprocessing and modeling—into
a single, streamlined workflow, which simplifies the process and reduces
the chance for errors.</p>
<h4 id="key-benefits-of-pipelines">Key Benefits of Pipelines:</h4>
<ul>
<li><strong>Cleaner Code</strong>: Keeps code more organized, preventing
the need to track training and validation data separately during
preprocessing.</li>
<li><strong>Fewer Bugs</strong>: Reduces the likelihood of mistakes by
automating the sequence of preprocessing steps.</li>
<li><strong>Easier to Deploy</strong>: Pipelines make transitioning from
a prototype to a production model easier.</li>
<li><strong>Better Model Validation</strong>: They allow for more
sophisticated validation methods, like cross-validation.</li>
</ul>
<h4 id="example-workflow">Example Workflow:</h4>
<ol type="1">
<li><p><strong>Preprocessing</strong>:</p>
<ul>
<li>A <strong>ColumnTransformer</strong> handles preprocessing for both
numerical and categorical data.</li>
<li>For numerical data: missing values are imputed.</li>
<li>For categorical data: missing values are imputed and one-hot
encoding is applied.</li>
</ul></li>
<li><p><strong>Model Definition</strong>:</p>
<ul>
<li>A <strong>RandomForestRegressor</strong> is used as the model in
this example.</li>
</ul></li>
<li><p><strong>Creating and Evaluating the Pipeline</strong>:</p>
<ul>
<li>A <strong>Pipeline</strong> bundles preprocessing and modeling in a
single object. This allows for seamless preprocessing during both
training and prediction.</li>
<li>The pipeline simplifies the process by automatically handling
preprocessing when fitting the model and making predictions.</li>
</ul></li>
</ol>
<h4 id="results-2">Results:</h4>
<p>The model is evaluated using <strong>Mean Absolute Error
(MAE)</strong>, and the result for this example is
<strong>160,679</strong>.</p>
<h4 id="conclusion-3">Conclusion:</h4>
<p>Pipelines make machine learning workflows cleaner, more efficient,
and less error-prone, particularly in cases requiring complex
preprocessing steps. They are essential for handling sophisticated data
pipelines and transitioning models into production.</p>
<p>This tutorial encourages you to try pipelines in future exercises to
streamline data preprocessing and improve model performance.</p>
<h3 id="section-5-cross-validation">Section 5: Cross-Validation</h3>
<p>This tutorial introduces <strong>cross-validation</strong>, a
technique for obtaining more reliable measures of model performance by
running the model on different subsets of data.</p>
<h4 id="key-concepts">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Limitations of Single Validation Sets</strong>:</p>
<ul>
<li>Typically, a dataset is split into training and validation sets
(e.g., 80% training and 20% validation). However, this approach
introduces randomness, as results may vary depending on which subset is
chosen for validation.</li>
<li>A smaller validation set reduces randomness but can compromise model
quality due to fewer rows available for training.</li>
</ul></li>
<li><p><strong>What is Cross-Validation?</strong>:</p>
<ul>
<li>Cross-validation splits the dataset into multiple subsets (or
“folds”). For example, in <strong>5-fold cross-validation</strong>, the
data is divided into 5 parts, and the model is trained on 4 parts while
using the 5th as a validation set.</li>
<li>This process is repeated for each fold, ensuring that each subset is
used as validation data once. This results in a more accurate and
comprehensive assessment of model performance since all data points are
used for both training and validation.</li>
</ul></li>
<li><p><strong>When to Use Cross-Validation</strong>:</p>
<ul>
<li><strong>Small datasets</strong>: Cross-validation is recommended for
small datasets where the computational cost is manageable, as it
provides a more reliable measure of model quality.</li>
<li><strong>Large datasets</strong>: A single validation set is often
sufficient because the size of the dataset reduces the noise from
randomness.</li>
<li>You can test cross-validation and compare the results to see if a
single validation set suffices.</li>
</ul></li>
</ol>
<h4 id="example-3">Example:</h4>
<p>A <strong>pipeline</strong> is created using a random forest model
and an imputer to handle missing values. The
<strong>cross_val_score</strong> function from scikit-learn is used to
compute the <strong>Mean Absolute Error (MAE)</strong> for each fold.
The final performance is reported as the average MAE across all
folds.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> cross_val_score(my_pipeline, X, y, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;neg_mean_absolute_error&#39;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Average MAE score (across experiments):&quot;</span>, scores.mean())</span></code></pre></div>
<ul>
<li>In the example, the average MAE across folds is
<strong>277,707</strong>, providing a more reliable estimate of model
performance than a single validation set.</li>
</ul>
<h4 id="conclusion-4">Conclusion:</h4>
<p>Cross-validation offers a more accurate and robust evaluation of
model performance, especially useful for small datasets. By using all
data points in both training and validation, it reduces the randomness
and variance in model assessment. Additionally, cross-validation
simplifies workflows by eliminating the need to manually handle separate
training and validation sets.</p>
<h3 id="section-6-xgboost">Section 6: XGBoost</h3>
<p>This tutorial introduces <strong>gradient boosting</strong>, a
powerful machine learning technique that iteratively builds an ensemble
of models to improve predictive accuracy. Specifically, it focuses on
using <strong>XGBoost</strong>, a high-performance implementation of
gradient boosting that excels in many machine learning competitions.</p>
<h4 id="key-concepts-1">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Ensemble Methods</strong>:</p>
<ul>
<li>Like <strong>random forests</strong>, gradient boosting is an
ensemble method, combining multiple models to improve predictions.
However, while random forests average predictions from multiple decision
trees, <strong>gradient boosting</strong> adds models iteratively to
correct errors made by previous models.</li>
</ul></li>
<li><p><strong>Gradient Boosting Process</strong>:</p>
<ul>
<li>The process starts with a simple model and cycles through the
following steps:
<ol type="1">
<li>Use the ensemble to make predictions.</li>
<li>Calculate a <strong>loss function</strong> (e.g., mean squared
error).</li>
<li>Fit a new model to correct the errors based on the loss
function.</li>
<li>Add the new model to the ensemble.</li>
</ol></li>
<li>This process repeats until the ensemble reaches a specified size or
stops improving.</li>
</ul></li>
</ol>
<h4 id="example-with-xgboost">Example with XGBoost:</h4>
<p>Using XGBoost in Python is similar to other scikit-learn models.
Here’s a simple model creation and evaluation example:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>my_model <span class="op">=</span> XGBRegressor()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>my_model.fit(X_train, y_train)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> my_model.predict(X_valid)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Absolute Error:&quot;</span>, mean_absolute_error(predictions, y_valid))</span></code></pre></div>
<p>In this case, the <strong>Mean Absolute Error (MAE)</strong> was
<strong>241,041</strong>, but performance can be improved through
parameter tuning.</p>
<h4 id="key-parameters-for-tuning-xgboost">Key Parameters for Tuning
XGBoost:</h4>
<ol type="1">
<li><p><strong>n_estimators</strong>:</p>
<ul>
<li>Determines how many models (or iterations) the ensemble will
include. Too few causes underfitting, while too many causes overfitting.
Typical values range from 100 to 1000.</li>
</ul></li>
<li><p><strong>early_stopping_rounds</strong>:</p>
<ul>
<li>Stops training when performance on the validation set stops
improving. It helps prevent overfitting by finding the optimal number of
trees automatically.</li>
</ul>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>my_model.fit(X_train, y_train, early_stopping_rounds<span class="op">=</span><span class="dv">5</span>, eval_set<span class="op">=</span>[(X_valid, y_valid)])</span></code></pre></div></li>
<li><p><strong>learning_rate</strong>:</p>
<ul>
<li>Controls how much each model contributes to the ensemble. A lower
learning rate requires more trees but generally leads to better
performance. Default is 0.1, but reducing it (e.g., 0.05) can increase
accuracy.</li>
</ul>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>my_model <span class="op">=</span> XGBRegressor(n_estimators<span class="op">=</span><span class="dv">1000</span>, learning_rate<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div></li>
<li><p><strong>n_jobs</strong>:</p>
<ul>
<li>Controls parallelism, speeding up training on large datasets by
utilizing multiple CPU cores.</li>
</ul></li>
</ol>
<h4 id="conclusion-5">Conclusion:</h4>
<p><strong>XGBoost</strong> is a powerful library for building models on
tabular data, offering high accuracy with proper parameter tuning. It’s
widely used in competitive machine learning and is known for its speed
and performance on large datasets.</p>
<h3 id="section-7-data-leakage">Section 7: Data Leakage</h3>
<p>This tutorial introduces <strong>data leakage</strong>, a critical
issue in machine learning that can significantly harm model performance
when it goes undetected. Leakage happens when the training data contains
information that won’t be available during real-world predictions,
leading to deceptively high model accuracy during validation but poor
performance in production.</p>
<h4 id="key-concepts-2">Key Concepts:</h4>
<ol type="1">
<li><strong>Types of Leakage</strong>:
<ul>
<li><strong>Target Leakage</strong>: Occurs when predictors include
information that will not be available at the time of prediction but is
correlated with the target. For example, using post-disease treatment
data to predict the onset of pneumonia.</li>
<li><strong>Train-Test Contamination</strong>: Happens when training
data influences validation data, often due to preprocessing or feature
engineering done before splitting the data into training and validation
sets. This contaminates the validation results, making the model appear
better than it truly is.</li>
</ul></li>
</ol>
<h4 id="example-of-target-leakage">Example of Target Leakage:</h4>
<p>The tutorial provides an example of <strong>target leakage</strong>
using a dataset of credit card applications. The variable “expenditure”
indicates the average monthly expenditure, which is only relevant after
a credit card is issued. This information creates leakage, as the model
would incorrectly associate expenditure with receiving a credit card,
leading to inflated validation accuracy.</p>
<p>To fix this, the variables “expenditure” and “share” are removed from
the dataset. After removing these predictors, the model’s accuracy drops
from <strong>98%</strong> to <strong>83%</strong>, but the new model
will generalize better to unseen data.</p>
<h4 id="prevention-of-leakage">Prevention of Leakage:</h4>
<ul>
<li><strong>Target Leakage</strong>: Ensure that no feature reflects
information that will be unavailable at prediction time. Any feature
updated after the target value is realized should be excluded.</li>
<li><strong>Train-Test Contamination</strong>: Split the data into
training and validation sets before any preprocessing steps. Pipelines
can help enforce this separation.</li>
</ul>
<h4 id="conclusion-6">Conclusion:</h4>
<p>Data leakage can lead to misleadingly high model performance and
potentially costly mistakes. Identifying and preventing leakage requires
careful data separation, thoughtful feature selection, and the use of
pipelines to manage preprocessing. Developing an understanding of the
data and its context is key to detecting and eliminating leakage
issues.</p>
<p>The tutorial encourages practicing these concepts by exploring real
datasets to recognize and avoid leakage.</p>
<h2 id="feature-engineering">Feature Engineering</h2>
<h3 id="section-1-what-is-feature-engineering">Section 1: What is
Feature Engineering?</h3>
<p>This tutorial introduces <strong>feature engineering</strong>, a
crucial step in building a great machine learning model. The course
covers several techniques for enhancing data to improve model
performance, including:</p>
<ul>
<li>Identifying important features using <strong>mutual
information</strong>.</li>
<li>Creating new features based on real-world problems.</li>
<li>Handling high-cardinality categoricals with <strong>target
encoding</strong>.</li>
<li>Using <strong>k-means clustering</strong> to generate segmentation
features.</li>
<li>Applying <strong>principal component analysis (PCA)</strong> to
decompose dataset variation.</li>
</ul>
<h4 id="key-concepts-3">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Feature Engineering Goal</strong>: The goal is to make
data more suitable for the machine learning task, improving predictive
performance, reducing computational needs, and enhancing
interpretability.</p></li>
<li><p><strong>Transformation and Models</strong>: A useful feature must
have a learnable relationship with the target. For instance, in linear
models, transforming features to linearize their relationship with the
target helps the model perform better.</p></li>
</ol>
<h4 id="example-concrete-formulations">Example: Concrete
Formulations</h4>
<p>The tutorial demonstrates feature engineering using a
<strong>Concrete Formulations</strong> dataset. Initially, a
<strong>Random Forest</strong> model is trained on un-augmented data,
yielding a <strong>baseline MAE score</strong> of
<strong>8.232</strong>.</p>
<h4 id="adding-synthetic-features">Adding Synthetic Features:</h4>
<p>The tutorial introduces <strong>three new ratio features</strong>
(e.g., Fine to Coarse Aggregate ratio), which improved the model’s MAE
score to <strong>7.948</strong>. This demonstrates that engineered
features can reveal important patterns that the model couldn’t initially
detect.</p>
<h4 id="conclusion-7">Conclusion:</h4>
<p>Feature engineering can significantly enhance model performance, and
understanding how to transform or create features is essential for
improving predictive accuracy. The course emphasizes experimentation, as
new features can either improve or degrade performance, so establishing
baselines and testing transformations is key to success.</p>
<h3 id="section-2-mutual-information">Section 2: Mutual Information</h3>
<p>This tutorial introduces the concept of <strong>mutual
information</strong> (MI), a metric for ranking features by their
usefulness in predicting a target variable. It’s especially helpful when
encountering a large dataset with many features, as it helps prioritize
which features to focus on.</p>
<h4 id="key-concepts-4">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Mutual Information (MI)</strong>:</p>
<ul>
<li>MI measures the reduction in uncertainty about the target when
knowing a feature’s value. It is more versatile than correlation because
it can detect any kind of relationship, not just linear ones.</li>
<li>MI is useful for feature selection, as it helps identify which
features have the strongest relationship with the target variable.</li>
</ul></li>
<li><p><strong>How MI Works</strong>:</p>
<ul>
<li>MI is measured in terms of “uncertainty” or “entropy.” The more a
feature reduces uncertainty about the target, the higher its MI
score.</li>
<li>MI scores range from 0 (indicating no relationship) to higher values
(indicating a stronger relationship). Values over 2.0 are rare in
practice.</li>
</ul></li>
<li><p><strong>Interpreting MI Scores</strong>:</p>
<ul>
<li>High MI scores suggest that a feature has predictive value on its
own.</li>
<li>However, MI is a <strong>univariate</strong> metric, meaning it
doesn’t account for interactions between features. A feature with a low
MI score might still be useful when combined with other features.</li>
</ul></li>
</ol>
<h4 id="example-automobile-dataset">Example: Automobile Dataset</h4>
<p>The example uses the <strong>1985 Automobiles dataset</strong> to
predict car prices based on 23 features. Features like <strong>curb
weight</strong> and <strong>highway MPG</strong> have the highest MI
scores, indicating strong relationships with the target.</p>
<h4 id="visualization">Visualization:</h4>
<ul>
<li>Data visualization complements MI by revealing additional insights
about feature relationships and potential interaction effects. For
example, <strong>fuel type</strong> has a low MI score, but visualizing
it against <strong>horsepower</strong> shows its relevance in specific
contexts.</li>
</ul>
<h4 id="conclusion-8">Conclusion:</h4>
<p>MI is a powerful tool for identifying important features, especially
in the early stages of feature selection. However, pairing MI with data
visualization and domain knowledge is crucial for a more comprehensive
understanding of feature relationships and interactions.</p>
<h3 id="section-3-creating-features">Section 3: Creating Features</h3>
<p>This tutorial provides an introduction to <strong>feature
development</strong> and common transformations you can apply to
features using Pandas to enhance your machine learning models. It
emphasizes how understanding and manipulating features can significantly
improve model performance.</p>
<h4 id="key-concepts-5">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Discovering New Features</strong>:</p>
<ul>
<li>Understand the features, refer to documentation, research the
domain, and study previous work.</li>
<li>Use data visualization to identify patterns or relationships that
can suggest transformations.</li>
</ul></li>
<li><p><strong>Mathematical Transformations</strong>:</p>
<ul>
<li>Numerical features often benefit from transformations like ratios,
powers, or logarithms to reveal underlying relationships.</li>
<li>Example: In the Automobile dataset, a <strong>stroke ratio</strong>
was calculated to represent engine efficiency.</li>
<li>Skewed distributions (e.g., <strong>WindSpeed</strong> in accidents)
can be normalized using logarithms.</li>
</ul></li>
<li><p><strong>Counting Features</strong>:</p>
<ul>
<li>For binary or boolean features, summing across multiple columns can
create new aggregate features.</li>
<li>Example: Counting <strong>roadway features</strong> near accidents
to provide an aggregated risk factor.</li>
</ul></li>
<li><p><strong>Breaking Down or Combining Features</strong>:</p>
<ul>
<li>Complex strings (like phone numbers, IDs, addresses) can be split
into simpler features using Pandas’ string methods.</li>
<li>Example: Splitting the <strong>Policy</strong> feature into
<strong>Type</strong> and <strong>Level</strong> in the Customer
Lifetime Value dataset.</li>
<li>Conversely, simple features can be combined if interactions between
them are meaningful (e.g., <strong>make_and_style</strong> combining
make and body style).</li>
</ul></li>
<li><p><strong>Group Transforms</strong>:</p>
<ul>
<li>Aggregating information across rows with group-by transformations
creates new features, such as calculating average income by state or
frequency encoding for categorical features.</li>
<li>Example: Calculating <strong>average claim amount by coverage
type</strong> and merging the result into the validation set.</li>
</ul></li>
<li><p><strong>Model-Specific Feature Considerations</strong>:</p>
<ul>
<li><strong>Linear models</strong> easily learn sums and differences but
struggle with more complex combinations like ratios.</li>
<li><strong>Tree models</strong> (like random forests and XGBoost) can
learn complex interactions but still benefit from explicit feature
combinations.</li>
<li><strong>Normalization</strong>: Important for linear models and
neural networks, though tree models benefit less from
normalization.</li>
</ul></li>
</ol>
<h4 id="conclusion-9">Conclusion:</h4>
<p>Feature engineering is an essential step in improving model
performance. Techniques such as mathematical transformations, counting,
splitting, and group transforms help reveal important relationships in
the data. Understanding your model’s strengths and weaknesses allows you
to create features that maximize predictive power.</p>
<h3 id="section-4-clustering-with-k-means">Section 4: Clustering with
k-Means</h3>
<p>This lesson introduces <strong>unsupervised learning
algorithms</strong>, specifically focusing on
<strong>clustering</strong> as a feature engineering technique.
Clustering algorithms group similar data points, and these groups (or
<strong>clusters</strong>) can be added as new features to help machine
learning models identify patterns and relationships in the data.</p>
<h4 id="key-concepts-6">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Clustering and Feature Engineering</strong>:</p>
<ul>
<li>Clustering divides data into groups of similar points, which can
help simplify complex relationships across multiple features.</li>
<li>The clusters can be added to the dataset as a categorical feature
(e.g., <strong>cluster labels</strong>) and used in models, enabling a
“divide and conquer” approach to modeling.</li>
</ul></li>
<li><p><strong>k-Means Clustering</strong>:</p>
<ul>
<li><strong>k-means</strong> is a widely-used clustering algorithm that
assigns points to clusters based on their distance to centroids.</li>
<li>You choose <strong>k</strong> (the number of clusters), and the
algorithm iteratively assigns points and adjusts the centroids until
convergence.</li>
<li>k-means creates a <strong>Voronoi tessellation</strong>, a structure
that shows how new data will be assigned to clusters.</li>
</ul></li>
<li><p><strong>Parameters in k-Means</strong>:</p>
<ul>
<li><strong>n_clusters</strong>: The number of clusters to create.</li>
<li><strong>max_iter</strong>: The maximum number of iterations for
adjusting centroids.</li>
<li><strong>n_init</strong>: How many times the algorithm runs with
different initial centroid placements to find the best clustering.</li>
</ul></li>
</ol>
<h4 id="example-california-housing">Example: California Housing</h4>
<p>The example clusters California Housing data using
<strong>Latitude</strong>, <strong>Longitude</strong>, and
<strong>Median Income</strong> to create distinct economic segments.
This clustering revealed meaningful patterns, such as geographic regions
where higher-income areas were grouped on the coast. Additionally, box
plots of median house values within each cluster showed that the
clusters separated the target variable well, indicating that the
clustering feature was informative.</p>
<h4 id="visualization-1">Visualization:</h4>
<ul>
<li><strong>Geographical clusters</strong>: A scatter plot shows how
clusters are distributed spatially.</li>
<li><strong>Cluster-target relationship</strong>: Box plots visualize
how the target variable (median house value) varies across clusters,
highlighting the usefulness of the clusters.</li>
</ul>
<h4 id="conclusion-10">Conclusion:</h4>
<p>Clustering, particularly with k-means, is a powerful technique for
feature engineering, helping simplify complex relationships by dividing
data into meaningful groups. This approach is beneficial in scenarios
where patterns based on proximity or similarity are critical, such as
geographical or market segmentation tasks. By adding cluster labels as
features, models can capture complex relationships more effectively.</p>
<h3 id="section-5-principal-component-analysis">Section 5: Principal
Component Analysis</h3>
<p>This lesson introduces <strong>Principal Component Analysis
(PCA)</strong>, a model-based feature engineering technique used to
decompose and simplify the variation in data. PCA helps discover
important relationships in the data and can be used to create more
informative features.</p>
<h4 id="key-concepts-7">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Principal Component Analysis (PCA)</strong>:</p>
<ul>
<li>PCA identifies <strong>axes of variation</strong> within data,
creating new features (principal components) that represent this
variation.</li>
<li>Instead of using the original features, PCA describes data in terms
of its <strong>principal components</strong>, which are linear
combinations of the original features.</li>
</ul></li>
<li><p><strong>How PCA Works</strong>:</p>
<ul>
<li><strong>Components</strong>: These are new features that capture the
major sources of variation in the data. Each component is a combination
of original features with different weights (called
<strong>loadings</strong>).</li>
<li><strong>Explained Variance</strong>: Each component captures a
portion of the data’s total variance. The first principal component
typically captures the most variation, while subsequent components
capture less.</li>
</ul></li>
<li><p><strong>Using PCA for Feature Engineering</strong>:</p>
<ul>
<li><strong>Descriptive Technique</strong>: PCA can help identify
important variations that are predictive of the target. For example,
discovering that the size or shape of an object is important can lead to
feature creation.</li>
<li><strong>Dimensionality Reduction</strong>: By reducing redundant or
correlated features into fewer principal components, PCA can reduce the
number of features.</li>
<li><strong>Anomaly Detection</strong>: Unusual patterns in low-variance
components can be used to identify outliers or anomalies.</li>
<li><strong>Noise Reduction</strong>: PCA can help separate meaningful
signal from background noise, especially in sensor data.</li>
<li><strong>Decorrelation</strong>: It transforms correlated features
into uncorrelated components, which can make it easier for machine
learning models to learn.</li>
</ul></li>
<li><p><strong>Best Practices</strong>:</p>
<ul>
<li>PCA works only with <strong>numeric features</strong>.</li>
<li>Data should be <strong>standardized</strong> (normalized) before
applying PCA to ensure that different feature scales don’t affect the
results.</li>
<li>Outliers should be handled carefully, as they can strongly influence
PCA.</li>
</ul></li>
</ol>
<h4 id="example-1985-automobiles-dataset">Example: 1985 Automobiles
Dataset</h4>
<p>In the example, PCA is applied to four features of automobiles:
<strong>highway MPG</strong>, <strong>engine size</strong>,
<strong>horsepower</strong>, and <strong>curb weight</strong>. After
standardizing the data, PCA creates new components. The first principal
component (PC1) captures a contrast between larger, less fuel-efficient
cars and smaller, more economical ones (a “Luxury/Economy” axis). The
principal components are examined for their <strong>mutual
information</strong> (MI) scores, showing that PC1 is the most
informative for predicting car price.</p>
<h4 id="conclusion-11">Conclusion:</h4>
<p>PCA is a powerful tool for feature engineering, helping to simplify
complex datasets by revealing and capturing the primary axes of
variation. It can be used for dimensionality reduction, noise reduction,
or identifying important relationships. By transforming original
features into new uncorrelated components, PCA enhances model
performance and improves interpretability.</p>
<h3 id="section-6-target-encoding">Section 6: Target Encoding</h3>
<p>This lesson introduces <strong>target encoding</strong>, a supervised
feature engineering technique designed for encoding categorical
features. Unlike one-hot or label encoding, target encoding incorporates
information from the target variable to create the encoding. This method
is particularly useful for high-cardinality features (those with many
categories) or domain-specific features that are suspected to be
important.</p>
<h4 id="key-concepts-8">Key Concepts:</h4>
<ol type="1">
<li><p><strong>Target Encoding</strong>:</p>
<ul>
<li>In target encoding, categories in a feature are replaced by a number
derived from the target variable. A common method is to use the
<strong>mean</strong> of the target within each category.</li>
<li>For example, in the <strong>Automobiles dataset</strong>, the
average car price for each vehicle’s make is calculated and used as the
encoded value for that make.</li>
</ul></li>
<li><p><strong>Challenges with Target Encoding</strong>:</p>
<ul>
<li><strong>Unknown Categories</strong>: If a new category appears in
the data that was not present in the encoding, missing values may occur,
which need to be handled.</li>
<li><strong>Rare Categories</strong>: Categories that occur infrequently
can have inaccurate or noisy encodings, increasing the risk of
<strong>overfitting</strong>.</li>
</ul></li>
<li><p><strong>Smoothing</strong>:</p>
<ul>
<li><strong>Smoothing</strong> addresses the problems of rare and
unknown categories by blending the category’s mean target value with the
overall average target value.</li>
<li>The weight of this blend is determined by the frequency of the
category. Categories that appear more often are weighted more heavily
toward their own mean, while rare categories are more influenced by the
overall mean.</li>
<li>The degree of smoothing is controlled by a parameter
<strong>m</strong>, where higher values of <strong>m</strong> put more
weight on the overall mean.</li>
</ul>
<p>Example formula:</p>
<pre><code>encoding = (weight * in-category mean) + ((1 - weight) * overall mean)
weight = n / (n + m)</code></pre></li>
<li><p><strong>Use Cases for Target Encoding</strong>:</p>
<ul>
<li><strong>High-cardinality features</strong>: When there are many
categories, target encoding reduces the number of features compared to
one-hot encoding, making it more manageable.</li>
<li><strong>Domain-specific features</strong>: If you suspect a
categorical feature is important based on domain knowledge, target
encoding can help capture its true informativeness even if it doesn’t
score well using other feature selection metrics.</li>
</ul></li>
</ol>
<h4 id="example-movielens1m-dataset">Example: MovieLens1M Dataset</h4>
<p>In this example, the MovieLens1M dataset, which contains user ratings
for movies, is used. The <strong>Zipcode</strong> feature, with over
3,000 unique values, is chosen for target encoding. Using 25% of the
data, the encoding is calculated, and smoothing is applied with
<strong>m=5</strong> to control for noise. The distribution of the
encoded Zipcode values follows the distribution of the movie ratings,
indicating that the Zipcode feature contains useful information.</p>
<h4 id="conclusion-12">Conclusion:</h4>
<p><strong>Target encoding</strong> is a powerful method for encoding
categorical features, especially when dealing with a large number of
categories or when domain knowledge suggests a feature’s importance.
However, it comes with the risk of overfitting, which can be mitigated
using techniques like smoothing. This encoding method is well-suited for
supervised learning tasks where the relationship between the categorical
feature and the target is essential for making accurate predictions.</p>
<h2 id="model-explainability">Model Explainability</h2>
<h3 id="section-1-use-cases-for-model-insights">Section 1: Use Cases for
Model Insights</h3>
<p>This lesson discusses the types of insights that can be extracted
from machine learning models, which are often considered “black boxes”
because many data scientists struggle to interpret their inner workings.
However, various techniques can provide insights into:</p>
<ol type="1">
<li><strong>Feature importance</strong>: Which features the model
considers most crucial for predictions.</li>
<li><strong>Impact of features on specific predictions</strong>: How
each feature influences a single prediction.</li>
<li><strong>General effect of features</strong>: The broader influence
of features across a wide range of predictions.</li>
</ol>
<h4 id="why-are-these-insights-valuable">Why Are These Insights
Valuable?</h4>
<p>These insights offer significant benefits, such as:</p>
<ol type="1">
<li><strong>Debugging</strong>: Helps detect and resolve errors,
especially in messy or unreliable data, by checking whether the model’s
patterns align with real-world expectations.</li>
<li><strong>Feature Engineering</strong>: Guides the creation of new
features to improve model accuracy by highlighting the most influential
features and potential transformations.</li>
<li><strong>Directing Future Data Collection</strong>: Helps
organizations decide which new data to collect by analyzing the current
features and their value.</li>
<li><strong>Informing Human Decision-Making</strong>: Provides
explanations for model predictions, which is crucial in situations where
humans, rather than models, make decisions.</li>
<li><strong>Building Trust</strong>: Demonstrates to stakeholders that
the model’s insights align with their understanding of the problem, thus
building confidence in the model’s reliability.</li>
</ol>
<p>By using these insights, data scientists can better understand their
models, improve performance, and increase the trustworthiness of their
predictions. The first technique to be explored is <strong>permutation
importance</strong>.</p>
<h3 id="section-2-permutation-importance">Section 2: Permutation
Importance</h3>
<p>This lesson introduces <strong>feature importance</strong>, a concept
that helps us understand which features in a dataset have the biggest
impact on a model’s predictions. The focus is on <strong>permutation
importance</strong>, a method that is fast, widely used, and easy to
understand.</p>
<h4 id="how-permutation-importance-works">How Permutation Importance
Works:</h4>
<ol type="1">
<li><strong>Model Setup</strong>: After training a model, we keep the
model’s predictions intact.</li>
<li><strong>Shuffle Columns</strong>: We randomly shuffle one column of
the validation data at a time, leaving the other columns and the target
unchanged.</li>
<li><strong>Measure Impact</strong>: By comparing the accuracy of
predictions with and without the shuffled column, we measure how much
the performance deteriorates, which tells us the importance of that
column.</li>
<li><strong>Repeat</strong>: This is repeated for each column to
calculate the importance of all features.</li>
</ol>
<h4 id="code-example">Code Example:</h4>
<p>Using a dataset that predicts whether a soccer team will have the
“Man of the Game” winner based on team statistics, the code builds a
random forest model and calculates permutation importance using the
<strong>eli5</strong> library.</p>
<h4 id="key-results">Key Results:</h4>
<ul>
<li>The most important feature was <strong>Goals Scored</strong>, as it
caused the greatest performance drop when shuffled.</li>
<li>Features like <strong>Distance Covered</strong> and <strong>Yellow
Card</strong> were also somewhat important.</li>
<li>Some features, like <strong>Red</strong> and <strong>Yellow &amp;
Red Cards</strong>, had very low or even negative importance, meaning
they had little to no impact on the model.</li>
</ul>
<h4 id="interpretation">Interpretation:</h4>
<ul>
<li><strong>Higher importance</strong>: Features at the top of the list
caused the largest drop in model accuracy when shuffled.</li>
<li><strong>Randomness</strong>: The “±” values show how much the
importance score varied due to randomness.</li>
<li><strong>Negative values</strong>: Sometimes shuffled data can
accidentally improve predictions, especially in small datasets, leading
to negative importance scores, though this is rare.</li>
</ul>
<p>This method allows us to identify the most crucial features in a
model, improving our understanding of what the model relies on to make
predictions.</p>
<h3 id="section-3-partial-plots">Section 3: Partial Plots</h3>
<p><strong>Partial Dependence Plots (PDPs)</strong> help us understand
how specific features affect predictions made by a machine learning
model. Unlike feature importance, which tells us <em>which</em> features
matter most, PDPs show <em>how</em> the features influence the model’s
predictions.</p>
<h4 id="key-concepts-9">Key Concepts:</h4>
<ol type="1">
<li><p><strong>PDPs Explained</strong>: PDPs show the effect of a
feature on a model’s predictions while holding other features constant.
For example, you can examine how the number of goals scored impacts the
probability of winning “Man of the Match” in a soccer game, while
keeping all other features constant.</p></li>
<li><p><strong>How PDPs Work</strong>: After fitting the model, the
value of one feature is changed (e.g., from low to high), and the
model’s predictions are recalculated. This process is repeated across
many rows, and the average change in the predictions is
plotted.</p></li>
<li><p><strong>Interpreting PDPs</strong>: The y-axis shows how much a
feature changes the predicted outcome compared to the baseline (the
prediction when the feature is at its lowest value). For instance, in
the soccer example, scoring more goals increases the chances of winning
“Man of the Match” up to a certain point, after which additional goals
have little effect.</p></li>
</ol>
<h4 id="code-example-1">Code Example:</h4>
<ul>
<li>A <strong>decision tree model</strong> is built and a PDP is
generated using the <code>PartialDependenceDisplay</code> function from
scikit-learn. The PDP shows how features like “Goals Scored” and
“Distance Covered” affect predictions.</li>
<li>The <strong>PDP for “Goals Scored”</strong> shows that scoring one
goal significantly increases the chances of winning “Man of the Match,”
but further goals have minimal impact.</li>
<li>The <strong>PDP for “Distance Covered”</strong> shows that running
100km improves the chances of winning, but running more than that
decreases the prediction.</li>
</ul>
<h4 id="d-pdps">2D PDPs:</h4>
<p>2D PDPs are useful for examining <strong>interactions between two
features</strong>. For example, the interaction between “Goals Scored”
and “Distance Covered” shows that scoring at least one goal combined
with covering around 100km leads to the highest prediction for winning
“Man of the Match.”</p>
<h4 id="summary">Summary:</h4>
<p>PDPs are useful for understanding the effect of features on a model’s
predictions. They can reveal non-linear relationships and interactions
between features that would be missed by simpler models like linear
regression.</p>
<h3 id="section-4-shap-values">Section 4: SHAP Values</h3>
<p><strong>SHAP (SHapley Additive exPlanations) Values</strong> help
break down individual predictions from a machine learning model to show
the contribution of each feature. They are useful in scenarios where
detailed explanations are required, such as explaining why a bank loan
was denied or why a patient has a high risk of a certain disease.</p>
<h4 id="how-shap-values-work">How SHAP Values Work:</h4>
<ul>
<li><strong>SHAP values</strong> show how each feature’s value impacted
a specific prediction, comparing it to a baseline or average value.</li>
<li>The sum of the SHAP values for all features equals the difference
between the predicted value and the baseline prediction.</li>
</ul>
<h4 id="example-scenario">Example Scenario:</h4>
<p>In a soccer/football example, a model predicts whether a team will
have a “Man of the Match” winner. SHAP values allow us to see how
different features like “goals scored” or “ball possession” affect the
probability of winning this award.</p>
<ul>
<li>SHAP breaks down how each feature influences the prediction by
increasing or decreasing the probability of the team having a “Man of
the Match” winner.</li>
</ul>
<h4 id="code-overview">Code Overview:</h4>
<ol type="1">
<li><strong>Model Setup</strong>: A random forest classifier is trained
to predict whether a team has the “Man of the Match” winner.</li>
<li><strong>SHAP Calculation</strong>: SHAP values are calculated using
the <code>shap</code> library. These values explain the contribution of
each feature to the prediction for a specific team.</li>
<li><strong>Visualization</strong>: SHAP values can be visualized using
a force plot, which shows the impact of each feature on the
prediction.</li>
</ol>
<h4 id="example-of-shap-interpretation">Example of SHAP
Interpretation:</h4>
<ul>
<li>A SHAP force plot shows both positive (increasing the prediction)
and negative (decreasing the prediction) influences on a
prediction.</li>
<li>For example, if “Goals Scored” has the largest positive impact, it
will push the prediction higher, while features like “ball possession”
might have a negative impact.</li>
</ul>
<h4 id="key-features-of-shap">Key Features of SHAP:</h4>
<ul>
<li><strong>Additivity</strong>: SHAP ensures that the sum of all
feature contributions equals the model’s prediction.</li>
<li><strong>Flexibility</strong>: SHAP values can be used with different
models, including decision trees, deep learning, and any other machine
learning models (with <code>shap.TreeExplainer</code>,
<code>shap.DeepExplainer</code>, and
<code>shap.KernelExplainer</code>).</li>
</ul>
<h4 id="conclusion-13">Conclusion:</h4>
<p>SHAP values provide a detailed breakdown of individual predictions,
making them an excellent tool for understanding and explaining model
decisions at the individual level. They are widely applicable in fields
like finance, healthcare, and any situation where transparency and
interpretability of predictions are essential.</p>
<h3 id="section-5-advanced-uses-of-shap-values">Section 5: Advanced Uses
of SHAP Values</h3>
<h4 id="recap">Recap</h4>
<p>This lesson focuses on using <strong>SHAP values</strong> to gain
deeper insights into machine learning model predictions. SHAP values
break down individual predictions into the contributions of each
feature, providing explanations for how each feature value affects the
prediction.</p>
<h4 id="key-concepts-10">Key Concepts:</h4>
<ol type="1">
<li><p><strong>SHAP Values Review</strong>:</p>
<ul>
<li>SHAP values show how much each feature contributes to a prediction
compared to a baseline value.</li>
<li>For example, if a model predicts based on the equation
<code>y = 4 * x1 + 2 * x2</code>, and <code>x1 = 2</code> instead of a
baseline of <code>0</code>, the SHAP value for <code>x1</code> would be
8.</li>
</ul></li>
<li><p><strong>SHAP Summary Plots</strong>:</p>
<ul>
<li>SHAP summary plots provide a birds-eye view of feature importance
and how individual values of a feature affect predictions.</li>
<li>These plots show:
<ul>
<li><strong>Vertical location</strong>: The feature being
represented.</li>
<li><strong>Color</strong>: Whether the feature value is high or
low.</li>
<li><strong>Horizontal location</strong>: The effect of the feature
value on the prediction.</li>
</ul></li>
<li>Example: In a soccer dataset, the model might show that a higher
number of goals scored increases the likelihood of being “Man of the
Match.”</li>
</ul></li>
<li><p><strong>SHAP Dependence Contribution Plots</strong>:</p>
<ul>
<li>These plots show how a single feature impacts predictions, similar
to <strong>Partial Dependence Plots (PDP)</strong> but with more
detail.</li>
<li>Each dot in the plot represents a data point, with the horizontal
axis showing the feature value and the vertical axis showing how that
value affects the prediction.</li>
<li>The spread of points shows if there are interactions with other
features. Color coding helps highlight such interactions. For instance,
teams with similar ball possession might have different outcomes based
on the number of goals scored.</li>
<li>Example: In a soccer game, while more ball possession generally
increases the chances of winning “Man of the Match,” this effect can
reverse if the team only scores one goal.</li>
</ul></li>
<li><p><strong>Interpreting SHAP Dependence Plots</strong>:</p>
<ul>
<li>These plots can highlight how different feature interactions affect
predictions. You can inspect outliers or other unusual patterns to
better understand the model’s behavior.</li>
</ul></li>
</ol>
<h4 id="code-example-2">Code Example:</h4>
<ul>
<li><p><strong>Summary Plots</strong>:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.TreeExplainer(my_model)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer.shap_values(val_X)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values[<span class="dv">1</span>], val_X)</span></code></pre></div>
<ul>
<li>This plot shows the summary of SHAP values for all validation
data.</li>
</ul></li>
<li><p><strong>Dependence Contribution Plots</strong>:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>shap.dependence_plot(<span class="st">&#39;Ball Possession %&#39;</span>, shap_values[<span class="dv">1</span>], X, interaction_index<span class="op">=</span><span class="st">&quot;Goal Scored&quot;</span>)</span></code></pre></div>
<ul>
<li>This plot shows how “Ball Possession %” impacts predictions while
taking “Goal Scored” into account as an interacting feature.</li>
</ul></li>
</ul>
<h4 id="conclusion-14">Conclusion:</h4>
<ul>
<li><strong>Summary plots</strong> provide a comprehensive overview of
feature importance and their impact on predictions, while
<strong>dependence contribution plots</strong> delve deeper into how a
single feature and its interactions affect predictions.</li>
<li>These techniques give more insight than basic feature importance
metrics like permutation importance, allowing for detailed
interpretations of model behavior and interactions between
features.</li>
</ul>
    
</body>
</html>