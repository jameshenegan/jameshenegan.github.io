<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1 id="summary">Summary</h1>
<p><strong>Background</strong></p>
<p>Eight to nine months ago I joined a life-insurance company as the
second data scientist on a three-person Data Analytics team (two data
scientists and a manager). Our flagship project is a monthly
<em>Leads</em> pipeline that predicts which prospects are most likely to
become customers.</p>
<hr />
<h3 id="current-leads-workflow-legacy">1. Current “Leads” workflow
(legacy)</h3>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 36%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr>
<th>Step</th>
<th>Tooling</th>
<th>What happens</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data refresh</strong></td>
<td>On-prem SQL → <strong>Azure Data Factory</strong> → Blob
storage</td>
<td>Updated training set + new scoring (“inferencing”) files land in
blob</td>
</tr>
<tr>
<td><strong>Notebook trigger</strong></td>
<td><strong>Azure ML</strong> (notebooks)</td>
<td>A notebook kicks off an <strong>Azure ML pipeline</strong></td>
</tr>
<tr>
<td><strong>Model training</strong></td>
<td>Azure ML pipeline</td>
<td>Train model on refreshed data; register model in <em>Models</em>
list</td>
</tr>
<tr>
<td><strong>Inferencing</strong></td>
<td>Same pipeline</td>
<td>Score new data; write <em>leads</em> file back to blob</td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td>CSV of high-probability prospects</td>
<td>Consumed by downstream business teams</td>
</tr>
</tbody>
</table>
<p>Everything—code, notebooks, data assets—lives in a single
<em>development</em> Azure ML workspace.</p>
<hr />
<h3 id="mlops-upgrades-already-implemented">2. MLOps upgrades already
implemented</h3>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 3%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr>
<th>Goal</th>
<th>Status</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Source control</strong></td>
<td>✅</td>
<td>All training &amp; scoring code now in <strong>Azure DevOps
Repos</strong> (GitHub is blocked)</td>
</tr>
<tr>
<td><strong>CI/CD orchestration</strong></td>
<td>✅</td>
<td>Two <strong>Azure DevOps pipelines</strong><br>• <em>train</em>
pipeline → triggers Azure ML training job<br>• <em>score</em> pipeline →
triggers Azure ML inferencing job</td>
</tr>
<tr>
<td><strong>Approval gate</strong></td>
<td>✅</td>
<td>After training, metrics are emailed to the manager.<br>If approved,
the model is promoted to the <strong>Azure ML Registry</strong>.</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="next-objective-production-workspace">3. Next objective:
Production workspace</h3>
<p><strong>Purpose</strong> Create a <em>production</em> Azure ML
workspace that auditors can inspect. Only production models/pipelines
should appear there.</p>
<p><strong>Challenge</strong> Our accounts team granted us
<strong>read-only</strong> access to the production workspace. We need
the <em>inferencing</em> pipeline to run there, but it must still
reach:</p>
<ol type="1">
<li><strong>The model</strong> – already solved via Azure ML
Registry.</li>
<li><strong>The scoring data</strong> – <em>unresolved</em>.</li>
</ol>
<hr />
<h3 id="data-access-options-for-production-scoring-jobs">4. Data-access
options for production scoring jobs</h3>
<table style="width:100%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>Option</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Register data as a data asset <em>inside</em> production
workspace</strong></td>
<td>Familiar pattern; simple <code>ml_client.data.get()</code></td>
<td>Requires <em>write</em> permission we don’t currently have.</td>
</tr>
<tr>
<td><strong>Store data in shared Azure Storage (Blob, ADLS Gen2) and
reference the URI</strong></td>
<td>Workspace-agnostic; leverages existing ADF feed; satisfies Microsoft
Learn “path on Azure Storage” guidance</td>
<td>Must manage SAS tokens / RBAC so production workspace can read.</td>
</tr>
<tr>
<td><strong>Share data via Azure ML Registry</strong></td>
<td>Unified promotion flow like models</td>
<td>Registry is designed for <em>metadataed artifacts</em> (models,
code, environments); large, mutable scoring datasets are a poor
fit.</td>
</tr>
<tr>
<td><strong>Batch Endpoints</strong></td>
<td>Abstracts infra; can mount storage once, then submit scores</td>
<td>Extra service to maintain; not needed if monthly cadence is
fine.</td>
</tr>
</tbody>
</table>
<p><strong>Recommended path</strong></p>
<ol type="1">
<li><strong>Keep data in Blob/ADLS</strong>—your ADF pipeline is already
landing files there.</li>
<li>Grant the production workspace’s managed identity <strong>Storage
Blob Data Reader</strong> on the container.</li>
<li>In the DevOps <em>score</em> pipeline, pass the blob URI as an
<code>Input(type="uri_file" | "uri_folder")</code> to the Azure ML
job.</li>
<li>Register only <em>models</em> in the Azure ML Registry; keep data in
storage.</li>
</ol>
<p>This avoids write access to production, satisfies auditors (data
lineage lives in storage + DevOps logs), and matches Microsoft’s best
practice of “external storage for large inference data.” Batch Endpoints
could be revisited later if you need on-demand or higher-frequency
scoring.</p>
<hr />
<h3 id="open-questions-for-the-team">5. Open questions for the team</h3>
<ul>
<li>Do we actually need the production workspace to be <strong>strictly
read-only</strong>? Granting <em>Contributor</em> scoped to
<code>AzureML/Data</code> might simplify things without risking code
changes.</li>
<li>How will we version the monthly scoring files (folder per run?
timestamped blob)? Auditors will want a clear link from model-version →
data-version → leads file.</li>
<li>Should we add an automated <strong>integration test</strong> in
DevOps that validates model + data accessibility before firing the full
scoring job?</li>
</ul>
<hr />
<p>Feel free to pass this along to David and Harris—happy to dive deeper
on any piece!</p>
<h1 id="next-steps">Next Steps</h1>
<p>Below is the practical “decision tree” I usually give teams that are
in exactly the spot you are now—one dev workspace that trains the model,
a prod workspace that only runs monthly scoring—together with the
Azure-ML primitives that make each option work.</p>
<hr />
<h2
id="can-the-prod-job-read-a-data-asset-that-lives-only-in-the-dev-workspace">1
Can the prod job read a <strong>data asset</strong> that lives only in
the dev workspace?</h2>
<p><strong>No.</strong> A workspace-scoped data asset can be resolved
only inside the workspace that owns it.  • The only cross-workspace
mechanism is <strong>Azure ML Registry</strong>. Since April 2025
registries can store data assets, but the service <strong>physically
copies the bytes into the registry storage</strong> and is <em>not</em>
intended for large or sensitive data sets — exactly the scenario you
described (<a
href="https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction?utm_source=chatgpt.com"
title="Introduction to Azure Data Lake Storage - Learn Microsoft">learn.microsoft.com</a>).</p>
<hr />
<h2 id="recommended-pattern-for-your-monthly-lead-scoring-job">2
Recommended pattern for your monthly lead-scoring job</h2>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 63%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>Piece</th>
<th>What to do</th>
<th>Why it solves the problem</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Central storage</strong></td>
<td>Put the monthly scoring file (or folder/MLTable) in a “neutral”
Azure Storage account (Blob or ADLS Gen2). Grant the prod workspace’s
<em>managed identity</em> <strong>Storage Blob Data Reader</strong> on
that container.</td>
<td>Nothing to copy; both workspaces see the <em>same</em> canonical
data.</td>
</tr>
<tr>
<td><strong>Datastore in prod</strong></td>
<td><code>az ml datastore attach --name leadsdatastore --type azure_blob   --account-name &lt;acct&gt; --container-name leads-inference</code></td>
<td>Gives the prod workspace a stable pointer to the container.</td>
</tr>
<tr>
<td><strong>(Optional) data asset in prod</strong></td>
<td><code>az ml data create --name leads_input --version 202506 --type uri_folder --path azureml://datastores/leadsdatastore/paths/2025-06/</code></td>
<td>Versioning + lineage for auditors, but only a <em>pointer</em> — no
second copy.</td>
</tr>
<tr>
<td><strong>Pipeline input</strong></td>
<td><code>yaml inputs:  scoring_data:  type: uri_folder  path: azureml://datastores/leadsdatastore/paths/2025-06/  mode: ro_mount</code></td>
<td>Uses the datastore URI path scheme that AML jobs accept (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&amp;utm_source=chatgpt.com"
title="Set up service authentication - Azure Machine Learning">learn.microsoft.com</a>).</td>
</tr>
</tbody>
</table>
<h3 id="why-this-is-auditor-friendly">Why this is auditor-friendly</h3>
<p>All inference jobs, their pipeline definition, the exact model
version (coming from the registry), the input URI, and the output
artifact are captured inside the <strong>prod workspace</strong> run
history. Auditors open one workspace and see only production activity,
exactly the goal you stated.</p>
<hr />
<h2 id="when-would-i-use-registry-based-data-assets-instead">3 When
would I use <strong>Registry-based data assets</strong> instead?</h2>
<ul>
<li>Small (-ish) reference data that really must be versioned alongside
the model (e.g., a lookup table or risk‐tier thresholds).</li>
<li>Data that many workspaces need and copying it once into the registry
is acceptable. Remember that the registry <strong>ingests a full copy
and replicates it across regions</strong>, so skip this for monthly
10-GB CSV drops (<a
href="https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction?utm_source=chatgpt.com"
title="Introduction to Azure Data Lake Storage - Learn Microsoft">learn.microsoft.com</a>).</li>
</ul>
<hr />
<h2 id="should-we-switch-the-inference-job-to-batch-endpoints">4 Should
we switch the inference job to <strong>Batch Endpoints</strong>?</h2>
<p><strong>Batch Endpoints</strong> wrap the same compute &amp; pipeline
plumbing you have now but give you:</p>
<ul>
<li>A first-class deployment artifact that you can version, swap, roll
back, and call from REST.</li>
<li>Built-in parallelization / sharding of large input folders.</li>
<li>Explicit separation between <em>endpoint</em> (stable name) and
<em>deployment</em> (model + code bundle).</li>
</ul>
<p>They still read input from a datastore/URI path and still write to
blob storage, so the storage pattern above stays the same. Use them if
you’d like a cleaner DevOps surface or if another team (e.g., ADF) will
invoke scoring by REST rather than triggering a full pipeline. Batch
endpoints are meant for high-volume, offline inference and <strong>do
not target real-time latency</strong> (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2"
title="Use datastores - Azure Machine Learning | Microsoft Learn">learn.microsoft.com</a>).</p>
<hr />
<h2 id="rbac-tip-for-the-read-only-prod-workspace-idea">5 RBAC tip for
the “read-only prod workspace” idea</h2>
<p>Humans can stay <strong>Reader</strong> on prod, but the
<strong>service principal</strong> (or managed identity) that your Azure
DevOps pipeline runs as must be <strong>Contributor</strong> on the
workspace to submit jobs and create run records. That still meets
segregation-of-duties requirements while keeping the UI locked down.</p>
<hr />
<h2 id="putting-it-together-high-level-workflow">6 Putting it together –
high-level workflow</h2>
<ol type="1">
<li><p><strong>Dev workspace</strong></p>
<ul>
<li>Train model → register to <em>registry</em></li>
<li>Unit-test pipeline with sample input</li>
</ul></li>
<li><p><strong>Azure DevOps release stage</strong></p>
<ul>
<li>Approval gate promotes the <em>model version</em> and the
<em>inference component</em> to “prod” tags</li>
<li>Pipeline YAML (or Batch Endpoint deployment) is executed
<strong>against the prod workspace</strong>, pointing to the central
datastore path of that month’s leads file.</li>
</ul></li>
<li><p><strong>Prod workspace</strong></p>
<ul>
<li>Records the run, links to model in registry, captures metrics &amp;
output file path</li>
<li>Auditors get a clean ledger: run-id ⇢ model-id ⇢ data-URI ⇢ output
artifact, with no dev clutter.</li>
</ul></li>
</ol>
<hr />
<h3 id="checklist-you-can-action-next-week">Checklist you can action
next week</h3>
<ul class="task-list">
<li><label><input type="checkbox" />Create shared Blob/ADLS container
and assign access to prod workspace MI.</label></li>
<li><label><input type="checkbox" />Register datastore in prod
workspace.</label></li>
<li><label><input type="checkbox" />Modify inference pipeline YAML to
use <code>azureml://datastores/...</code> path.</label></li>
<li><label><input type="checkbox" />Give DevOps service principal
Contributor on prod workspace.</label></li>
<li><label><input type="checkbox" />(Optional) wrap the scoring step in
a <strong>Batch Endpoint</strong> for easier invocation.</label></li>
</ul>
<p>That will unblock your immediate need without duplicating data, keeps
the auditors happy, and leaves you room to mature into registries or
endpoints later as the project evolves.</p>
<h2 id="question-2">Question 2</h2>
<h3 id="quick-glossary-clear-the-jargon-first">1 Quick glossary (clear
the jargon first)</h3>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr>
<th>Term</th>
<th>What it really means</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ADLS Gen2</strong> – <em>Azure Data Lake Storage</em></td>
<td>It’s just a normal <strong>Blob Storage account that has the
“hierarchical namespace” switch turned on.</strong> You get all blob
features plus POSIX-style directories and ACLs, which are handy for
data-lake workloads. (<a
href="https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction?utm_source=chatgpt.com"
title="Introduction to Azure Data Lake Storage - Learn Microsoft">learn.microsoft.com</a>)</td>
</tr>
<tr>
<td><strong>MI (Managed Identity)</strong></td>
<td>A Microsoft Entra-ID identity that Azure manages for you. Your
<strong>Machine Learning workspace already has a system-assigned
MI</strong>; you can grant it RBAC roles (no keys or SAS tokens in
code). (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&amp;utm_source=chatgpt.com"
title="Set up service authentication - Azure Machine Learning">learn.microsoft.com</a>)</td>
</tr>
<tr>
<td><strong>Workspace MI vs. DevOps pipeline identity</strong></td>
<td>Humans can stay <em>Reader</em> on prod, but the <strong>service
principal / MI that the DevOps pipeline runs as must be
<em>Contributor</em> on the prod workspace</strong> so it can submit
jobs. (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2"
title="Use datastores - Azure Machine Learning | Microsoft Learn">learn.microsoft.com</a>)</td>
</tr>
<tr>
<td><strong>WORM (immutable)</strong></td>
<td>A container-level policy that locks every blob after it’s written.
Great for audit, but the job can <em>only read</em> from that container
unless you enabled the <em>append-writes</em> flag. (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles?view=azureml-api-2&amp;utm_source=chatgpt.com"
title="Manage roles in your workspace - Azure Machine Learning">learn.microsoft.com</a>)</td>
</tr>
</tbody>
</table>
<hr />
<h3
id="is-the-current-layout-storage-and-workspaces-in-different-resource-groups-ok">2 Is
the current layout (storage and workspaces in different resource groups)
OK?</h3>
<p>Yes. RBAC scopes cross resource-group boundaries. What matters is
that <strong>both workspaces’ managed identities (and/or your DevOps
SP)</strong> are granted <em>Storage Blob Data Reader</em> (or
<em>Contributor</em> if a job ever writes) on the container or the
storage account.</p>
<hr />
<h3
id="step-by-step-create-the-shared-container-and-wire-it-to-the-prod-workspace">3 Step-by-step:
Create the shared container and wire it to the prod workspace</h3>
<blockquote>
<p>Shown with the Azure CLI; the Portal buttons do the same thing.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create an ADLS-enabled container</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> storage container create <span class="dt">\</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>   <span class="at">--account-name</span> mystgacctprod <span class="dt">\</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">--name</span> leads-inference <span class="dt">\</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">--auth-mode</span> login</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. (Optional) turn on WORM immutability for 7 years</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> storage container immutability-policy create <span class="dt">\</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>   <span class="at">--account-name</span> mystgacctprod <span class="dt">\</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>   <span class="at">--container-name</span> leads-inference <span class="dt">\</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>   <span class="at">--policy-mode</span> Locked <span class="dt">\</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>   <span class="at">--period</span> 2555   <span class="co"># days</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>   <span class="ex">--allow-protected-append-writes</span> true   <span class="co"># only if you need append rights</span></span></code></pre></div>
<p><em>The CLI needs <code>az login</code> with an identity that is
<strong>Storage Blob Data Owner</strong> on the account.</em></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Grant the prod workspace’s Managed Identity read access</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> role assignment create <span class="dt">\</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>   <span class="at">--assignee</span> <span class="va">$(</span><span class="ex">az</span> ml workspace show <span class="at">-g</span> prod-rg <span class="at">-n</span> prod-ws <span class="at">--query</span> identity.principalId <span class="at">-o</span> tsv<span class="va">)</span> <span class="dt">\</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">--role</span> <span class="st">&quot;Storage Blob Data Reader&quot;</span> <span class="dt">\</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">--scope</span>  /subscriptions/<span class="op">&lt;</span>sub<span class="op">&gt;</span>/resourceGroups/prod-rg/providers/Microsoft.Storage/storageAccounts/mystgacctprod/blobServices/default/containers/leads-inference</span></code></pre></div>
<p>Repeat the same assignment for the DevOps service principal if that
identity mounts the datastore at runtime.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. From *inside* the prod workspace, attach the datastore</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> ml datastore attach-blob <span class="dt">\</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>   <span class="at">--workspace-name</span> prod-ws <span class="dt">\</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">--resource-group</span>  prod-rg <span class="dt">\</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">--name</span> leadsdatastore <span class="dt">\</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">--account-name</span> mystgacctprod <span class="dt">\</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>   <span class="at">--container-name</span> leads-inference <span class="dt">\</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>   <span class="at">--grant-workspace-msi-access</span>  <span class="co"># CLI adds the role above for you</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="kw">```</span> <span class="ex">:contentReference[oaicite:4]{index=4}</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="ex">---</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">### 4 How the inference pipeline points to the data</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="kw">```</span>yaml</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="ex">inputs:</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="ex">scoring_data:</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">type:</span> uri_folder</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># partitioned path keeps every monthly drop separate</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">path:</span> azureml://datastores/leadsdatastore/paths/year=2025/month=06/</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="ex">mode:</span> ro_mount      <span class="co"># read-only mount, perfect for WORM</span></span></code></pre></div>
<p>Because the datastore authenticates with the workspace MI, <strong>no
keys or SAS tokens</strong> end up in Git. If you later switch to
<strong>Batch Endpoints</strong>, the very same URI path works
unchanged.</p>
<hr />
<h3 id="naming-conventionscontainer-folders-files">5 Naming
conventions—container, folders, files</h3>
<p>Microsoft doesn’t prescribe one rigid pattern, but these three rules
are baked into their storage docs and the Cloud Adoption Framework:</p>
<ol type="1">
<li><p><strong>Lower-case, no underscores, max 63 chars</strong> for
container names (<a
href="https://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?utm_source=chatgpt.com"
title="Assign an Azure role for access to blob data - Learn Microsoft">learn.microsoft.com</a>)</p></li>
<li><p><strong>Path = semantic + time hierarchy</strong> (good for
pruning and partition-pushdown). Example:</p>
<pre><code>leads-inference/segment=individual/year=2025/month=06/run=2025-06-15/</code></pre>
<p>That mirrors guidance in ADLS partitioning FAQs. (<a
href="https://learn.microsoft.com/en-us/answers/questions/1816755/azure-data-lake-storage-and-strategy?utm_source=chatgpt.com"
title="Azure data lake storage and strategy - Microsoft Q&amp;A">learn.microsoft.com</a>)</p></li>
<li><p><strong>Immutable data ⇒ new folder, not overwrite.</strong> With
WORM enabled you <em>must</em> write a new blob anyway, so your MLTable
or CSV drops just land in a fresh <code>run=</code> sub-folder each
month.</p></li>
</ol>
<hr />
<h3 id="what-about-adls-gen2-vs.-plain-blob">6 What about ADLS Gen2
vs. plain blob?</h3>
<ul>
<li>If you <strong>enabled “hierarchical namespace” when you created the
account</strong>, you already have ADLS Gen2 and can use POSIX ACLs for
fine-grained permissions.</li>
<li>If you didn’t, you’re still fine—ML jobs only need blob APIs. You’d
lose per-file ACLs but RBAC at container scope still works.</li>
</ul>
<hr />
<h3 id="is-read-only-portal-access-to-prod-a-problem">7 Is read-only
portal access to prod a problem?</h3>
<p>Not at all. Keep humans in <strong>Reader</strong> (or <em>Storage
Blob Data Reader</em> if you want them to browse the container). Only
the <strong>automated pipeline identity</strong> must have
write/contributor rights on:</p>
<ul>
<li>The <strong>prod workspace</strong> (to submit jobs &amp; log
runs)</li>
<li>The <strong>datastore / storage container</strong> (write not needed
if the job <em>only reads</em> the scoring file)</li>
</ul>
<hr />
<h3 id="your-next-actions-checklist">8 Your next actions checklist</h3>
<ul class="task-list">
<li><label><input type="checkbox" />Flip on the hierarchical namespace
on your storage account if you want ADLS features.</label></li>
<li><label><input type="checkbox" />Grant <em>Storage Blob Data
Reader</em> to both workspaces’ managed identities.</label></li>
<li><label><input type="checkbox" />Attach the datastore in prod (and
dev, if desired).</label></li>
<li><label><input type="checkbox" />Update the pipeline YAML to the
<code>azureml://datastores/...</code> URI.</label></li>
<li><label><input type="checkbox" />Keep your WORM policy’s
<strong>append-writes</strong> flag off unless you really need to let
Data Factory append data.</label></li>
</ul>
<p>Once this is done, your inference run in the prod workspace can see
the <em>exact same</em> immutable data everyone else sees, auditors have
a clean paper trail, and no one is copying gigabytes between workspaces
ever again.</p>
<h2 id="question-3">Question 3</h2>
<p>Below is the “cookbook” I give new ops teams for wiring an
<strong>existing Blob / ADLS Gen2 container</strong> to a
<em>production</em> Azure ML workspace. (If you only have
<em>Reader</em> on the workspace now, jump to <strong>§
7</strong>—you’ll need a small RBAC change before you can do the
steps.)</p>
<hr />
<h2 id="vocabulary-refresher-so-the-commands-make-sense">1 Vocabulary
refresher (so the commands make sense)</h2>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>Acronym / term</th>
<th>What it really is</th>
<th>Why you care</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ADLS Gen2</strong></td>
<td>A normal <strong>Blob Storage account with the “hierarchical
namespace” switch on</strong>—you get folders + POSIX ACLs on top of
blobs (<a
href="https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction?utm_source=chatgpt.com"
title="Introduction to Azure Data Lake Storage - Learn Microsoft">learn.microsoft.com</a>)</td>
<td>Nice if you want fine-grained access control or Hadoop-style
tools.</td>
</tr>
<tr>
<td><strong>MI – Managed Identity</strong></td>
<td>Every ML workspace is created with a <strong>system-assigned
Microsoft Entra ID identity</strong>. You can grant it RBAC roles just
like a user or SPN (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&amp;utm_source=chatgpt.com"
title="Set up service authentication - Azure Machine Learning">learn.microsoft.com</a>)</td>
<td>Lets jobs read data without embedding keys or SAS tokens.</td>
</tr>
<tr>
<td><strong>Datastore</strong></td>
<td>A <em>pointer</em> that ML keeps to external storage. The pointer
(name + creds) is stored in workspace Key Vault. It does
<strong>not</strong> copy your files (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2"
title="Use datastores - Azure Machine Learning | Microsoft Learn">learn.microsoft.com</a>)</td>
<td>Pipelines can then use
<code>azureml://datastores/&lt;name&gt;/paths/...</code> URIs.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="permissions-you-need-and-why-reader-isnt-enough">2 Permissions
you need (and why <em>Reader</em> isn’t enough)</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 14%" />
<col style="width: 55%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr>
<th>Resource</th>
<th>Minimum role needed</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prod ML workspace</strong></td>
<td><em>Contributor</em> (or the built-in <strong>Azure ML Data
Scientist</strong> role) (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles?view=azureml-api-2&amp;utm_source=chatgpt.com"
title="Manage roles in your workspace - Azure Machine Learning">learn.microsoft.com</a>)</td>
<td>To <em>create</em> the datastore object and to let DevOps submit
jobs.</td>
</tr>
<tr>
<td><strong>Storage account / container</strong></td>
<td><em>Storage Blob Data Reader</em> (read-only) – plus
<em>Contributor</em> if you will ever write results (<a
href="https://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?utm_source=chatgpt.com"
title="Assign an Azure role for access to blob data - Learn Microsoft">learn.microsoft.com</a>)</td>
<td>Allows the workspace MI to mount or stream the files.</td>
</tr>
</tbody>
</table>
<p><em>Reader</em> on the workspace lets you <em>see</em> assets but not
add new ones, so a datastore registration will fail. Typical pattern:
keep humans at <em>Reader</em> but give the <strong>DevOps service
principal</strong> and the <strong>workspace MI</strong>
<em>Contributor</em>.</p>
<hr />
<h2 id="locate-the-prod-workspaces-managed-identity-object-id">3 Locate
the prod workspace’s Managed Identity object ID</h2>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> ml workspace show <span class="at">-g</span> <span class="op">&lt;</span>prod-rg<span class="op">&gt;</span> -n <span class="op">&lt;</span>prod-ws<span class="op">&gt;</span> <span class="dt">\</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--query</span> identity.principalId <span class="at">-o</span> tsv</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># returns something like: 3f23b6c0-a1b2-4c5d-9e6f-7890abcd1234</span></span></code></pre></div>
<hr />
<h2 id="grant-the-storage-role-once">4 Grant the storage role
(once)</h2>
<div class="sourceCode" id="cb6"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="va">STG_SCOPE</span><span class="op">=</span><span class="st">&quot;/subscriptions/&lt;sub&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Storage/ </span><span class="dt">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="st">storageAccounts/&lt;acct&gt;/blobServices/default/containers/&lt;container&gt;&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> role assignment create <span class="dt">\</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--assignee</span> 3f23b6c0-a1b2-4c5d-9e6f-7890abcd1234 <span class="dt">\</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">--role</span> <span class="st">&quot;Storage Blob Data Reader&quot;</span> <span class="dt">\</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">--scope</span> <span class="va">$STG_SCOPE</span></span></code></pre></div>
<p><em>If</em> your scoring job is going to write its results back into
the same container, repeat with <strong>Storage Blob Data
Contributor</strong>.</p>
<hr />
<h2
id="register-the-datastore-from-any-box-that-has-azure-cli-ml-v2">5 Register
the datastore from <em>any</em> box that has Azure CLI ml v2</h2>
<blockquote>
<p>The YAML route is easier to audit and re-run in DevOps.</p>
</blockquote>
<h3 id="create-a-tiny-yaml-file">5.1 Create a tiny YAML file</h3>
<div class="sourceCode" id="cb7"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># leads_datastore.yml</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">$schema</span><span class="kw">:</span><span class="at"> https://azuremlschemas.azureedge.net/latest/azureBlob.schema.json</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">name</span><span class="kw">:</span><span class="at"> leadsdatastore</span><span class="co"> # must be unique in the workspace</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">type</span><span class="kw">:</span><span class="at"> azure_blob</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">description</span><span class="kw">:</span><span class="at"> Monthly lead-scoring input (read-only)</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="fu">account_name</span><span class="kw">:</span><span class="at"> mystgacctprod</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">container_name</span><span class="kw">:</span><span class="at"> leads-inference</span><span class="co"> # already WORM-enabled</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">protocol</span><span class="kw">:</span><span class="at"> https</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># No &#39;credentials:&#39; block – we’re using the workspace MI</span></span></code></pre></div>
<h3 id="run-the-cli">5.2 Run the CLI</h3>
<div class="sourceCode" id="cb8"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> ml datastore create <span class="dt">\</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   <span class="at">--file</span> leads_datastore.yml <span class="dt">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>   <span class="at">--workspace-name</span> <span class="op">&lt;</span>prod-ws<span class="op">&gt;</span> <span class="dt">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">--resource-group</span> <span class="op">&lt;</span>prod-rg<span class="op">&gt;</span></span></code></pre></div>
<p>Because no credentials were supplied, the CLI stores
<em>“identity-based access”</em>. That is exactly the pattern described
in the ML docs’ <strong>CLI (identity-based)</strong> section (<a
href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2"
title="Use datastores - Azure Machine Learning | Microsoft Learn">learn.microsoft.com</a>).</p>
<blockquote>
<p><strong>Troubleshooting tip:</strong> If you see <em>“Forbidden:
authorization failure”</em> the RBAC assignment in § 4 hasn’t propagated
yet (takes up to 5 min) or you accidentally scoped it only to the
storage account but are pointing at a <em>brand-new</em> container.</p>
</blockquote>
<hr />
<h2 id="use-the-datastore-in-your-pipeline-yaml">6 Use the datastore in
your pipeline YAML</h2>
<div class="sourceCode" id="cb9"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">inputs</span><span class="kw">:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">scoring_data</span><span class="kw">:</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> uri_folder</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">path</span><span class="kw">:</span><span class="at"> azureml://datastores/leadsdatastore/paths/year=2025/month=06/</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">mode</span><span class="kw">:</span><span class="at"> ro_mount</span><span class="co"> # read-only, plays nicely with WORM</span></span></code></pre></div>
<p>No further secrets are needed—the job runtime pulls a token for the
workspace MI and mounts the container automatically.</p>
<hr />
<h2 id="what-to-do-if-you-cant-get-contributor-on-prod">7 What to do if
you <em>can’t</em> get Contributor on prod</h2>
<ol type="1">
<li><strong>Ask infra to run § 3–5 once.</strong> After the datastore
object exists, <em>Reader</em> users can reference it freely.</li>
<li>Or <strong>let the DevOps pipeline</strong> (running as a service
principal that <em>does</em> have Contributor) create/update the
datastore as its first stage. Store <code>leads_datastore.yml</code> in
the repo; a one-liner <code>az ml datastore update --file ...</code>
makes it idempotent.</li>
</ol>
<hr />
<h2 id="naming-layout-conventions-borrowed-from-caf">8 Naming &amp;
layout conventions (borrowed from CAF)</h2>
<ul>
<li><strong>Container name:</strong> lower-case, no underscores, ≤ 63
chars (Blob rule of thumb).</li>
<li><strong>Folder path pattern:</strong>
<code>segment=individual/year=2025/month=06/run=2025-06-15/</code> –
semantic keys left-to-right, then time partitions. This lines up with
Microsoft’s ADLS partitioning guidance (<a
href="https://learn.microsoft.com/en-us/answers/questions/1816755/azure-data-lake-storage-and-strategy?utm_source=chatgpt.com"
title="Azure data lake storage and strategy - Microsoft Q&amp;A">learn.microsoft.com</a>).</li>
<li><strong>Never overwrite</strong> in a WORM container; each month is
a new <code>run=</code> folder.</li>
</ul>
<hr />
<h3 id="recap-checklist">9 Recap checklist</h3>
<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 93%" />
</colgroup>
<thead>
<tr>
<th>Done?</th>
<th>Step</th>
</tr>
</thead>
<tbody>
<tr>
<td>☐</td>
<td>Promote yourself or the DevOps SP to <em>Contributor</em> on prod
workspace</td>
</tr>
<tr>
<td>☐</td>
<td>Give workspace MI <em>Storage Blob Data Reader</em> (and Contributor
if writes)</td>
</tr>
<tr>
<td>☐</td>
<td>Commit <code>leads_datastore.yml</code></td>
</tr>
<tr>
<td>☐</td>
<td><code>az ml datastore create --file leads_datastore.yml</code></td>
</tr>
<tr>
<td>☐</td>
<td>Point pipeline input to
<code>azureml://datastores/leadsdatastore/...</code></td>
</tr>
</tbody>
</table>
<p>Once those boxes are ticked, prod jobs see the same immutable leads
files everyone else sees, you stay audit-compliant, and you never copy
data between workspaces again.</p>
    
</body>
</html>