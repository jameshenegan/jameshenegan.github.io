<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1 id="section">1</h1>
<p>Yes — you can turn this into a <strong>spatial-constrained
regionalisation</strong> problem, then pick the solution whose <em>group
× cluster</em> contingency table gives the largest chi-square (or some
effect-size derived from it, e.g. Cramér’s V). Here’s the workflow that
tends to work well in Python:</p>
<hr />
<h3 id="build-a-spatial-graph-of-the-counties">1 Build a spatial graph
of the counties</h3>
<p><em>If you already have a “get-neighbours” function you can skip the
GIS bit.</em></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> geopandas <span class="im">as</span> gpd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> libpysal <span class="im">import</span> weights</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># counties_gdf must contain a geometry column and a unique county id</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> weights.Queen.from_dataframe(counties_gdf)      <span class="co"># or weights.W(adj_list)</span></span></code></pre></div>
<p><code>w</code> is a sparse contiguity matrix that PySAL’s
regionalisation algorithms will accept.</p>
<hr />
<h3 id="represent-the-categorical-cluster-label-numerically">2 Represent
the categorical cluster label numerically</h3>
<p>For regionalisation we need an <em>attribute matrix</em>
<code>X (n_counties × n_features)</code>. If each county sits in exactly
one “cluster_label”, a one-hot encoding works:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(counties_df[<span class="st">&#39;cluster_label&#39;</span>]).values</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>col_labels <span class="op">=</span> pd.get_dummies(counties_df[<span class="st">&#39;cluster_label&#39;</span>]).columns.tolist()</span></code></pre></div>
<p>Each row is all zeros except a single 1, which is fine – algorithms
that <em>minimise</em> within-group variance (Ward, AZP, region-k-means)
will naturally want to put counties with different 1’s into different
regions, which pushes the chi-square statistic up.</p>
<hr />
<h3 id="choose-a-spatially-constrained-clustering-algorithm">3 Choose a
spatially-constrained clustering algorithm</h3>
<p>PySAL’s <strong>spopt</strong> module gives you several off-the-shelf
choices:</p>
<table>
<colgroup>
<col style="width: 32%" />
<col style="width: 30%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr>
<th>Algorithm</th>
<th>Good for</th>
<th>Where to find it</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SKATER</strong> (spanning-tree pruning, you choose
<em>k</em>)</td>
<td>Fast, works with any <em>n_features</em></td>
<td><code>spopt.region.Skater</code> (<a
href="https://pysal.org/spopt/generated/spopt.region.Skater.html?utm_source=chatgpt.com"
title="spopt.region.Skater - PySAL">pysal.org</a>)</td>
</tr>
<tr>
<td><strong>Region-k-means</strong> (k-means + contiguity)</td>
<td>Tends to maximise between-group differences</td>
<td><code>spopt.region.RegionKMeansHeuristic</code> (<a
href="https://pysal.org/spopt/notebooks/reg-k-means.html?utm_source=chatgpt.com"
title="Regional-k-means — spopt v0.6.1 Manual - PySAL">pysal.org</a>)</td>
</tr>
<tr>
<td><strong>Ward (spatial)</strong></td>
<td>Hierarchical, lets you see the whole dendrogram</td>
<td><code>spopt.region.Ward</code> (see tutorial index) (<a
href="https://pysal.org/spopt/tutorials.html?utm_source=chatgpt.com"
title="Tutorials — spopt v0.6.1 Manual - PySAL">pysal.org</a>)</td>
</tr>
<tr>
<td><strong>Max-P</strong> (endogenous <em>k</em>, min-size
constraint)</td>
<td>When you need a lower bound on population, etc.</td>
<td><code>spopt.region.MaxPHeuristic</code> (<a
href="https://pysal.org/spopt/notebooks/maxp.html?utm_source=chatgpt.com"
title="Max-P Regionalization — spopt v0.6.1 Manual - PySAL">pysal.org</a>)</td>
</tr>
</tbody>
</table>
<p>All of them guarantee every region is one connected component.</p>
<p>Example with SKATER for a given <em>k</em>:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spopt.region <span class="im">import</span> Skater</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Skater(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    gdf<span class="op">=</span>counties_gdf,         <span class="co"># must contain the attributes</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    w<span class="op">=</span>w,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    attrs_name<span class="op">=</span>col_labels,    <span class="co"># columns created by get_dummies</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    n_clusters<span class="op">=</span>k              <span class="co"># try several k’s</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>regions <span class="op">=</span> model.labels_      <span class="co"># 0 … k-1</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>counties_gdf[<span class="st">&#39;region&#39;</span>] <span class="op">=</span> regions</span></code></pre></div>
<hr />
<h3 id="evaluate-the-grouping-with-a-chi-square-test">4 Evaluate the
grouping with a chi-square test</h3>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>ct <span class="op">=</span> pd.crosstab(counties_gdf[<span class="st">&#39;region&#39;</span>],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                 counties_gdf[<span class="st">&#39;cluster_label&#39;</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>chi2, p, dof, expected <span class="op">=</span> chi2_contingency(ct.values)  <span class="co"># :contentReference[oaicite:4]{index=4}</span></span></code></pre></div>
<p>Because the algorithms above are <strong>heuristics</strong>, you
usually:</p>
<ol type="1">
<li>Loop over a sensible range of <code>k</code> (say 4 – 40).</li>
<li>Keep the solution with the largest <code>chi2</code> (or Cramér’s V
= √(χ² / (N·(min(r,c)-1))).</li>
<li>Optionally re-seed the algorithm a few times per <code>k</code> to
escape local minima.</li>
</ol>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>, <span class="dv">25</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Skater(counties_gdf, w, col_labels, n_clusters<span class="op">=</span>k, trace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    chi2, <span class="op">*</span>_ <span class="op">=</span> chi2_contingency(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        pd.crosstab(model.labels_, counties_gdf[<span class="st">&#39;cluster_label&#39;</span>]).values</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> chi2 <span class="op">&gt;</span> best[<span class="st">&#39;chi2&#39;</span>]:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        best <span class="op">=</span> <span class="bu">dict</span>(k<span class="op">=</span>k, chi2<span class="op">=</span>chi2, labels<span class="op">=</span>model.labels_)</span></code></pre></div>
<hr />
<h3 id="fine-tuning-alternatives">5 Fine-tuning / alternatives</h3>
<ul>
<li><strong>Simulated annealing, genetic algorithms or integer
programming</strong> can maximise the chi-square directly under
contiguity constraints, but for a few hundred counties the PySAL
heuristics usually get you ≥ 95 % of the optimum in seconds.</li>
<li>If your clusters are <em>very</em> unbalanced (e.g. one label
dominates), consider weighting counties so that rare labels matter more
(give each one-hot column a weight ∝ 1/√pₗ where pₗ is label
proportion).</li>
<li>You can also treat the <code>cluster_label</code> as an ordinal
number and use a 1-d attribute; Region-k-means then behaves like a
contiguity-constrained Jenks break finder.</li>
</ul>
<hr />
<h2 id="putting-it-all-together">Putting it all together</h2>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> spatial_grouping(counties_gdf, cluster_col, neighbour_weights,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                     k_range<span class="op">=</span><span class="bu">range</span>(<span class="dv">4</span>, <span class="dv">25</span>), algo<span class="op">=</span><span class="st">&#39;skater&#39;</span>, <span class="op">**</span>kw):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pd.get_dummies(counties_gdf[cluster_col])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    attrs <span class="op">=</span> X.columns.tolist()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    best <span class="op">=</span> {<span class="st">&#39;chi2&#39;</span>: <span class="op">-</span><span class="dv">1</span>}</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> algo <span class="op">==</span> <span class="st">&#39;skater&#39;</span>:</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            <span class="im">from</span> spopt.region <span class="im">import</span> Skater <span class="im">as</span> RAlgo</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> RAlgo(counties_gdf, neighbour_weights, attrs, n_clusters<span class="op">=</span>k, <span class="op">**</span>kw)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> algo <span class="op">==</span> <span class="st">&#39;rkmeans&#39;</span>:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            <span class="im">from</span> spopt.region <span class="im">import</span> RegionKMeansHeuristic <span class="im">as</span> RAlgo</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> RAlgo(X.values, n_clusters<span class="op">=</span>k, w<span class="op">=</span>neighbour_weights, <span class="op">**</span>kw)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&#39;Unknown algorithm&#39;</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        chi2, <span class="op">*</span>_ <span class="op">=</span> chi2_contingency(</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>            pd.crosstab(model.labels_,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                        counties_gdf[cluster_col]).values</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> chi2 <span class="op">&gt;</span> best[<span class="st">&#39;chi2&#39;</span>]:</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>            best.update(k<span class="op">=</span>k, chi2<span class="op">=</span>chi2, labels<span class="op">=</span>model.labels_)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    counties_gdf[<span class="st">&#39;region&#39;</span>] <span class="op">=</span> best[<span class="st">&#39;labels&#39;</span>]</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> counties_gdf, best</span></code></pre></div>
<hr />
<h3 id="bottom-line">Bottom line</h3>
<p><em>Yes, it’s definitely doable.</em> Spatial-constrained
regionalisation algorithms give you contiguous groups; evaluating them
with the chi-square test lets you pick the grouping whose <em>region ×
cluster</em> relationship is most “surprising”. The combination of PySAL
+ SciPy makes the whole workflow 100 % Python and scales to a few
thousand counties quite comfortably.</p>
<h1 id="section-1">2</h1>
<p>Absolutely. Everything you need lives inside <strong>SciPy</strong>
and <strong>scikit-learn</strong>:</p>
<hr />
<h2 id="turn-the-neighbour-list-into-a-sparse-connectivity-matrix">1
Turn the neighbour list into a sparse connectivity matrix</h2>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sp</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_connectivity(county_ids, get_neighbors):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Return a scipy.sparse CSR matrix with 1 where two counties touch.&quot;&quot;&quot;</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> {c: i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(county_ids)}</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    rows, cols <span class="op">=</span> [], []</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> county_ids:</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> idx[c]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> get_neighbors(c):          <span class="co"># your function</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            j <span class="op">=</span> idx[n]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            rows.append(i)<span class="op">;</span>  cols.append(j)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>            rows.append(j)<span class="op">;</span>  cols.append(i)  <span class="co"># make it symmetric</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.ones(<span class="bu">len</span>(rows), dtype<span class="op">=</span>np.int8)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(county_ids)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sp.csr_matrix((data, (rows, cols)), shape<span class="op">=</span>(n, n))</span></code></pre></div>
<p><code>connectivity[i, j] = 1</code> means counties <em>i</em> and
<em>j</em> must end up in the <strong>same</strong> cluster (region) if
the agglomerative algorithm ever merges them.</p>
<hr />
<h2 id="encode-the-cluster_label-as-features">2 Encode the
<code>cluster_label</code> as features</h2>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(df[<span class="st">&#39;cluster_label&#39;</span>]).to_numpy()   <span class="co"># n_counties × n_labels</span></span></code></pre></div>
<p>Because each row has exactly one “1”, Ward’s linkage
(variance-minimising) tries to keep different labels apart, exactly the
behaviour you want.</p>
<hr />
<h2 id="spatially-constrained-agglomerative-clustering">3
Spatially-constrained agglomerative clustering</h2>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_one_k(X, connectivity, k, linkage<span class="op">=</span><span class="st">&#39;ward&#39;</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AgglomerativeClustering(</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        n_clusters<span class="op">=</span>k,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        linkage<span class="op">=</span>linkage,          <span class="co"># &#39;ward&#39; or &#39;complete&#39; usually best</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        connectivity<span class="op">=</span>connectivity <span class="co"># enforces contiguity</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    ).fit(X)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model.labels_</span></code></pre></div>
<p><code>AgglomerativeClustering</code> is the only scikit-learn
estimator that <em>honours</em> a connectivity graph, so every resulting
“region” is a single connected piece of geography.</p>
<hr />
<h2 id="score-each-solution-with-a-χ²-test">4 Score each solution with a
χ² test</h2>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chi2_score(region_labels, cluster_labels):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    ct <span class="op">=</span> pd.crosstab(region_labels, cluster_labels).to_numpy()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    chi2, <span class="op">*</span>_ <span class="op">=</span> chi2_contingency(ct, correction<span class="op">=</span><span class="va">False</span>)   <span class="co"># no Yates</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chi2</span></code></pre></div>
<p>Cramér’s V = √(χ² / (N · (min(r,c)−1))) is a size-free alternative,
but maximising χ² directly is perfectly fine when you keep
<code>k</code> in a sensible range.</p>
<hr />
<h2 id="grid-search-over-the-number-of-regions">5 Grid-search over the
number of regions</h2>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> best_spatial_grouping(df, get_neighbors, k_min<span class="op">=</span><span class="dv">4</span>, k_max<span class="op">=</span><span class="dv">25</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    county_ids <span class="op">=</span> df[<span class="st">&#39;county&#39;</span>].tolist()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    conn <span class="op">=</span> to_connectivity(county_ids, get_neighbors)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pd.get_dummies(df[<span class="st">&#39;cluster_label&#39;</span>]).to_numpy()</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    best <span class="op">=</span> {<span class="st">&#39;chi2&#39;</span>: <span class="op">-</span><span class="dv">1</span>}</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(k_min, k_max <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> run_one_k(X, conn, k)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        chi2 <span class="op">=</span> chi2_score(labels, df[<span class="st">&#39;cluster_label&#39;</span>])</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> chi2 <span class="op">&gt;</span> best[<span class="st">&#39;chi2&#39;</span>]:</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>            best.update(k<span class="op">=</span>k, chi2<span class="op">=</span>chi2, labels<span class="op">=</span>labels)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&#39;region&#39;</span>] <span class="op">=</span> best[<span class="st">&#39;labels&#39;</span>]          <span class="co"># write back</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df, best</span></code></pre></div>
<p><em>Typical run-times</em>: a few hundred counties and ≤ 40 regions
finishes in a couple of seconds on a laptop.</p>
<hr />
<h3 id="why-this-works-without-extra-packages">Why this works without
extra packages</h3>
<ul>
<li><strong>Contiguity</strong> is enforced by the
<code>connectivity</code> matrix.</li>
<li><strong>Between-group distinctiveness</strong> is pushed up by
Ward/complete linkage, then <em>explicitly</em> optimised by keeping the
solution with the largest χ².</li>
<li>The whole workflow stays inside SciPy + scikit-learn, so it meets
your “no external libraries” rule.</li>
</ul>
<hr />
<h2 id="practical-tips">Practical tips</h2>
<ul>
<li><strong>Try both</strong> <code>linkage='ward'</code> and
<code>'complete'</code> – whichever gives the higher χ² for your
data.</li>
<li><strong>Seed sensitivity</strong>: Agglomerative clustering is
deterministic, but the <em>order of ties</em> can affect results;
shuffling the DataFrame once and re-running is usually enough to be
safe.</li>
<li><strong>Very unbalanced labels</strong>: weight each one-hot column
by <code>1 / √p</code> (where <em>p</em> is the label’s overall
proportion) before clustering; do this by multiplying the corresponding
column in <code>X</code>.</li>
</ul>
<hr />
<h3 id="bottom-line-1">Bottom line</h3>
<p>Yes, you can get geographically contiguous county-groups that
maximally “surprise” your χ² test <strong>using only SciPy and
scikit-learn</strong>. The key is <code>AgglomerativeClustering</code>
with a connectivity matrix plus a simple grid-search over
<em>k</em>.</p>
    
</body>
</html>