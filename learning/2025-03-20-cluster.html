<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><p>When you have a dataset with continuous, categorical, and ordinal
variables, the main challenge in cluster analysis is choosing or
defining a distance (or similarity) measure that accommodates these
different data types. Below is an overview of common strategies and some
practical tips on how to implement them in Python.</p>
<hr />
<h2 id="common-strategies-for-clustering-mixed-type-data">1. Common
Strategies for Clustering Mixed-Type Data</h2>
<h3 id="a-k-prototypes-clustering"><strong>a) K-Prototypes
Clustering</strong></h3>
<ul>
<li><strong>Algorithm</strong>: K-Prototypes is an extension of K-Means
(which only handles continuous features) and K-Modes (which only handles
categorical features).</li>
<li><strong>Key idea</strong>:
<ul>
<li>It uses different distance measures for the continuous and
categorical components.</li>
<li>Typically, Euclidean distance is used for continuous features and
simple matching dissimilarity (0 if the categories match, 1 otherwise)
for categorical features.</li>
<li>A hyperparameter () is used to balance the influence of the
continuous distance vs. the categorical distance.</li>
</ul></li>
<li><strong>Pros</strong>: Easy to apply, straightforward extension of
K-Means.</li>
<li><strong>Cons</strong>: You have to tune the hyperparameter (). It
doesn’t natively handle ordinal variables any differently than regular
categorical variables.</li>
</ul>
<h3
id="b-hierarchical-clustering-or-any-distance-based-clustering-with-gower-distance"><strong>b)
Hierarchical Clustering (or Any Distance-Based Clustering) with Gower
Distance</strong></h3>
<ul>
<li><strong>Distance measure</strong>: Gower distance is specifically
designed for mixed data. It can handle continuous, nominal
(categorical), and ordinal variables.</li>
<li><strong>Key idea</strong>:
<ul>
<li>For each pair of data points, it computes feature-wise similarities
and then averages them.</li>
<li>Continuous features are typically scaled by their range.</li>
<li>Nominal (categorical) features are 0/1 matching.</li>
<li>Ordinal features can be treated like continuous, but using their
rank scaled over [0, 1] if you want to respect the ordinal aspect.</li>
</ul></li>
<li><strong>Implementation</strong>:
<ul>
<li>Scikit-learn does not have a built-in Gower distance function, but
you can either implement it yourself or use third-party packages such as
<code>gower</code> or <code>pygower</code>.</li>
<li>Once you have a distance (or similarity) matrix, you can feed it
into any clustering algorithm that accepts a custom distance matrix
(e.g., Agglomerative Clustering, DBSCAN, etc.).</li>
</ul></li>
<li><strong>Pros</strong>: Very flexible and interpretable.</li>
<li><strong>Cons</strong>: Computing and storing a distance matrix of
size (n n) can be expensive for large (n).</li>
</ul>
<h3
id="c-model-based-clustering-for-mixed-data-e.g.-latent-class-analysis"><strong>c)
Model-Based Clustering for Mixed Data (e.g., Latent Class
Analysis)</strong></h3>
<ul>
<li><strong>Algorithm</strong>: You can consider a mixture model
approach that explicitly models the distribution of each variable type.
For instance, a Gaussian for continuous features and a multinomial for
categorical features (this starts resembling Latent Class Analysis in
some contexts).</li>
<li><strong>Pros</strong>: Potentially more interpretable, provides
probabilistic memberships.</li>
<li><strong>Cons</strong>: Less common in standard Python libraries; you
may need specialized packages.</li>
</ul>
<hr />
<h2 id="practical-implementation-tips">2. Practical Implementation
Tips</h2>
<ol type="1">
<li><p><strong>Data Preprocessing</strong></p>
<ul>
<li><strong>Encoding</strong>:
<ul>
<li>Ordinal variables: make sure to map categories to integer ranks that
reflect the natural order (1 &lt; 2 &lt; 3, etc.).</li>
<li>Nominal (categorical) variables: can be handled via one-hot encoding
(for some algorithms) or via a matching dissimilarity (for Gower
distance or K-Prototypes).</li>
</ul></li>
<li><strong>Scaling</strong>:
<ul>
<li>Continuous variables should usually be scaled so that large
numerical values do not overshadow categorical differences.</li>
</ul></li>
</ul></li>
<li><p><strong>Choosing the Number of Clusters</strong></p>
<ul>
<li>For K-Prototypes, you can use the elbow method (plotting the cost
against (k)), silhouette coefficient, or other internal clustering
metrics (though they often assume Euclidean space, so interpret with
caution).</li>
<li>For hierarchical clustering, you can inspect the dendrogram or use
internal metrics that allow custom distance functions.</li>
</ul></li>
<li><p><strong>Checking Cluster Quality</strong></p>
<ul>
<li>For mixed data, you can use silhouette-like scores if your library
supports a custom distance. Alternatively, rely on domain knowledge: do
the clusters make sense in the context of the problem?</li>
</ul></li>
</ol>
<hr />
<h2 id="example-gower-distance-hierarchical-clustering">3. Example:
Gower Distance + Hierarchical Clustering</h2>
<p>Below is a small, self-contained Python example showing how to:</p>
<ol type="1">
<li>Create a synthetic dataset with continuous, ordinal, and categorical
columns.</li>
<li>Compute the Gower distance manually (using a simple function).</li>
<li>Perform hierarchical clustering based on that distance matrix.</li>
</ol>
<blockquote>
<p><strong>Note</strong>: If you have an existing library for Gower
distance (e.g., <code>gower</code>), that will simplify the distance
calculation step.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------------------------</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a synthetic dataset</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------------------------</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous feature</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>continuous <span class="op">=</span> np.random.randn(n_samples)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinal feature (e.g. ranks 1,2,3)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ordinal <span class="op">=</span> np.random.choice([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], size<span class="op">=</span>n_samples)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Categorical feature (e.g. &#39;A&#39;, &#39;B&#39;, &#39;C&#39;)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>categorical <span class="op">=</span> np.random.choice([<span class="st">&#39;A&#39;</span>, <span class="st">&#39;B&#39;</span>, <span class="st">&#39;C&#39;</span>], size<span class="op">=</span>n_samples)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;cont&#39;</span>: continuous,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;ord&#39;</span>: ordinal,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;cat&#39;</span>: categorical</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Sample data:&quot;</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define a simple Gower distance function</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Gower distance for two samples x and y is the average of partial distances.</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># For continuous features: dist_cont = |x - y| / range</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># For ordinal features: treat them like continuous but ensure they are on [0,1] scale if you have a known min/max</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># For categorical features: dist_cat = 0 if same, 1 if different</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># We&#39;ll compute a distance matrix for all pairs.</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gower_distance(data, feature_types):</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co">    data: Pandas DataFrame</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co">    feature_types: List specifying the type for each column: &#39;continuous&#39;, &#39;ordinal&#39;, &#39;categorical&#39;</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a distance matrix of shape (n_samples, n_samples).</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> data.shape[<span class="dv">0</span>]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> np.zeros((n, n))  <span class="co"># distance matrix</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Precompute ranges/unique values for each column</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    min_vals <span class="op">=</span> {}</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    max_vals <span class="op">=</span> {}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (col, f_type) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(data.columns, feature_types)):</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f_type <span class="kw">in</span> [<span class="st">&#39;continuous&#39;</span>, <span class="st">&#39;ordinal&#39;</span>]:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>            min_vals[col] <span class="op">=</span> data[col].<span class="bu">min</span>()</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            max_vals[col] <span class="op">=</span> data[col].<span class="bu">max</span>()</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, n):</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>            dist_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            valid_features <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (col, f_type) <span class="kw">in</span> <span class="bu">zip</span>(data.columns, feature_types):</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>                x_i <span class="op">=</span> data[col].iloc[i]</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>                x_j <span class="op">=</span> data[col].iloc[j]</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> f_type <span class="op">==</span> <span class="st">&#39;continuous&#39;</span>:</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>                    range_val <span class="op">=</span> max_vals[col] <span class="op">-</span> min_vals[col]</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> range_val <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># If all values are the same, distance contribution = 0</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>                        dist <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>                        dist <span class="op">=</span> <span class="bu">abs</span>(x_i <span class="op">-</span> x_j) <span class="op">/</span> range_val</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                    dist_sum <span class="op">+=</span> dist</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                    valid_features <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> f_type <span class="op">==</span> <span class="st">&#39;ordinal&#39;</span>:</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># scale ordinal to [0,1]</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>                    range_val <span class="op">=</span> max_vals[col] <span class="op">-</span> min_vals[col]</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> range_val <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>                        dist <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>                        dist <span class="op">=</span> <span class="bu">abs</span>(x_i <span class="op">-</span> x_j) <span class="op">/</span> range_val</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>                    dist_sum <span class="op">+=</span> dist</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>                    valid_features <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> f_type <span class="op">==</span> <span class="st">&#39;categorical&#39;</span>:</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>                    dist <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> x_i <span class="op">==</span> x_j <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>                    dist_sum <span class="op">+=</span> dist</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>                    valid_features <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Average distance across all features</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>            D[i, j] <span class="op">=</span> dist_sum <span class="op">/</span> valid_features</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>            D[j, i] <span class="op">=</span> D[i, j]</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> D</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>feature_types <span class="op">=</span> [<span class="st">&#39;continuous&#39;</span>, <span class="st">&#39;ordinal&#39;</span>, <span class="st">&#39;categorical&#39;</span>]</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>dist_matrix <span class="op">=</span> gower_distance(df, feature_types)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Perform hierarchical clustering using the matrix</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="co"># AgglomerativeClustering can accept a precomputed distance matrix with</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="co"># affinity=&#39;precomputed&#39; and linkage=&#39;average&#39; (for Gower, average or complete</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="co"># linkage is commonly used).</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="co"># If you have a large dataset, be mindful of memory usage when storing distances.</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>agglo <span class="op">=</span> AgglomerativeClustering(</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>    n_clusters<span class="op">=</span><span class="dv">3</span>,         <span class="co"># choose a cluster count</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>    affinity<span class="op">=</span><span class="st">&#39;precomputed&#39;</span>,</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>    linkage<span class="op">=</span><span class="st">&#39;average&#39;</span></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> agglo.fit_predict(dist_matrix)</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------------</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Analyze result</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------------</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;cluster&#39;</span>] <span class="op">=</span> labels</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Data with cluster labels:&quot;</span>)</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code></pre></div>
<p><strong>Key points in this example:</strong></p>
<ul>
<li>We used a simple function for Gower distance. In production, you
might use a library-based implementation for efficiency and
reliability.</li>
<li>We used hierarchical clustering
(<code>AgglomerativeClustering</code>) with
<code>affinity='precomputed'</code> to directly pass in the distance
matrix.</li>
<li>After fitting, each sample gets a cluster label, which you can
analyze or visualize.</li>
</ul>
<hr />
<h2 id="example-k-prototypes-with-kmodes-library">4. Example:
K-Prototypes with <code>kmodes</code> Library</h2>
<p>If you want to try <strong>K-Prototypes</strong>, you can install the
<a href="https://github.com/nicodv/kmodes"><em>kmodes</em>
library</a>:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install kmodes</span></code></pre></div>
<p>Then run something like:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kmodes.kprototypes <span class="im">import</span> KPrototypes</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose &#39;df_mixed&#39; has continuous and categorical columns</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Let&#39;s say continuous_cols = [&#39;cont1&#39;, &#39;cont2&#39;] and categorical_cols = [&#39;cat1&#39;, &#39;cat2&#39;]</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># You might encode ordinal columns as integers and treat them as either continuous or categorical</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># depending on your preference.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert categorical columns to string type to ensure they&#39;re recognized as categories by K-Prototypes</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> categorical_cols:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    df_mixed[col] <span class="op">=</span> df_mixed[col].astype(<span class="bu">str</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Indicate the index positions of the categorical columns</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>cat_cols_idx <span class="op">=</span> [df_mixed.columns.get_loc(c) <span class="cf">for</span> c <span class="kw">in</span> categorical_cols]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the K-Prototypes model</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>kproto <span class="op">=</span> KPrototypes(n_clusters<span class="op">=</span><span class="dv">3</span>, init<span class="op">=</span><span class="st">&#39;Huang&#39;</span>, n_init<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kproto.fit_predict(df_mixed, categorical<span class="op">=</span>cat_cols_idx)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>df_mixed[<span class="st">&#39;cluster&#39;</span>] <span class="op">=</span> clusters</span></code></pre></div>
<hr />
<h2 id="final-thoughts">Final Thoughts</h2>
<p>When clustering <strong>mixed-type data</strong>, your main concern
is finding or defining a distance measure (or objective function) that
properly accounts for all variable types.</p>
<ol type="1">
<li><strong>For purely nominal/categorical data</strong>: K-Modes or
hierarchical clustering with an appropriate matching distance is
common.</li>
<li><strong>For mixed continuous and categorical</strong>: K-Prototypes
or Gower-based clustering is typically the go-to.</li>
<li><strong>For ordinal</strong> data, be sure to encode ordinal
features in a way that respects the ranked nature (e.g., mapping to 1,
2, 3, scaled to [0,1], etc.) rather than one-hot encoding.</li>
<li>Always <strong>check cluster quality</strong> with domain knowledge,
or use internal metrics carefully.</li>
</ol>
<p>With these tools and approaches, you should be able to perform
cluster analysis that handles continuous, categorical, and ordinal
features effectively.</p>
    
</body>
</html>