<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1 id="udemy-course-mlflow-in-action">Udemy course: MLflow in
Action</h1>
<h2 id="introduction-to-mlops">Introduction to MLOps</h2>
<h3 id="what-is-mlops">What is MLOps</h3>
<p>The video provides an introduction to MLflow, but before diving into
that, it emphasizes the importance of understanding MLOps (Machine
Learning Operations). MLOps is a culture, not a tool, that integrates
the traditionally separate phases of machine learning model development
(handled by data scientists) and operational deployment (handled by
engineers). It is inspired by DevOps but specifically tailored for
machine learning workflows.</p>
<p>MLOps aims to streamline the machine learning lifecycle by promoting
collaboration between data scientists, engineers, and operations teams.
It facilitates continuous integration, delivery, and training to improve
the speed and efficiency of developing, testing, and deploying
models.</p>
<p>The video highlights that MLOps addresses challenges from siloed
teams, where data scientists and engineers worked independently,
resulting in difficulties when operationalizing models. By adopting
MLOps principles, teams can work on a shared platform, enhancing
communication and efficiency while maintaining their distinct roles.</p>
<p>In the course, the instructor will delve into the various stages of
the machine learning lifecycle, the challenges faced in conventional
processes, and how MLOps principles provide solutions to these issues.
For those already familiar with MLOps and looking to learn about MLflow
specifically, they are invited to skip the foundational MLOps
section.</p>
<h3 id="traditional-machine-learning-lifecycle">Traditional Machine
Learning Lifecycle</h3>
<p>The lesson provides an overview of the traditional machine learning
(ML) lifecycle and explores why MLOps is necessary by identifying
limitations in the conventional process.</p>
<p>A typical ML project begins with <strong>business understanding and
planning</strong>, where subject matter experts and data scientists
define the project objectives and gather insights. This is followed by
<strong>data acquisition</strong> from various sources like files,
databases, or apps, with the quality and quantity of data being critical
to the model’s performance. The next step involves <strong>data cleaning
and transformation</strong> to make the data suitable for training, a
process that is iterative and involves many rounds of refinement.</p>
<p>Once clean data is prepared, the <strong>modeling stage</strong>
begins, which includes tasks like <strong>feature selection, algorithm
selection, model training, and hyperparameter tuning</strong>. This
phase is also highly experimental and iterative, involving repeated
cycles of training and testing to improve the model’s accuracy.</p>
<p>After achieving a satisfactory model, it is <strong>deployed to
production</strong>, meaning the model is integrated into existing
systems to generate predictions on real data. However, deployment is not
the final step. Models need continuous <strong>monitoring</strong> to
detect performance degradation, especially due to <strong>data
drift</strong>, where new data patterns (e.g., new shapes in the
example) cause the model’s accuracy to drop. This requires ongoing
<strong>retraining</strong> of the model with updated data.</p>
<p>This traditional lifecycle has clear limitations, such as being
<strong>highly manual and iterative</strong> and prone to
<strong>inefficiencies</strong> in maintaining model performance over
time. These shortcomings highlight the need for MLOps, which aims to
streamline and automate these processes for more efficient model
deployment and management.</p>
<h3
id="challenges-in-traditional-machine-learning-lifecylce-part-1">Challenges
in Traditional Machine Learning Lifecylce, part 1</h3>
<p>The lesson discusses key challenges in the traditional machine
learning (ML) lifecycle, which contribute to the fact that 80% of ML
models never make it to production. It outlines two main phases of an ML
project: research and development (experimentation) and operations
(deployment). Data scientists handle the experimental phase, while
operational teams manage deployment. However, the process is fraught
with inefficiencies, leading to many models remaining as proofs of
concept (POCs) and never being deployed.</p>
<p>The main issue lies in how isolated these phases are, with data
scientists often working on local machines using tools like Jupyter
notebooks. These notebooks, which may not be scalable or robust, are
then passed to operational teams, often without clear communication or
systemization. This handover creates significant friction because the
two teams use different tools, speak different technical languages, and
lack standardized processes. Engineers often face difficulty
understanding the data scientists’ code, leading to errors,
redundancies, and, in some cases, a complete re-implementation of the
model.</p>
<p>Furthermore, data scientists end up spending 30-40% of their time on
tasks unrelated to model development, such as assisting the operations
team with deployment, rather than focusing on their core responsibility
of model building. This inefficiency, along with the technical gaps and
misunderstandings, is a major reason for the high failure rate in
getting models to production.</p>
<p>The speaker stresses that this “virtual wall” between data scientists
and engineers must be broken for smoother collaboration and project
success, pointing to the inefficiency of current practices as the
primary obstacle.</p>
<h3 id="challenges-part-2">Challenges, part 2</h3>
<p>This section outlines the numerous challenges in deploying machine
learning (ML) models into production, emphasizing that productionizing
ML projects involves many complex activities beyond just building the
model. Even if data scientists and operational teams worked seamlessly
together, the process of moving a model to production is lengthy and
intricate. Here are the key challenges discussed:</p>
<ol type="1">
<li><p><strong>Packaging and Dependencies</strong>: After building a
model, it must be compiled, packaged, and made compatible with
production environments, including managing dependencies and
scripts.</p></li>
<li><p><strong>Performance Considerations</strong>: Training models with
large datasets can take hours or days. Ensuring the model’s performance
(scaling, load balancing, caching, parallelism) is crucial, especially
for use cases like real-time fraud detection, where even milliseconds of
delay are unacceptable.</p></li>
<li><p><strong>Versioning</strong>: In ML, versioning involves not only
code but also data, algorithms, parameters, and environments. Proper
version control is essential to ensure models are reproducible and can
be redeployed quickly if needed, which can become complex without a
proper system in place.</p></li>
<li><p><strong>Monitoring</strong>: Monitoring goes beyond model
accuracy and system health; data quality and drift must also be tracked
to prevent model performance degradation over time. A lack of automated
monitoring often results in reactive fixes only after user
complaints.</p></li>
<li><p><strong>Manual Processes and Lack of Automation</strong>: Many
aspects of ML workflows, including model retraining, are manual, leading
to time-consuming delays. Retraining is inevitable, as models degrade
over time, and manually repeating the full lifecycle can take
weeks.</p></li>
<li><p><strong>Technical Debt</strong>: Much of the time spent on ML
projects is devoted to managing infrastructure, configurations, and
other activities outside model development. This leads to significant
“technical debt,” where more time is spent on non-coding activities than
on actual model building.</p></li>
<li><p><strong>Lack of Standardization</strong>: Traditional approaches
to ML deployment lack standardization and automation, leading to
inefficiencies and lengthy deployment timelines. While data scientists
may develop models in a few weeks, deployments can take months due to
infrastructure and operational challenges.</p></li>
</ol>
<p>In conclusion, the traditional ML lifecycle faces numerous
challenges, from infrastructure and performance concerns to monitoring
and automation gaps. These problems are a major bottleneck for
enterprise ML projects, making streamlined processes, automation, and
MLOps essential for efficient model deployment.</p>
<h3 id="how-mlops-addresses-the-challenges">How MLOps addresses the
challenges</h3>
<p>The lesson introduces MLOps as an emerging trend and a culture aimed
at integrating and automating the machine learning (ML) development and
deployment processes. MLOps connects data engineers, data scientists,
and operations teams in a unified ecosystem, ensuring seamless
transitions between development and operations. MLOps has emerged
gradually as companies encountered challenges in traditional ML
projects, leading to the formation of best practices and standards.</p>
<p>Some key MLOps solutions highlighted include:</p>
<ol type="1">
<li><p><strong>Reducing Friction</strong>: Establishing protocols, like
using standardized Jupyter notebook templates and thorough
documentation, can bridge the gap between data scientists and engineers.
This minimizes the friction between experimentation and
deployment.</p></li>
<li><p><strong>Version Control</strong>: Standardized version control
for code, data, algorithms, and environments enables easy tracking,
comparison, and reproducibility of experiments and models. This ensures
smooth handovers and continuity, even if key contributors
leave.</p></li>
<li><p><strong>Containerization and Distributed Computing</strong>:
Tools like Docker and Kubernetes allow for efficient management of model
dependencies and scalable computing resources, improving performance and
deployment flexibility.</p></li>
<li><p><strong>Automation with CI/CD Pipelines</strong>: Instead of
manually transitioning between ML phases, teams can build continuous
integration and continuous delivery (CI/CD) pipelines to automate the
entire ML workflow, from data ingestion to model deployment and
monitoring.</p></li>
<li><p><strong>Monitoring</strong>: Tools like Prometheus and Grafana
enable real-time monitoring of data, features, accuracy, and system
performance.</p></li>
<li><p><strong>Continuous Training</strong>: Pipelines can be configured
to automatically retrain models based on triggers or regular intervals,
ensuring models stay accurate over time.</p></li>
</ol>
<p>In summary, MLOps shifts the focus from model-centric workflows to
pipeline-centric ones, emphasizing automation, versioning, and
monitoring to overcome traditional ML lifecycle challenges.</p>
<h2 id="introduction-to-mlflow">Introduction to MLflow</h2>
<h3 id="what-is-mlflow">What is MLflow</h3>
<p>This section introduces MLflow, the main topic of the course, and
explains how it addresses the challenges of deploying machine learning
models in production environments. MLflow is an open-source MLOps
platform that helps automate the building, training, and deployment of
ML models, allowing different teams to collaborate effectively. It
provides a comprehensive set of tools for managing the entire ML
lifecycle, from experimentation to deployment.</p>
<p>MLflow consists of four key components:</p>
<ol type="1">
<li><strong>MLflow Tracking</strong>: Tracks and compares experiments,
parameters, and results.</li>
<li><strong>MLflow Projects</strong>: Packages code to ensure it can be
reused and reproduced.</li>
<li><strong>MLflow Models</strong>: Standardizes the packaging and
reusing of models for different platforms.</li>
<li><strong>MLflow Model Registry</strong>: Manages the full lifecycle
of models, including versioning and state transitions.</li>
</ol>
<p>MLflow is platform-agnostic and integrates easily with popular
machine learning libraries (e.g., TensorFlow, PyTorch, scikit-learn) and
languages like Python and R. It offers a modular API-first approach,
enabling seamless integration into existing workflows via REST APIs and
CLI.</p>
<p>Created by Databricks, MLflow has been widely adopted since its
release in 2018. It is inspired by internal MLOps platforms from
companies like Facebook and Google but distinguishes itself by being
open-source, making it accessible to organizations of all sizes. Big
companies such as Microsoft, Facebook, Toyota, and others use MLflow for
productionizing their models.</p>
<p>The course will cover MLflow in detail, with both theoretical and
practical examples, ensuring that by the end, students can incorporate
MLflow into their own projects.</p>
<h3 id="components-of-mlflow">Components of MLflow</h3>
<p>This lesson provides an overview of the four key components of
MLflow, each addressing specific challenges in the machine learning
lifecycle:</p>
<ol type="1">
<li><p><strong>MLflow Tracking</strong>: This component tracks
experiments by logging parameters, code versions, metrics, and results
of model training. It helps address the problem of experiment tracking
by centralizing all the details needed to reproduce and compare
experiments. It offers a web-based UI for easy comparison and supports
local and remote servers.</p></li>
<li><p><strong>MLflow Projects</strong>: This component simplifies the
process of packaging and sharing machine learning code. It ensures
reproducibility by using a project file (in YAML format) to define
dependencies, entry points, and configurations. Projects can be run
locally or remotely and support various environments like Docker and
Conda.</p></li>
<li><p><strong>MLflow Models</strong>: This component streamlines model
deployment by packaging trained models into a standard format that can
be deployed across various environments. It supports multiple ML
libraries and allows models to be saved in different “flavors,” such as
TensorFlow, PyTorch, or scikit-learn. Models can be deployed via REST
APIs, Docker containers, and serverless functions.</p></li>
<li><p><strong>MLflow Model Registry</strong>: This is a centralized
repository for managing models and their versions. It tracks model
changes, provides a searchable interface for models, and allows users to
collaborate on model development. It also supports metadata tagging for
better model discoverability and management.</p></li>
</ol>
<p>These components help automate and standardize various aspects of the
ML lifecycle, from experimentation and code sharing to deployment and
version control.</p>
<h2 id="mlflow-tracking-component">MLflow Tracking Component</h2>
<h3 id="sklearn-regression-model">Sklearn regression model</h3>
<p>In this lesson, the instructor begins the practical section of the
MLflow course, focusing on implementing the four main MLflow components:
Tracking, Projects, Models, and Model Registry. The lesson starts with a
simple regression model using the scikit-learn library, and MLflow will
be integrated into the code in later sections to show how it enhances
the model development and deployment process.</p>
<p>The code example involves training an ElasticNet regression model on
the wine quality dataset, which includes 11 features for predicting wine
quality. Key points of the code:</p>
<ul>
<li><strong>Libraries imported</strong>: Packages like
<code>pandas</code>, <code>NumPy</code>, and <code>scikit-learn</code>
are used, along with <code>ElasticNet</code> from scikit-learn.</li>
<li><strong>Hyperparameters</strong>: The <code>alpha</code> and
<code>L1_ratio</code> hyperparameters are passed using
<code>argparse</code>, with default values set.</li>
<li><strong>Train-Test Split</strong>: The data is split into 75%
training and 25% testing sets.</li>
<li><strong>Model Training</strong>: The ElasticNet model is trained
using the training data, and predictions are made on the test data.</li>
<li><strong>Model Evaluation</strong>: Performance metrics like RMSE
(Root Mean Squared Error) and R² score are calculated to assess model
accuracy.</li>
</ul>
<p>The model produces basic results with an RMSE of 0.79, MAE of 0.62,
and R² score of 0.10. In the next lesson, MLflow components will be
integrated into this code to track experiments, package the model, and
manage deployments.</p>
<p>This lesson lays the foundation for introducing MLflow by walking
through a pure machine learning code example.</p>
<h3 id="sklearn-regression-model-with-mlflow">Sklearn regression model
with MLflow</h3>
<p>In this lesson, the instructor demonstrates how to integrate MLflow
into a machine learning code to track experiments, log parameters, and
store models for future use. The goal is to introduce basic tracking
functionality in MLflow and show how it can help organize, reproduce,
and compare model experiments.</p>
<p>Key concepts:</p>
<ul>
<li><strong>Experiments and Runs</strong>: An <em>experiment</em>
contains multiple <em>runs</em>. A run represents a single execution of
a machine learning model with specific hyperparameters, metrics, and
results.</li>
<li><strong>Logging with MLflow</strong>: By wrapping the machine
learning code within <code>MLflow.start_run()</code>, MLflow can track
metadata related to the model training process, such as parameters,
metrics, and artifacts (models).</li>
<li><strong>Tracking Parameters and Metrics</strong>: The instructor
logs hyperparameters (like <code>alpha</code> and <code>L1_ratio</code>)
and metrics (like RMSE) using <code>log_param()</code> and
<code>log_metric()</code> functions.</li>
<li><strong>Logging Models</strong>: The trained model is saved as an
artifact using <code>mlflow.sklearn.log_model()</code>. This stores the
model in a format that can later be retrieved for further
experimentation or deployment.</li>
<li><strong>MLflow Artifacts</strong>: These are files or sets of files
produced during the experiment, such as the serialized model.</li>
</ul>
<p>After setting up the basic tracking code, the instructor runs the
code and explains how MLflow stores all logs and models in a folder
called <code>mlruns</code>. In this example, the tracking is done
locally, but it can also be extended to remote servers.</p>
<p>The instructor emphasizes that more detailed explanations of MLflow
functions will come later, but for now, the focus is on understanding
the fundamental tracking process.</p>
<h3 id="mlruns-directory">MLruns directory</h3>
<p>In this lesson, the instructor explains the folder structure created
by MLflow when tracking machine learning experiments and demonstrates
how it helps in organizing and storing metadata for each experiment and
run. The key points are as follows:</p>
<ol type="1">
<li><p><strong>MLflow Runs Folder</strong>: After running the MLflow
tracking code, a folder called <code>mlruns</code> is created, which
stores all the metadata and artifacts for each run. The structure
includes:</p>
<ul>
<li><strong>Trash Folder</strong>: Stores deleted experiments and runs,
similar to a computer’s recycle bin.</li>
<li><strong>Experiment Folders</strong>: Each experiment is represented
by a unique ID, not the experiment name. Inside each experiment folder,
individual run folders are created, each with its run ID.</li>
<li><strong>Run Folders</strong>: Inside the run folder, there are
subfolders for storing artifacts, metrics, parameters, and tags. Logged
items like models, metrics (RMSE, R², etc.), hyperparameters (alpha,
L1_ratio), and other metadata are saved.</li>
</ul></li>
<li><p><strong>Metadata Files</strong>: MLflow generates metadata files
(YAML format) for experiments and individual runs, containing details
like experiment ID, run ID, creation time, and more. These files allow
easy retrieval of information about each run.</p></li>
<li><p><strong>Artifacts</strong>: In the run folder, the model is
stored as an artifact, along with environment files (like
<code>conda.yml</code>, <code>requirements.txt</code>) to ensure the
same execution environment can be recreated later.</p></li>
<li><p><strong>Multiple Runs</strong>: You can execute multiple runs
under the same experiment by changing hyperparameters or other
variables. MLflow tracks each run separately, logging all the relevant
details in its respective folder.</p></li>
<li><p><strong>Command Line Parameter Setting</strong>: Instead of
changing hyperparameters directly in the code, you can pass them via
command line arguments (using <code>argparse</code>), making it easier
to experiment with different values without modifying the
script.</p></li>
</ol>
<p>By the end of this lesson, students should understand how MLflow
tracks and organizes experiments, making it easier to compare,
reproduce, and manage their machine learning experiments. The next step
involves visualizing this tracked data in a browser-based interface.</p>
<h3 id="mlflow-ui-tour">MLflow UI tour</h3>
<p>This video provides a tour of MLflow’s user interface (UI), which
helps visualize and compare experiments and runs in a graphical and
interactive way. The UI is accessed via a web browser after starting the
MLflow server from the terminal.</p>
<p>Key points covered in the video:</p>
<ol type="1">
<li><p><strong>UI Structure</strong>: The MLflow UI has two main
sections—Experiments and Models.</p>
<ul>
<li><strong>Experiments Section</strong>: Displays all experiments and
their associated runs. You can sort, filter, and compare runs using
various metrics, parameters, and views (table, chart, or artifact
view).</li>
<li><strong>Runs Comparison</strong>: The table allows for an easy
comparison of multiple runs to find the best-performing model based on
logged metrics like RMSE and MAE. For more advanced comparisons, you can
use various graphing options such as parallel coordinates, scatter
plots, and box plots.</li>
</ul></li>
<li><p><strong>Run Details</strong>: Each run includes details like run
ID, run name, status, and artifacts (e.g., the trained model). Logged
models can be tracked, but registering them for further use (like
production or evaluation) is separate and covered later in the
course.</p></li>
<li><p><strong>Model Section</strong>: This section is currently empty
because no models have been registered yet. Only registered models will
show up here, which is useful for managing models ready for
production.</p></li>
</ol>
<p>Overall, the MLflow UI provides a user-friendly way to track
experiments, compare runs, and manage model development. More advanced
features like model registry will be covered in future lessons.</p>
<h2 id="mlflow-logging-functions">MLflow Logging Functions</h2>
<h3 id="setting-and-getting-tracking-uri---part-1">Setting and Getting
Tracking Uri - Part 1</h3>
<p>In this video, the instructor introduces two important MLflow
functions: <code>set_tracking_uri</code> and
<code>get_tracking_uri</code>. These functions are used to specify and
retrieve the location where MLflow tracks and stores experiment data,
such as metrics, parameters, and artifacts.</p>
<h3 id="key-points-covered">Key points covered:</h3>
<ol type="1">
<li><p><strong><code>MLflow.set_tracking_uri()</code></strong>:</p>
<ul>
<li>This function sets the location where experiment data will be
stored.</li>
<li>By default, MLflow stores everything in a local folder called
<code>mlruns</code>, but users can change this by specifying a different
location.</li>
<li>Different types of URIs can be used:
<ul>
<li><strong>Empty String</strong>: MLflow will create an
<code>mlruns</code> folder in the current directory.</li>
<li><strong>Custom Folder</strong>: You can specify a folder name to
store the logs in a different directory.</li>
<li><strong>File Path</strong>: You can set a custom file path to store
the logs locally, but MLflow only supports storing files on the default
disk (usually the C drive).</li>
<li><strong>Remote URI</strong>: You can set a remote server (HTTP URI)
or use a Databricks workspace to store the experiment data.</li>
</ul></li>
</ul></li>
<li><p><strong><code>MLflow.get_tracking_uri()</code></strong>:</p>
<ul>
<li>This function retrieves the current tracking location that was set
using <code>set_tracking_uri</code>.</li>
<li>It will return either the custom URI set by the user or the default
local directory (<code>mlruns</code>).</li>
</ul></li>
</ol>
<p>The instructor explains these concepts and outlines how you can use
these functions to manage where your experiment logs are stored, either
locally or on a remote server. A practical demonstration of these
functions will be provided in the next video.</p>
<h3 id="setting-and-getting-tracking-uri---part-2">Setting and Getting
Tracking Uri - Part 2</h3>
<p>In this video, the instructor demonstrates how to use the MLflow
<code>set_tracking_uri()</code> and <code>get_tracking_uri()</code>
functions within the code. These functions help control where the
experiment data is stored.</p>
<h3 id="key-steps">Key steps:</h3>
<ol type="1">
<li><p><strong>Set Tracking URI (Default)</strong>:</p>
<ul>
<li>The instructor first sets the tracking URI to an empty string using
<code>set_tracking_uri()</code>. This keeps the default behavior, where
MLflow stores experiment logs in the <code>mlruns</code> folder.</li>
<li>The <code>get_tracking_uri()</code> function is called to retrieve
the URI, which returns an empty string since the default directory is
used.</li>
</ul></li>
<li><p><strong>Custom Folder (Tracking URI)</strong>:</p>
<ul>
<li>The tracking URI is set to a custom folder called “my_tracks” by
passing this value to <code>set_tracking_uri()</code>.</li>
<li>After running the code, a new folder, “my_tracks,” is created, and
experiment data is stored there.</li>
</ul></li>
<li><p><strong>File Path (Tracking URI)</strong>:</p>
<ul>
<li>The instructor then sets the tracking URI to a specific file path
within the project folder, using the required <code>file://</code>
format.</li>
<li>MLflow creates the specified folder (e.g., “Test1”), and experiment
data is stored in that path. Note that MLflow only accepts paths on the
C drive for local file storage.</li>
</ul></li>
</ol>
<p>By the end of the lesson, the instructor has successfully
demonstrated how to set and retrieve custom locations for storing MLflow
experiment logs using the <code>set_tracking_uri()</code> and
<code>get_tracking_uri()</code> functions.</p>
<h3 id="set-mlflow-experiment-function">Set MLflow experiment
function</h3>
<p>In this video, the instructor demonstrates how to use the
<code>set_experiment()</code> function in MLflow. This function is used
to work with an existing experiment and continue running new executions
(runs) under it.</p>
<h3 id="key-points">Key points:</h3>
<ol type="1">
<li><p><strong>Difference Between <code>create_experiment()</code> and
<code>set_experiment()</code></strong>:</p>
<ul>
<li><code>create_experiment()</code>: Used to create a new
experiment.</li>
<li><code>set_experiment()</code>: Used to work with an already existing
experiment and log new runs under it. If the specified experiment name
doesn’t exist, it creates a new one.</li>
</ul></li>
<li><p><strong>Parameters</strong>:</p>
<ul>
<li><strong>Experiment Name</strong>: Takes the name of an existing
experiment. If the name doesn’t exist, a new experiment is created.</li>
<li><strong>Experiment ID</strong>: You can also pass an experiment ID
to specify an experiment directly, but if the ID doesn’t exist, it
throws an exception (unlike the name parameter, which creates a new
experiment if needed).</li>
</ul></li>
<li><p><strong>Return</strong>: The function returns an experiment
object containing the experiment’s details, such as its ID, artifact
location, etc.</p></li>
</ol>
<h3 id="implementation">Implementation:</h3>
<ul>
<li>The instructor demonstrates how to use <code>set_experiment()</code>
to reuse the previously created experiment
<code>experiment_1</code>.</li>
<li>The experiment’s ID is retrieved from the returned experiment
object, and then the code is executed to log new runs under that
experiment.</li>
<li>The experiment and run details are verified in the MLflow UI.</li>
</ul>
<p>This function allows users to easily continue working with existing
experiments without creating new ones.</p>
<h3 id="start-and-end-run-functions">Start and End run functions</h3>
<p>In this video, the instructor explains how to manage runs in MLflow
using several key functions, including <code>start_run()</code>,
<code>end_run()</code>, <code>active_run()</code>, and
<code>last_active_run()</code>.</p>
<h3 id="key-points-1">Key Points:</h3>
<ol type="1">
<li><p><strong>MLflow <code>start_run()</code> Function</strong>:</p>
<ul>
<li><strong>Purpose</strong>: This function starts a new run within an
experiment and logs metrics, parameters, and artifacts. It can also be
used to continue an existing run by providing a run ID.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>Run ID</strong>: If provided, MLflow continues an existing
run; otherwise, a new run is created.</li>
<li><strong>Experiment ID</strong>: Specifies the experiment under which
the run should be created. Only used when creating a new run.</li>
<li><strong>Run Name</strong>: Allows setting a name for the run, but
only for new runs.</li>
<li><strong>Nested</strong>: Enables nested runs (runs within runs),
with the topmost run called the parent and subsequent runs as child
runs.</li>
<li><strong>Tags</strong>: Optional dictionary of key-value pairs to
label the run.</li>
<li><strong>Description</strong>: Optional text to describe the
run.</li>
</ul></li>
<li><strong>Return</strong>: Returns an <code>active_run</code> object
representing the run, which allows for logging and managing metrics,
parameters, and artifacts.</li>
</ul></li>
<li><p><strong><code>end_run()</code> Function</strong>:</p>
<ul>
<li><strong>Purpose</strong>: Ends an active MLflow run. It is used when
<code>start_run()</code> is not in a <code>with</code> block (since the
<code>with</code> block automatically closes the run).</li>
<li><strong>Status</strong>: Takes an optional status parameter (e.g.,
finished, failed) to indicate the run’s outcome.</li>
</ul></li>
<li><p><strong>Managing Existing Runs</strong>:</p>
<ul>
<li>You can continue or overwrite an existing run by passing its run ID
to <code>start_run()</code>. Any changes made to the run (e.g., updating
artifact names) will overwrite the previous run data.</li>
</ul></li>
<li><p><strong>Comparison Between <code>with</code> and
<code>end_run()</code></strong>:</p>
<ul>
<li>When <code>start_run()</code> is used inside a <code>with</code>
block, the run is closed automatically after execution. If not in a
<code>with</code> block, you must manually end the run using
<code>end_run()</code>.</li>
</ul></li>
</ol>
<h3 id="code-example">Code Example:</h3>
<ul>
<li>The instructor demonstrates creating a new experiment with a run,
setting a run name, and overwriting an existing run by using its run
ID.</li>
<li>The code also shows how to manually close a run using
<code>end_run()</code> when the <code>with</code> block is not
used.</li>
</ul>
<p>This lesson explains how to manage runs in MLflow, including
starting, continuing, and ending runs, and discusses the significance of
nested runs and the manual closing of runs with the
<code>end_run()</code> function.</p>
<h3 id="active-last-active-run-functions">Active &amp; Last active run
functions</h3>
<p>In this video, the instructor explains two MLflow functions:
<code>active_run()</code> and <code>last_active_run()</code>, which help
manage and track the status of runs during an MLflow experiment.</p>
<h3 id="key-points-2">Key Points:</h3>
<ol type="1">
<li><p><strong><code>active_run()</code></strong>:</p>
<ul>
<li>This function returns the currently active run during
execution.</li>
<li>It takes no arguments and returns a run object if a run is active;
otherwise, it returns nothing.</li>
<li>This function is only valid between <code>start_run()</code> and
<code>end_run()</code> because a run is active only during this
period.</li>
<li>The instructor demonstrates how to use this function within the
<code>start_run()</code> and prints the run’s ID and name using
<code>run.info</code>.</li>
</ul></li>
<li><p><strong><code>last_active_run()</code></strong>:</p>
<ul>
<li>This function returns the most recent active run that has
ended.</li>
<li>It behaves differently based on its placement in the code:
<ul>
<li>If placed between <code>start_run()</code> and
<code>end_run()</code>, it will return the current active run.</li>
<li>If placed after the <code>end_run()</code>, it will return the last
completed run.</li>
</ul></li>
<li>The instructor shows how to use this function in both scenarios:
within the <code>start_run()</code> block and after
<code>end_run()</code>, demonstrating that it works in both cases to
return the active or last run.</li>
</ul></li>
</ol>
<h3 id="use-case">Use Case:</h3>
<ul>
<li>These functions help monitor the state of runs during and after
execution, which is useful when dealing with multiple runs in an
experiment.</li>
</ul>
<p>The instructor concludes by mentioning that these functions will be
used more in future lessons involving multiple runs. With this, the
explanation of all the run-handling functions in MLflow is complete.</p>
<h2 id="log-multiple-parameters-metrics-function">Log multiple
parameters &amp; metrics function</h2>
<p>In this video, the instructor explains how to use MLflow functions to
log metrics, parameters, and artifacts in machine learning experiments.
The key functions discussed include <code>log_param()</code>,
<code>log_params()</code>, <code>log_metric()</code>, and
<code>log_metrics()</code>. These functions allow tracking key details
during model training.</p>
<h3 id="key-points-3">Key Points:</h3>
<ol type="1">
<li><p><strong><code>log_param()</code> and
<code>log_params()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Used to log hyperparameters for model
training.</li>
<li><strong><code>log_param()</code></strong> logs a single key-value
pair (e.g., a hyperparameter and its value).</li>
<li><strong><code>log_params()</code></strong> logs multiple key-value
pairs at once using a dictionary.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>Key</strong>: The name of the parameter (a string).</li>
<li><strong>Value</strong>: The value of the parameter.</li>
</ul></li>
<li><strong>Returns</strong>: <code>log_param()</code> returns the value
logged, while <code>log_params()</code> returns nothing.</li>
</ul></li>
<li><p><strong><code>log_metric()</code> and
<code>log_metrics()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Used to log metrics like RMSE, MAE, and
R².</li>
<li><strong><code>log_metric()</code></strong> logs a single metric as a
key-value pair.</li>
<li><strong><code>log_metrics()</code></strong> logs multiple metrics at
once using a dictionary.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>Key</strong>: The name of the metric.</li>
<li><strong>Value</strong>: The value of the metric.</li>
<li><strong>Step</strong> (optional): Logs the step for tracking values
during processes like deep learning epochs.</li>
</ul></li>
<li><strong>Returns</strong>: Both functions return nothing.</li>
</ul></li>
</ol>
<h3 id="code-implementation">Code Implementation:</h3>
<ul>
<li>The instructor demonstrates how to replace repeated calls to
<code>log_param()</code> and <code>log_metric()</code> with their plural
counterparts, <code>log_params()</code> and
<code>log_metrics()</code>.</li>
<li>A dictionary of parameters and metrics is created, which is then
logged using the plural functions.</li>
</ul>
<h3 id="practical-demo">Practical Demo:</h3>
<ul>
<li>After implementing the plural logging functions, the code is run
under a new experiment (Experiment 3).</li>
<li>The instructor verifies that both parameters and metrics are
successfully logged using the MLflow UI.</li>
</ul>
<p>The video shows how to simplify code by using
<code>log_params()</code> and <code>log_metrics()</code> to log multiple
parameters and metrics efficiently, making the experimentation process
more streamlined.</p>
<h3 id="log-multiple-artifacts-function">Log multiple artifacts
function</h3>
<p>In this video, the instructor explains how to use the MLflow
functions <code>log_artifact()</code> and <code>log_artifacts()</code>
to log files and directories as artifacts in MLflow experiments. The
video also introduces <code>get_artifact_uri()</code>, which retrieves
the absolute path of logged artifacts.</p>
<h3 id="key-points-4">Key Points:</h3>
<ol type="1">
<li><p><strong><code>log_artifact()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Logs a single file as an artifact for a
run. Artifacts can be anything, such as datasets, models, images, or
output files.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>local_path</strong>: The path of the file to be logged.</li>
<li><strong>artifact_path</strong> (optional): The path where you want
to store the artifact. If not specified, it stores in the default
artifact location.</li>
</ul></li>
<li><strong>Returns</strong>: Nothing.</li>
<li><strong>Example</strong>: The instructor demonstrates how to log the
dataset used in the code by passing the CSV file path to
<code>log_artifact()</code>.</li>
</ul></li>
<li><p><strong><code>log_artifacts()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Logs multiple files by specifying a
directory that contains them.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>local_dir</strong>: The path of the directory containing the
files.</li>
<li><strong>artifact_path</strong> (optional): The path where you want
to store the artifacts.</li>
</ul></li>
<li><strong>Returns</strong>: Nothing.</li>
<li><strong>Example</strong>: The instructor logs both the training and
testing datasets by placing them in a directory and logging the entire
directory using <code>log_artifacts()</code>.</li>
</ul></li>
<li><p><strong><code>get_artifact_uri()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Retrieves the absolute URI of the
specified artifact or the root directory of the active run’s
artifacts.</li>
<li><strong>Parameters</strong>: (optional)
<strong>artifact_path</strong>: The relative path of the artifact. If
not provided, it returns the root artifact directory.</li>
<li><strong>Returns</strong>: The absolute URI of the artifact or the
root directory.</li>
<li><strong>Example</strong>: The instructor demonstrates how to use
<code>get_artifact_uri()</code> to retrieve the URI of the root artifact
directory.</li>
</ul></li>
</ol>
<h3 id="practical-demonstration">Practical Demonstration:</h3>
<ul>
<li>The instructor logs the dataset as an artifact using
<code>log_artifact()</code> and then logs a directory containing
multiple files (training and testing datasets) using
<code>log_artifacts()</code>.</li>
<li>The <code>get_artifact_uri()</code> function is demonstrated, which
returns the absolute path of the root artifact directory.</li>
</ul>
<p>This video covers important functions to log and track files in
MLflow experiments, making it easy to store datasets, models, or any
other artifacts for future use.</p>
<h3 id="set-tags-function">Set tags function</h3>
<p>In this video, the instructor explains how to use the
<code>set_tag()</code> and <code>set_tags()</code> functions in MLflow
to associate tags (key-value pairs) with a run. Tags help label and
organize runs, making it easier to filter, search, and identify specific
runs later on.</p>
<h3 id="key-points-5">Key Points:</h3>
<ol type="1">
<li><p><strong><code>set_tag()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Used to log a single tag (key-value pair)
to the current active run. If there is no active run, a new run is
created.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>Key</strong>: The name of the tag (must be a string).</li>
<li><strong>Value</strong>: The value of the tag (must also be a
string).</li>
</ul></li>
<li><strong>Returns</strong>: Nothing.</li>
<li>Example: The instructor logs a tag called “release.version” with a
value of “0.1” in the code.</li>
</ul></li>
<li><p><strong><code>set_tags()</code></strong>:</p>
<ul>
<li><strong>Purpose</strong>: Used to log multiple tags at once by
passing a dictionary of key-value pairs.</li>
<li><strong>Parameters</strong>:
<ul>
<li><strong>Tags Dictionary</strong>: A dictionary where the keys
represent tag names and the values represent tag values.</li>
</ul></li>
<li><strong>Returns</strong>: Nothing.</li>
<li>Example: The instructor creates a dictionary with multiple tags
(e.g., “release.version” and “platform”) and logs them using the
<code>set_tags()</code> function.</li>
</ul></li>
<li><p><strong>System Tags</strong>:</p>
<ul>
<li>MLflow automatically assigns certain “system tags” to each run,
which are reserved for internal use. These include:
<ul>
<li><strong>MLflow.runName</strong>: Stores the name of the run.</li>
<li><strong>MLflow.source.name</strong>: Contains the file name from
which the run was generated (e.g., <code>main.py</code>).</li>
<li><strong>MLflow.source.type</strong>: Specifies the environment where
the run was executed (e.g., local machine or cloud).</li>
<li><strong>MLflow.user</strong>: Stores the name of the user running
the code.</li>
</ul></li>
</ul></li>
<li><p><strong>Practical Demonstration</strong>:</p>
<ul>
<li>The instructor demonstrates how to use both <code>set_tag()</code>
and <code>set_tags()</code> within the code and shows that the tags are
stored in the <code>tags</code> section in the MLflow artifacts folder.
The tags can also be viewed in the MLflow UI.</li>
<li>The system tags automatically created by MLflow are shown and
explained.</li>
</ul></li>
</ol>
<p>This video demonstrates how tags can help organize and manage runs in
MLflow, making it easier to retrieve and filter specific runs later
on.</p>
    
</body>
</html>