<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h3 id="simple-linear-regression-with-pymc"><strong>Simple Linear
Regression with PyMC</strong></h3>
<h4 id="step-1-install-pymc"><strong>Step 1: Install PyMC</strong></h4>
<p>First, make sure you have PyMC installed. If you haven’t installed it
yet, you can do so with:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pymc</span></code></pre></div>
<p>Additionally, you might need <code>numpy</code> and
<code>matplotlib</code> for data manipulation and visualization:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install numpy matplotlib</span></code></pre></div>
<h4 id="step-2-import-libraries"><strong>Step 2: Import
Libraries</strong></h4>
<p>Next, we need to import the necessary libraries.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
<h4 id="step-3-generate-synthetic-data"><strong>Step 3: Generate
Synthetic Data</strong></h4>
<p>To demonstrate how PyMC works, let’s create some synthetic data for a
simple linear regression model. The model we will be working with
is:</p>
<p>[ y = + x + ]</p>
<p>Where:</p>
<ul>
<li>( ) is the intercept,</li>
<li>( ) is the slope,</li>
<li>( ) is the error term (noise).</li>
</ul>
<p>We will create data where ( = 1 ) and ( = 2 ).</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seed for reproducibility</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some data</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>alpha_true <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>beta_true <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate noisy observations</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> alpha_true <span class="op">+</span> beta_true <span class="op">*</span> x <span class="op">+</span> np.random.normal(<span class="dv">0</span>, sigma_true, size<span class="op">=</span><span class="bu">len</span>(x))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Synthetic Data for Linear Regression&quot;</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>This generates some synthetic data and visualizes it as a scatter
plot.</p>
<h4 id="step-4-define-the-model-in-pymc"><strong>Step 4: Define the
Model in PyMC</strong></h4>
<p>Now we define a probabilistic model using PyMC. We’ll define priors
for the parameters ( ), ( ), and ( ), and then use observed data to
build the likelihood.</p>
<p>In Bayesian statistics, priors represent our beliefs about the
parameters before seeing the data, and the likelihood is how likely the
observed data is given the parameters.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Priors for unknown model parameters</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> pm.Normal(<span class="st">&quot;alpha&quot;</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> pm.Normal(<span class="st">&quot;beta&quot;</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;sigma&quot;</span>, sigma<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Expected value of the model (deterministic part of the model)</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> alpha <span class="op">+</span> beta <span class="op">*</span> x</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood (sampling distribution) of observations</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    Y_obs <span class="op">=</span> pm.Normal(<span class="st">&quot;Y_obs&quot;</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>y)</span></code></pre></div>
<ul>
<li>We define the priors for ( ), ( ), and ( ) using normal
distributions. These are weakly informative priors, meaning they allow
for a wide range of values but still constrain extreme values.</li>
<li>The <code>mu</code> variable represents the linear model.</li>
<li>The likelihood, <code>Y_obs</code>, is defined as a normal
distribution with mean <code>mu</code> and standard deviation
<code>sigma</code>, and it takes the observed data <code>y</code>.</li>
</ul>
<h4 id="step-5-sampling-from-the-posterior"><strong>Step 5: Sampling
from the Posterior</strong></h4>
<p>Once the model is defined, we can sample from the posterior
distribution to infer the parameters ( ), ( ), and ( ). PyMC provides
the <code>pm.sample()</code> function for this.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw posterior samples</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">1000</span>, return_inferencedata<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>This will use the NUTS (No-U-Turn Sampler) algorithm, which is a type
of Hamiltonian Monte Carlo (HMC) method, to sample from the posterior.
The result is a trace that contains samples for all parameters.</p>
<h4 id="step-6-analyze-the-results"><strong>Step 6: Analyze the
Results</strong></h4>
<p>After sampling, you can visualize the posterior distributions of the
parameters and their summary statistics.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot posterior distributions</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>az.plot_trace(trace)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize the posterior distribution</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>summary <span class="op">=</span> az.summary(trace)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(summary)</span></code></pre></div>
<p>This gives you a summary of the posterior distribution of each
parameter (mean, standard deviation, and credible intervals) and plots
of the trace (samples) and the posterior density.</p>
<h4 id="step-7-posterior-predictive-checks"><strong>Step 7: Posterior
Predictive Checks</strong></h4>
<p>Once you’ve inferred the parameters, it’s important to check how well
the model predicts new data. You can do this by drawing samples from the
posterior predictive distribution.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior predictive checks</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    posterior_predictive <span class="op">=</span> pm.sample_posterior_predictive(trace)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot posterior predictive samples</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y, label<span class="op">=</span><span class="st">&quot;Observed data&quot;</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, posterior_predictive[<span class="st">&quot;Y_obs&quot;</span>][i], color<span class="op">=</span><span class="st">&quot;gray&quot;</span>, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Posterior Predictive Samples&quot;</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>In this plot, you can see the observed data as scatter points, and
the lines represent different samples from the posterior predictive
distribution. This gives an idea of the uncertainty around the
predictions.</p>
<h4 id="step-8-conclusion"><strong>Step 8: Conclusion</strong></h4>
<p>In this tutorial, we performed simple linear regression using PyMC by
following these steps:</p>
<ol type="1">
<li>Generated synthetic data.</li>
<li>Defined the probabilistic model.</li>
<li>Sampled from the posterior distribution.</li>
<li>Analyzed the results.</li>
<li>Performed posterior predictive checks.</li>
</ol>
<p>With PyMC, you can extend this model to more complex regression
models (e.g., multiple linear regression, hierarchical models, etc.) or
even handle non-linear relationships by using different priors or
likelihoods.</p>
    
</body>
</html>