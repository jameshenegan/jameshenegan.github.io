<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1 id="section">1</h1>
<p>Below is a practical playbook you can adapt.Â Iâ€™ll start with the â€œone
big model vs.Â two stateâ€‘specific modelsâ€ question, then lay out a
workflow for feature reduction, picking the numberÂ of clusters, and the
â€œdescribeâ€‘afterâ€‘youâ€‘clusterâ€ stage.</p>
<hr />
<h2 id="should-tennessee-and-colorado-be-clustered-together">1Â Â Should
Tennessee and Colorado be clustered together?</h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 36%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr>
<th>What to check</th>
<th>How to check it (quick tests)</th>
<th>What the result tells you</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>How separable are the two states?</strong></td>
<td>â€¢ Train any fast classifier (e.g., gradientâ€‘boosted trees) to
predict <em>state</em> from <em>all 150 features</em>.<br>â€¢ Look at
crossâ€‘validated AUC / accuracy.</td>
<td><strong>AUC â‰ˆÂ 1</strong> â†’ The states already live in different
parts of featureâ€‘space; joint clustering will mostly recreate â€œstateâ€ as
the first split.<br><strong>AUC â‰ˆÂ 0.6â€“0.7</strong> â†’ States overlap a
lot; joint clustering is more defensible.</td>
</tr>
<tr>
<td><strong>Variance captured by state vs.Â everything else</strong></td>
<td>â€¢ Fit a oneâ€‘way ANOVA or, for mixed types, compute Î·Â² / CramÃ©râ€™sÂ V
for <em>each</em> feature against <em>state</em>.<br>â€¢ Summarize the
distribution of those effect sizes.</td>
<td>If many features have large Î·Â² or V (&gt;Â 0.14 â‰ˆ â€œlargeâ€), state
differences dominate the data â€“ consider splitting.</td>
</tr>
<tr>
<td><strong>Cluster stability across states</strong> (only takes
minutes)</td>
<td>â€¢ Run a pilot clustering on a 1% stratified sample <em>ignoring
state</em>.<br>â€¢ Crossâ€‘tab clustersÂ Ã—Â state.<br>â€¢ Compute AdjustedÂ Rand
vs.Â state.</td>
<td>If clusters almost map 1â€‘forâ€‘1 to TN vs.Â CO, youâ€™ve learned that
â€œstateâ€ is the natural firstâ€‘level segmentation, so run separate models
or take a hierarchical approach (see below).</td>
</tr>
</tbody>
</table>
<p><strong>Rules of thumb</strong></p>
<ul>
<li><p><strong>If â€œstateâ€ explainsÂ â‰«Â 30Â % of total variance (or you can
predict state with â‰³Â 90Â % accuracy), do <em>not</em> force one joint
model.</strong><br />
Youâ€™ll waste the 5 remaining clusters on rediscovering the
obvious.</p></li>
<li><p><strong>If state explains only a modest slice of variance, or you
<em>want</em> crossâ€‘state personas (e.g., marketing campaigns that apply
in both markets), keep one model but still make sure your postâ€‘hoc
cluster descriptions include the state breakdown.</strong></p></li>
<li><p><strong>Compromise option:</strong> hierarchical or multiâ€‘level
clustering</p>
<ol type="1">
<li>Split by state, find <em>withinâ€‘state</em> clusters (say 3
each).</li>
<li>Pool the cluster centroids and reâ€‘cluster them (3Â +Â 3Â â†’Â â‰¤Â 6 total
personas).<br />
This often yields interpretable personas that retain local nuance.</li>
</ol></li>
</ul>
<hr />
<h2 id="feature-reduction-before-clustering">2Â Â Feature reduction before
clustering</h2>
<p>Youâ€™re right: 150 dimensions + 6 clusters is asking for â€œsnowball
clustersâ€ that mix unrelated people. A proven twoâ€‘step recipe:</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 56%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th>Step</th>
<th>What to do</th>
<th>Tips</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2.1Â Â Structural filtering (get from
150Â â†’Â â‰ˆÂ 70)</strong></td>
<td>â€¢ Remove nearâ€‘zero variance columns.<br>â€¢ Collapse oneâ€‘hot groups
with rare levels into â€œotherâ€.<br>â€¢ Drop highly collinear pairs
(â”‚Ïâ”‚Â &gt;Â 0.9).</td>
<td>Keep a log of everything you drop â€“ stakeholders always ask.</td>
</tr>
<tr>
<td><strong>2.2Â Â Noise &amp; redundancy filtering
(â‰ˆÂ 70Â â†’Â 20â€“40)</strong></td>
<td>â€¢ Unsupervised: use variance threshold, sparse PCA, or autoencoder
bottleneck.<br>â€¢ Semiâ€‘supervised: calculate mutual information with a
highâ€‘level business target (e.g., spend, churn) and keep topâ€‘k features
<em>if</em> that aligns with goals.</td>
<td>You need enough features to <em>build</em> clusters (20â€“40 works
well), but you canâ€”and often shouldâ€”describe them later with the full
150.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="choosing-6-clusters-that-are-still-meaningful">3Â Â Choosing
<strong>â‰¤Â 6</strong> clusters that are still meaningful</h2>
<ol type="1">
<li><strong>Start from the data, not the mandate.</strong><br />
Compute internal metrics (silhouette, Daviesâ€“Bouldin, gap statistic) for
<em>k</em>Â =Â 2â€“12 on a 1Â % sample with MiniBatchÂ Kâ€‘means or GMM.</li>
<li><strong>Find the â€œelbowâ€Â /Â local optimum.</strong><br />
If the best <em>k</em> is 9 and your cap is 6, show the tradeâ€‘off graph
to decisionâ€‘makers. Often youâ€™ll get approval for 7â€“8 once they see the
loss in cohesion.</li>
<li><strong>If you must stay at 6:</strong>
<ul>
<li>Run <em>k</em>Â =Â 6, then compute <em>withinâ€‘cluster</em> silhouettes
and entropy.</li>
<li>Split any lowâ€‘cohesion cluster postâ€‘hoc into â€œsubâ€‘segmentsâ€ for
internal use while reporting the mandated 6 externally.</li>
</ul></li>
</ol>
<hr />
<h2 id="describing-profiling-the-clusters-stage-2">4Â Â Describing /
profiling the clusters (StageÂ 2)</h2>
<ul>
<li>After locking the cluster labels, <strong>freeze the clustering
feature set</strong> and bring back <em>all</em> 150 variables for
description.</li>
<li>Popular, scalable descriptive tools:
<ul>
<li><strong>Target vs.Â overall mean / lift charts</strong> for numeric
features.</li>
<li><strong>CramÃ©râ€™sÂ V heatmap</strong> for categorical enrichment.</li>
<li><strong>Oneâ€‘vsâ€‘rest SHAP values</strong> from a tree model trained
to predict each cluster labelâ€”gives intuitive â€œwhat drives membershipâ€
stories.</li>
</ul></li>
<li>Provide <strong>state mix (%) per cluster</strong> even if you ran
stateâ€‘specific models; execs love to see whether a segment is
â€œTNâ€‘skewedâ€ or â€œtruly national.â€</li>
</ul>
<hr />
<h2 id="putting-it-all-together-a-minimal-pipeline">5Â Â Putting it all
together â€“ a minimal pipeline</h2>
<pre class="text"><code>1. Stratified sample (1Â %) â†’ EDA + checks in Â§1.
2. Decide: joint vs. split vs. hierarchical.
3. Full data â†’ preprocessing (Â§2) â†’ MiniBatchÂ Kâ€‘means / GMM grid (k=2â€“12).
4. Pick k with metrics (Â§3) and business cap.
5. Fit final model on 100Â % data â†’ assign idsÂ â†’ store cluster_id.
6. Descriptive notebook (Â§4) â†’ tables, graphs, narrative deck.
7. Validate over time (monthly refresh): stability index, new data drift.</code></pre>
<hr />
<h3 id="next-steps-for-you">Next steps for you</h3>
<ul>
<li><strong>Run the separability quickâ€‘checks</strong> first; they
usually settle the â€œone vs.Â two modelsâ€ debate fast.</li>
<li>If youâ€™d like example code snippets (scikitâ€‘learn / PySpark) or a
template for the descriptive notebook, let me know and I can draft
them.</li>
</ul>
<p>Good luckâ€”once you pin down how much â€œstateâ€ matters, the rest of the
design decisions fall into place.</p>
<h1 id="section-1">2</h1>
<p>Absolutelyâ€”youÂ <em>can</em> salvage value from those 23
â€œoffâ€‘theâ€‘shelfâ€ clusters, but youâ€™ll want to treat them as raw material
rather than a finished product. Here are four practical ways teams fold
an inherited label set into a new, smaller segmentation, plus some quick
tests to decide whether collapsing to â‰ˆÂ 6 groups is defensible.</p>
<hr />
<h2 id="audit-the-23cluster-set-first">1. Audit the 23â€‘cluster set
first</h2>
<p>Before you collapse anything, check three things:</p>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 26%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr>
<th>Check</th>
<th>How (fast)</th>
<th>Why it matters</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Internal quality</strong></td>
<td>Silhouette / Daviesâ€“Bouldin on the original 150 features.</td>
<td>If the 23 clusters themselves are shaky, merging will only make them
worse.</td>
</tr>
<tr>
<td><strong>Business relevance</strong></td>
<td>Crossâ€‘tab the 23 labels with KPIs (spend tiers, churn).</td>
<td>You may discover only 10 of the 23 drive any outcome; those are the
candidates to keep distinct.</td>
</tr>
<tr>
<td><strong>State skew</strong></td>
<td><code>%Â Tennessee</code> vs.Â <code>%Â Colorado</code> per
cluster.</td>
<td>If some clusters are 95Â % TN, others 95Â %Â CO, youâ€™re really looking
at a hidden â€œstateÂ Ã—Â subclusterâ€ design; that changes how you
merge.</td>
</tr>
</tbody>
</table>
<p>If quality is poor or heavily stateâ€‘biased, start fresh. Otherwise
pick one of the merge strategies below.</p>
<hr />
<h2 id="strategies-for-collapsing-23-6">2. Strategies for collapsing
23Â â†’Â ~6</h2>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 50%" />
<col style="width: 17%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>Strategy</th>
<th>Steps</th>
<th>Good whenâ€¦</th>
<th>Caveats</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hierarchical merging of centroids</strong></td>
<td>1Â Â Compute 23 centroids in original feature
space.<br>2Â Â Agglomerative clustering (Ward or complete) on those 23
points.<br>3Â Â Cut the dendrogram atÂ <em>kÂ =Â 6</em>.</td>
<td>You trust the original geometry and just want a higherâ€‘level
rollâ€‘up.</td>
<td>If some original clusters are huge and others tiny, centroids can be
misleadingâ€”weight by cluster size.</td>
</tr>
<tr>
<td><strong>Outcomeâ€‘driven merge (supervised)</strong></td>
<td>1Â Â Pick a business target.<br>2Â Â Train a tree/GBM to predict the
target using <em>only</em> the 23â€‘cluster label.<br>3Â Â Prune the tree so
it produces â‰¤Â 6 leaves; each leaf is a merged group.</td>
<td>Management insists the segments predict a specific KPI.</td>
<td>You will ignore variance <em>not</em> related to that KPI.</td>
</tr>
<tr>
<td><strong>Graphâ€‘based coâ€‘membership</strong></td>
<td>1Â Â Compute misclassification probabilities: for each pairÂ (i,j), how
often do individuals switch between clusters across multiple reâ€‘runs or
perturbations?<br>2Â Â Build a similarity graph, then Louvain / Leiden to
detect â‰ˆÂ 6 communities.</td>
<td>You can rerun the vendorâ€™s algorithm or approximate it to get
coâ€‘membership stats.</td>
<td>Requires multiple runs; heavy for 1.5Â M rows unless you
subsample.</td>
</tr>
<tr>
<td><strong>Use labels as a <em>feature</em> rather than a
cluster</strong></td>
<td>Oneâ€‘hot the 23â€‘label column, feed it (with other features) into your
own clustering; let the algorithm decide whether to honor or ignore
it.</td>
<td>Youâ€™re not sure yet whether to trust or toss the vendor labels.</td>
<td>The label may dominate unless you downâ€‘weight it.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="how-to-test-whether-the-6group-merge-works">3. How to test
whether the 6â€‘group merge â€œworksâ€</h2>
<ol type="1">
<li><strong>Stability</strong> â€“ Jaccard similarity of group membership
when you reâ€‘sample or reâ€‘run the merge logic.</li>
<li><strong>Cohesion vs.Â separation</strong> â€“ Compare silhouette of
6â€‘group solution to that of the full 23. AÂ drop of â‰¤Â 10Â % is usually
acceptable.</li>
<li><strong>Business lift</strong> â€“ Reâ€‘check KPI differences. If the 6
groups still split KPIs cleanly (ANOVA <em>p</em>Â &lt;Â 0.05 or â‰¥Â 10Â %
lift), youâ€™ve kept most of the signal.</li>
<li><strong>Interpretability</strong> â€“ Can you write a twoâ€‘sentence
description of each merged group your stakeholders understand? If not,
reconsider the merge.</li>
</ol>
<hr />
<h2 id="practical-workflow-you-can-run-in-a-day">4. Practical workflow
you can run in a day</h2>
<pre class="text"><code>A.  Pull 100Â k stratified rows  âœ quick diagnostics (Â§1).
B.  Pick a merge strategy (start with centroidâ€‘hierarchical).
C.  Implement on full 1.5Â M:
    â€“ compute centroids            (PySpark or pandas groupbyâ€‘mean)
    â€“ scipy.cluster.hierarchy.linkage(â€¦, method=&quot;ward&quot;)
    â€“ scipy.cluster.hierarchy.fcluster(â€¦, t=6)
D.  Assign new_group_id to every record via a lookup.
E.  Validate: silhouettes, KPI lift, state mix.
F.  If unhappy, try supervised merge or keep &gt;Â 6 groups and
    negotiate with stakeholders (show the diagnostic plots).</code></pre>
<hr />
<h2 id="when-to-discard-the-vendor-labels-altogether">5. When to
<em>discard</em> the vendor labels altogether</h2>
<ul>
<li>They were built on a radically different feature set than
yours.</li>
<li>Most clusters are tiny (â‰¤Â 1Â % of population) or amorphous
(silhouetteÂ &lt;Â 0).</li>
<li>They embed a business logic you no longer care about (e.g., â€œcatalog
buyersâ€ in a mostlyâ€‘digital world).</li>
</ul>
<p>In those cases, treat the 23â€‘label column as historical metadata
onlyâ€”use it for backâ€‘testing and storytelling but not for the new
segmentation.</p>
<hr />
<h3 id="where-this-leaves-you">Where this leaves you</h3>
<ol type="1">
<li><strong>Run the quick audit</strong> in Â§1 to gauge label quality
and state bias.</li>
<li>If quality â‰¥Â â€œgood enough,â€ prototype the centroid merge; itâ€™s the
fastest path to a 6â€‘group rollâ€‘up you can show tomorrow.</li>
<li>Keep the diagnostic plots handyâ€”if stakeholders push back, youâ€™ll
have evidence for why 6 groups is (or isnâ€™t) defensible.</li>
</ol>
<p>Let me know if youâ€™d like code snippets for the hierarchical merge or
for the KPIâ€‘driven tree pruning, and I can paste them.</p>
<h1 id="section-2">3</h1>
<p>Youâ€™re absolutely right that <em>vanilla</em> hierarchical
agglomerative clustering (HAC) on the <strong>raw 1.5Â million
rows</strong> is a nonâ€‘starter:</p>
<ul>
<li><strong>Time complexity</strong>â€ƒâ‰ˆÂ O(nÂ²) distance computations â†’
(1.5Â M)Â²Â â‰ˆÂ 2.3Â Ã—Â 10Â¹Â² distances.</li>
<li><strong>Memory</strong>â€ƒThe condensed distance matrix alone would be
~17Â TB (at 8Â bytesÂ /Â float).</li>
</ul>
<p>So, when I spoke about â€œhierarchical merging,â€ I meant
<em>hierarchicalâ€‘onâ€‘prototypes</em>, <strong>not</strong> â€œrun HAC on
every record.â€ Here are the scalable variations that <em>are</em>
feasible:</p>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 39%" />
<col style="width: 15%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr>
<th>What actually runs</th>
<th>Data size HAC sees</th>
<th>Typical useâ€‘case</th>
<th>Feasible on 1.5Â M?</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Centroid / prototype merge</strong> (what I suggested for
the 23 vendor clusters)</td>
<td>23 centroids â†’ HAC on 23Â Ã—Â 150 matrix</td>
<td>Collapsing an existing label set</td>
<td>âœ“ instant</td>
</tr>
<tr>
<td><strong>Sampleâ€‘thenâ€‘assign</strong></td>
<td>HAC on a stratified sample (e.g., 50Â k) âœ train a classifier (kâ€‘NN,
tree) to label the remaining 1.45Â M</td>
<td>Need full dendrogram for interpretability</td>
<td>âœ“ (sample)</td>
</tr>
<tr>
<td><strong>Twoâ€‘stage â€œminiâ€‘Batch Kâ€‘means â†’ HAC on K centroidsâ€</strong>
(aka â€œKâ€‘meansâ€‘treeâ€ or <em>make_blobsÂ â†’Â Ward</em>)</td>
<td>Choose KÂ â‰ˆÂ 1Â 000; HAC on 1Â 000Â centroids</td>
<td>Want both global &amp; local structure</td>
<td>âœ“ (K tunable)</td>
</tr>
<tr>
<td><strong>Scalable specialâ€‘purpose algorithms</strong> (BIRCH, CURE,
HDBSCAN*, OPTICS)</td>
<td>Works in streaming / chunks</td>
<td>Densityâ€‘based or balanced trees</td>
<td>âœ“ (Spark / Dask)</td>
</tr>
</tbody>
</table>
<h3 id="where-the-23cluster-collapse-fits">Where the 23â€‘cluster collapse
fits</h3>
<ul>
<li><strong>StepÂ 1</strong>â€ƒYou already have 23 disjoint groupsâ€”treat
each as a <em>prototype</em>.</li>
<li><strong>StepÂ 2</strong>â€ƒCompute their 23Â Ã—Â 150 centroid matrix.</li>
<li><strong>StepÂ 3</strong>â€ƒRun Wardâ€™s linkage (or complete) on
<em>that</em> matrix.<br />
<em>Memory &amp; CPU are negligible because nÂ =Â 23.</em></li>
</ul>
<p>Cutting the dendrogram at <em>kÂ =Â 6</em> rolls the 23 originals into
6 superâ€‘clusters without ever touching all 1.5Â M rows in a pairwise
fashion.</p>
<hr />
<h3 id="if-you-want-a-fresh-hierarchical-model-on-everyone">If you want
a <em>fresh</em> hierarchical model on everyone</h3>
<ol type="1">
<li><p><strong>Miniâ€‘batch Kâ€‘means first
(KÂ â‰ˆÂ 1Â 000â€“5Â 000).</strong><br />
<em>Each person</em> is still assigned a cluster quickly (O(n)), and you
get K centroids.</p></li>
<li><p><strong>HAC on the K centroids only.</strong><br />
MemoryÂ â‰ˆÂ (KÂ²)/2 â†’ with KÂ =Â 2Â 000 thatâ€™s ~16Â MB, trivial.</p></li>
<li><p><strong>Label propagation.</strong><br />
Every original record inherits the HAC label of its Kâ€‘means
centroid.</p></li>
</ol>
<p>This â€œKâ€‘meansâ€‘treeâ€ (also called bisecting Kâ€‘means in Spark) gives
you a genuine hierarchical segmentation while staying linear in n.</p>
<hr />
<h3 id="quick-sanity-checklist">Quick sanity checklist</h3>
<table>
<colgroup>
<col style="width: 48%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr>
<th>Question</th>
<th>Rule of thumb</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Are you clustering &gt;Â 100Â k rows?</strong></td>
<td>Donâ€™t run HAC directlyâ€”prototype first.</td>
</tr>
<tr>
<td><strong>Have preâ€‘existing clusters or centroids?</strong></td>
<td>Perfect: run HAC on those centroids.</td>
</tr>
<tr>
<td><strong>Need interpretable dendrogram on full data?</strong></td>
<td>Sample 2â€“5Â % max; label the rest postâ€‘hoc.</td>
</tr>
<tr>
<td><strong>Need singleâ€‘pass, streaming, or Sparkâ€‘native?</strong></td>
<td>Use BIRCH or bisecting Kâ€‘means (builtâ€‘in to PySpark).</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="next-action-items-for-your-project">Next action items for your
project</h3>
<ol type="1">
<li><p><strong>If you plan to use the vendorâ€™s 23 labels as a starting
point:</strong><br />
go ahead with the centroidâ€‘merge strategyâ€”itâ€™s instantaneous.</p></li>
<li><p><strong>If you want a fresh hierarchy from
scratch:</strong><br />
prototype the â€œminiâ€‘batch KÂ =Â 2Â 000 â†’ HACâ€ workflow on a 5Â % subset,
measure silhouette / Daviesâ€“Bouldin at different cut levels, and check
that the business KPIs still separate clearly.</p></li>
<li><p><strong>Need code samples or Spark snippets?</strong><br />
Let me know the environment (pandas vs.Â PySpark) and I can send a
readyâ€‘toâ€‘run example.</p></li>
</ol>
<p>That should clarify why hierarchical methods can still be on the
tableâ€”even with 1.5Â million rowsâ€”as long as you donâ€™t apply vanilla HAC
to the entire raw dataset.</p>
<h1 id="section-3">4</h1>
<p>Below is a <strong>single, endâ€‘toâ€‘end script</strong> (PythonÂ â‰¥Â 3.9)
that strings together every step you listed.<br />
Itâ€™s written for a <em>pandasÂ +Â scikitâ€‘learn</em> workflow and should
run asâ€‘is once you plug in your own dataâ€‘loading block.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">Segmentation utilities for 1.5Â Mâ€‘row customer file</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">=================================================</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">Assumptions</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">-----------</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">* Raw table ==&gt; pandas DataFrame `df`</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - `state`          : &#39;TN&#39; / &#39;CO&#39;</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - `vendor_cluster` : integer 0â€’22 (23 preâ€‘built labels)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - 150 feature columns, mixed numeric + categorical</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">* Keep the script modular: every â€œstepâ€ is wrapped in a function.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.  Imports &amp; helpers</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans, AgglomerativeClustering</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> SparsePCA</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway, chi2_contingency</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> linkage, fcluster</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sp</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.Â Â Effectâ€‘size audit: Î·Â² (numeric) &amp; CramÃ©râ€™sÂ V (categorical) vs. state</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eta_squared(a, b):</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Oneâ€‘way ANOVA Î·Â² for numeric feature `a` against binary factor `b`.&quot;&quot;&quot;</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> [a[b <span class="op">==</span> g] <span class="cf">for</span> g <span class="kw">in</span> np.unique(b)]</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    ss_between <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(g) <span class="op">*</span> (g.mean() <span class="op">-</span> a.mean()) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> g <span class="kw">in</span> groups)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    ss_total   <span class="op">=</span> ((a <span class="op">-</span> a.mean()) <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ss_between <span class="op">/</span> ss_total</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cramers_v(cat, state):</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;CramÃ©râ€™sÂ V for two categoricals.&quot;</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    confusion <span class="op">=</span> pd.crosstab(cat, state).values</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    chi2 <span class="op">=</span> chi2_contingency(confusion, correction<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> confusion.<span class="bu">sum</span>()</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    r, k <span class="op">=</span> confusion.shape</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sqrt(chi2 <span class="op">/</span> (n <span class="op">*</span> (<span class="bu">min</span>(r, k) <span class="op">-</span> <span class="dv">1</span>)))</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> effect_size_vs_state(df, numeric_cols, cat_cols):</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> []</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> numeric_cols:</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        res.append({<span class="st">&#39;feature&#39;</span>: col,</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;type&#39;</span>: <span class="st">&#39;numeric&#39;</span>,</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;eta2&#39;</span>: eta_squared(df[col].values, df[<span class="st">&#39;state&#39;</span>].values)})</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> cat_cols:</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        res.append({<span class="st">&#39;feature&#39;</span>: col,</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;type&#39;</span>: <span class="st">&#39;categorical&#39;</span>,</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;cramersV&#39;</span>: cramers_v(df[col].values, df[<span class="st">&#39;state&#39;</span>].values)})</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(res)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 2.Â Â Pilot clustering on 1Â % stratified sample (ignore state)</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pilot_clustering(df, num_cols, cat_cols, sample_frac<span class="op">=</span><span class="fl">0.01</span>, k_range<span class="op">=</span><span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">13</span>)):</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    samp <span class="op">=</span> df.groupby(<span class="st">&#39;state&#39;</span>, group_keys<span class="op">=</span><span class="va">False</span>).<span class="bu">apply</span>(</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span>sample_frac, random_state<span class="op">=</span><span class="dv">42</span>))</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># basic preprocessing</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;num&#39;</span>, StandardScaler(), num_cols),</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;cat&#39;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pre.fit_transform(samp)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>    best_k, best_score <span class="op">=</span> <span class="va">None</span>, <span class="op">-</span>np.inf</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>, batch_size<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> km.fit_predict(X)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        sil <span class="op">=</span> silhouette_score(X, labels, sample_size<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;k=</span><span class="sc">{</span>k<span class="sc">:2d}</span><span class="ss">, silhouette=</span><span class="sc">{</span>sil<span class="sc">:0.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sil <span class="op">&gt;</span> best_score:</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>            best_k, best_score <span class="op">=</span> k, sil</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_k, best_score</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a><span class="co"># 3.Â Â Unsupervised dimensionality reduction: varianceâ€‘filter â†’ sparseÂ PCA</span></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a><span class="co">#     (Autoencoder hook left as </span><span class="al">TODO</span><span class="co"> if you prefer DL)</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> unsupervised_reducer(n_components<span class="op">=</span><span class="dv">30</span>, vt_thresh<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Pipeline([</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;vt&#39;</span>, VarianceThreshold(threshold<span class="op">=</span>vt_thresh)),  <span class="co"># drop nearâ€‘zeroâ€‘var cols</span></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;spca&#39;</span>, SparsePCA(n_components<span class="op">=</span>n_components, random_state<span class="op">=</span><span class="dv">42</span>)),</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;sc&#39;</span>, StandardScaler())  <span class="co"># optional: scale the components</span></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.Â Â Vendor 23â€‘cluster rollâ€‘up to 6 via HAC on centroids</span></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collapse_vendor_clusters(df, feature_cols):</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> df.groupby(<span class="st">&#39;vendor_cluster&#39;</span>)[feature_cols].mean().values  <span class="co"># 23Ã—150</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> linkage(centroids, method<span class="op">=</span><span class="st">&#39;ward&#39;</span>)          <span class="co"># hierarchical on 23 points</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    labels_6 <span class="op">=</span> fcluster(Z, t<span class="op">=</span><span class="dv">6</span>, criterion<span class="op">=</span><span class="st">&#39;maxclust&#39;</span>)  <span class="co"># array(size=23)</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">enumerate</span>(labels_6, start<span class="op">=</span><span class="dv">0</span>))   <span class="co"># {0:2, 1:5, â€¦}</span></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&#39;vendor_cluster_6&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;vendor_cluster&#39;</span>].<span class="bu">map</span>(mapping)</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df, mapping</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="co"># 5.Â Â HAC on 50Â k sample  â†’Â Â train classifier to label remaining 1.45Â M</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stratified_HAC_then_classifier(df, num_cols, cat_cols,</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>                                   sample_n<span class="op">=</span><span class="dv">50_000</span>,</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>                                   hac_linkage<span class="op">=</span><span class="st">&#39;ward&#39;</span>,</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>                                   classifier<span class="op">=</span><span class="st">&#39;rf&#39;</span>,</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>                                   n_clusters<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.1Â Â Draw stratified sample</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>    samp <span class="op">=</span> df.groupby(<span class="st">&#39;state&#39;</span>, group_keys<span class="op">=</span><span class="va">False</span>).<span class="bu">apply</span>(</span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> x: x.sample(n<span class="op">=</span><span class="bu">min</span>(sample_n <span class="op">//</span> <span class="dv">2</span>, <span class="bu">len</span>(x)), random_state<span class="op">=</span><span class="dv">7</span>))</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>    rest <span class="op">=</span> df.drop(samp.index)</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.2Â Â Preprocess (same transformer for both)</span></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;num&#39;</span>, StandardScaler(), num_cols),</span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;cat&#39;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>    ], sparse_threshold<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>    X_samp <span class="op">=</span> pre.fit_transform(samp)</span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.3Â Â HAC on sample</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>    hac <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span>n_clusters, linkage<span class="op">=</span>hac_linkage)</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>    samp_labels <span class="op">=</span> hac.fit_predict(X_samp.toarray() <span class="cf">if</span> sp.issparse(X_samp) <span class="cf">else</span> X_samp)</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>    samp[<span class="st">&#39;hac_label&#39;</span>] <span class="op">=</span> samp_labels</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.4Â Â Train classifier</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> (RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">400</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>           <span class="cf">if</span> classifier <span class="op">==</span> <span class="st">&#39;rf&#39;</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>           <span class="cf">else</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">25</span>))</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>    clf.fit(X_samp, samp_labels)</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.5Â Â Predict labels for the rest</span></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>    X_rest <span class="op">=</span> pre.transform(rest)</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>    rest[<span class="st">&#39;hac_label&#39;</span>] <span class="op">=</span> clf.predict(X_rest)</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.6Â Â Reâ€‘assemble</span></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.concat([samp, rest]).sort_index()</span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a><span class="co"># 6.Â Â â”€â”€ Put it all together</span></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘A Â Load your 1.5Â Mâ€‘row DataFrame here  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df = pd.read_parquet(&quot;customers.parquet&quot;)  # &lt;â€‘â€‘ EXAMPLE</span></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">SystemExit</span>(<span class="st">&quot;ğŸ”´Â Load `df` before running the pipeline.&quot;</span>)</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tag numeric vs categorical columns</span></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>    excluded <span class="op">=</span> {<span class="st">&#39;state&#39;</span>, <span class="st">&#39;vendor_cluster&#39;</span>}</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>    num_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.select_dtypes(include<span class="op">=</span><span class="st">&#39;number&#39;</span>) <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> excluded]</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>    cat_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.columns <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> num_cols <span class="kw">and</span> c <span class="kw">not</span> <span class="kw">in</span> excluded]</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘B Â Effectâ€‘size audit</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>    audit <span class="op">=</span> effect_size_vs_state(df, num_cols, cat_cols)</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>    audit.to_csv(<span class="st">&quot;effect_size_vs_state.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘C Â Pilot clustering on 1Â % sample</span></span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>    best_k, _ <span class="op">=</span> pilot_clustering(df, num_cols, cat_cols)</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘D Â Dimâ€‘reduced features (example: 30 sparseâ€‘PCA comps)</span></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>    reducer <span class="op">=</span> unsupervised_reducer(n_components<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>    <span class="co"># reducer.fit_transform(...)  â† plug into your real pipeline</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘E Â Collapse vendor clusters 23â†’6</span></span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>    df, mapping23to6 <span class="op">=</span> collapse_vendor_clusters(df, num_cols)</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘F Â HAC on 50Â k sample â†’ classifier for rest</span></span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> stratified_HAC_then_classifier(df, num_cols, cat_cols)</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6â€‘G Â Save or push `df` with new label columns wherever you need</span></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>    df.to_parquet(<span class="st">&quot;customers_with_segments.parquet&quot;</span>)</span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;âœ…Â Pipeline completed.&quot;</span>)</span></code></pre></div>
<h3 id="how-to-adapt">How to adapt:</h3>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr>
<th>Section</th>
<th>What you might change</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data load block</strong></td>
<td>Replace the placeholder at the bottom with your actual file
read.</td>
</tr>
<tr>
<td><strong>Feature lists</strong></td>
<td>If you have text or date columns, preprocess (TFâ€‘IDF, age) first,
then add to <code>num_cols</code> or <code>cat_cols</code>.</td>
</tr>
<tr>
<td><strong>Reducer</strong></td>
<td>Swap <code>SparsePCA</code> for an autoâ€‘encoder: build your Keras
model, then change the <code>unsupervised_reducer()</code> step.</td>
</tr>
<tr>
<td><strong>Classifier</strong></td>
<td>Use <code>classifier='knn'</code> if you prefer kâ€‘NN over Random
Forest.</td>
</tr>
<tr>
<td><strong>Cluster counts</strong></td>
<td>Tune <code>n_clusters</code> in both pilot and HAC sections to
whatever ceiling management sets.</td>
</tr>
</tbody>
</table>
<p>Run the script endâ€‘toâ€‘end once; youâ€™ll get:</p>
<ul>
<li><p><code>effect_size_vs_state.csv</code>â€ƒâ€“ a table of Î·Â² or
CramÃ©râ€™sÂ V per feature.</p></li>
<li><p><code>customers_with_segments.parquet</code>â€ƒâ€“ full 1.5Â M rows,
now with:</p>
<ul>
<li><code>vendor_cluster_6</code> (collapsed vendor labels)</li>
<li><code>hac_label</code> (fresh 50Â kâ€‘sample HAC propagated to all
records)</li>
</ul></li>
</ul>
<p>Feel free to drop any section you donâ€™t need, or ping me if youâ€™d
like the autoâ€‘encoder block fleshed out.</p>
<h1 id="section-4">4</h1>
<p>Below is the <strong>â€œfromâ€‘scratchâ€Â pipeline I would stand up if I
owned your project</strong>.<br />
It keeps everything in a single, reproducible Python file (call it
<code>seg_pipeline.py</code>).<br />
Key design goals:</p>
<ul>
<li><strong>Scales to 1.5Â MÂ rows on a single beefy workstation</strong>
(32â€‘64Â GB RAM) by
<ul>
<li>sampling for expensive diagnostics,</li>
<li>using MiniBatch algorithms,</li>
<li>working in sparse format when possible.</li>
</ul></li>
<li><strong>Allâ€‘inâ€‘one</strong> â€“ run it once and you get:
<ul>
<li>an effectâ€‘size audit (<code>audit_vs_state.csv</code>),</li>
<li>a pilotâ€‘clustering report (<code>pilot_metrics.csv</code>),</li>
<li>collapsed vendor labels (23Â â†’Â 6),</li>
<li>a brandâ€‘new 6â€‘cluster solution on the full data,</li>
<li>postâ€‘hoc â€œliftâ€ tables for cluster description,</li>
<li>a single Parquet with every personâ€™s final labels.</li>
</ul></li>
<li><strong>Modular</strong> â€“ each stage is a function; you can comment
out any step.</li>
</ul>
<hr />
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">seg_pipeline.py  Â·  Endâ€‘toâ€‘end customer segmentation</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">====================================================</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">Usage</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">-----</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">$ python seg_pipeline.py --input customers.parquet </span><span class="ch">\</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">                         --output_dir results </span><span class="ch">\</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">                         --n_components 30 </span><span class="ch">\</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">                         --target_clusters 6</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 0. Imports &amp; CLI</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> argparse, os, logging, json, math</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans, AgglomerativeClustering</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score, davies_bouldin_score</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> linkage, fcluster</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Helpers  Â·  Effect sizes</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eta_squared(x, groups):</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> [x[groups <span class="op">==</span> g] <span class="cf">for</span> g <span class="kw">in</span> np.unique(groups)]</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    ss_between <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(g) <span class="op">*</span> (g.mean() <span class="op">-</span> x.mean()) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> g <span class="kw">in</span> groups)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    ss_total <span class="op">=</span> ((x <span class="op">-</span> x.mean()) <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ss_between <span class="op">/</span> ss_total <span class="cf">if</span> ss_total <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cramers_v(cat, state):</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    tab <span class="op">=</span> pd.crosstab(cat, state).values</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    chi2 <span class="op">=</span> chi2_contingency(tab, correction<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> tab.<span class="bu">sum</span>()</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    r, k <span class="op">=</span> tab.shape</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> math.sqrt(chi2 <span class="op">/</span> (n <span class="op">*</span> (<span class="bu">min</span>(r, k) <span class="op">-</span> <span class="dv">1</span>))) <span class="cf">if</span> n <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> audit_vs_state(df, numeric_cols, cat_cols, out_csv):</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    rows <span class="op">=</span> []</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> numeric_cols:</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        rows.append({<span class="st">&quot;feature&quot;</span>: col,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;type&quot;</span>: <span class="st">&quot;num&quot;</span>,</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;eta2&quot;</span>: eta_squared(df[col].values, df[<span class="st">&quot;state&quot;</span>].values)})</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> cat_cols:</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        rows.append({<span class="st">&quot;feature&quot;</span>: col,</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;type&quot;</span>: <span class="st">&quot;cat&quot;</span>,</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;cramersV&quot;</span>: cramers_v(df[col].values, df[<span class="st">&quot;state&quot;</span>].values)})</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(rows).to_csv(out_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Pilot clustering on 1Â % stratified sample</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pilot_cluster(df, num_cols, cat_cols, k_range, out_csv):</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    samp <span class="op">=</span> df.groupby(<span class="st">&quot;state&quot;</span>, group_keys<span class="op">=</span><span class="va">False</span>)<span class="op">\</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>             .<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span><span class="dv">42</span>))</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;num&quot;</span>, StandardScaler(), num_cols),</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;cat&quot;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&quot;ignore&quot;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    ], n_jobs<span class="op">=-</span><span class="dv">1</span>, sparse_threshold<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pre.fit_transform(samp)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    records <span class="op">=</span> []</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        lbl <span class="op">=</span> km.fit_predict(X)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        sil <span class="op">=</span> silhouette_score(X, lbl, sample_size<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        db  <span class="op">=</span> davies_bouldin_score(X, lbl)</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        records.append({<span class="st">&quot;k&quot;</span>: k, <span class="st">&quot;silhouette&quot;</span>: sil, <span class="st">&quot;davies_bouldin&quot;</span>: db})</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(records).to_csv(out_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Dimensionality reduction block</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_reducer(vt_thresh, n_components):</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Pipeline([</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;vt&quot;</span>, VarianceThreshold(threshold<span class="op">=</span>vt_thresh)),</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;svd&quot;</span>, TruncatedSVD(n_components<span class="op">=</span>n_components, random_state<span class="op">=</span><span class="dv">2</span>)),</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;sc&quot;</span>,  StandardScaler())</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Collapse vendor clusters 23Â â†’Â k via HAC on centroids</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collapse_vendor(df, feature_cols, target_k):</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> df.groupby(<span class="st">&quot;vendor_cluster&quot;</span>)[feature_cols].mean().values</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> linkage(centroids, method<span class="op">=</span><span class="st">&quot;ward&quot;</span>)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>    map23 <span class="op">=</span> fcluster(Z, t<span class="op">=</span>target_k, criterion<span class="op">=</span><span class="st">&quot;maxclust&quot;</span>)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> {old: new <span class="cf">for</span> old, new <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="bu">len</span>(map23)), map23)}</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&quot;vendor_k</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(target_k)] <span class="op">=</span> df[<span class="st">&quot;vendor_cluster&quot;</span>].<span class="bu">map</span>(mapping)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mapping</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Fresh clustering on full data (dimâ€‘reduced)</span></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fresh_clustering(df, num_cols, cat_cols,</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>                     n_components, target_k, out_parquet):</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;num&quot;</span>, StandardScaler(), num_cols),</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;cat&quot;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&quot;ignore&quot;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    ], n_jobs<span class="op">=-</span><span class="dv">1</span>, sparse_threshold<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>    reducer <span class="op">=</span> make_reducer(<span class="fl">0.0</span>, n_components)</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>    pipe <span class="op">=</span> Pipeline([(<span class="st">&quot;pre&quot;</span>, pre), (<span class="st">&quot;red&quot;</span>, reducer)])</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    X_red <span class="op">=</span> pipe.fit_transform(df)      <span class="co"># sparse â†’ dense SVD(â€¦)</span></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>target_k,</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>                         random_state<span class="op">=</span><span class="dv">3</span>, batch_size<span class="op">=</span><span class="dv">4096</span>)</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&quot;new_cluster&quot;</span>] <span class="op">=</span> km.fit_predict(X_red)</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>    df.to_parquet(out_parquet)</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pipe, km</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Postâ€‘hoc lifts for description</span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lift_tables(df, cluster_col, numeric_cols, cat_cols, out_dir):</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>    grp <span class="op">=</span> df.groupby(cluster_col)</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>    base <span class="op">=</span> df.shape[<span class="dv">0</span>]</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>    Path(out_dir).mkdir(exist_ok<span class="op">=</span><span class="va">True</span>, parents<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numeric mean lift</span></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>    num_tbl <span class="op">=</span> pd.concat({</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>        c: (grp[c].mean() <span class="op">/</span> df[c].mean()) <span class="cf">for</span> c <span class="kw">in</span> numeric_cols</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>    }, axis<span class="op">=</span><span class="dv">1</span>).T</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>    num_tbl.to_csv(Path(out_dir) <span class="op">/</span> <span class="st">&quot;lift_numeric.csv&quot;</span>)</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># categorical overâ€‘/underâ€‘rep (% share)</span></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> cat_cols:</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>        tab <span class="op">=</span> pd.crosstab(df[cluster_col], df[c], normalize<span class="op">=</span><span class="st">&quot;index&quot;</span>)</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>        tab.to_csv(Path(out_dir) <span class="op">/</span> <span class="ss">f&quot;lift_</span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">.csv&quot;</span>)</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Main</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(args):</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> Path(args.output_dir)</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>    out.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>    logging.basicConfig(</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>        level<span class="op">=</span>logging.INFO,</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>        <span class="bu">format</span><span class="op">=</span><span class="st">&quot;</span><span class="sc">%(asctime)s</span><span class="st"> | </span><span class="sc">%(levelname)s</span><span class="st"> | </span><span class="sc">%(message)s</span><span class="st">&quot;</span>,</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>        handlers<span class="op">=</span>[logging.FileHandler(out <span class="op">/</span> <span class="st">&quot;run.log&quot;</span>),</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>                  logging.StreamHandler()])</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>    log <span class="op">=</span> logging.getLogger()</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;â–¶ Loading data â€¦&quot;</span>)</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_parquet(args.<span class="bu">input</span>)</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- column buckets ------------------------------------------------------</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>    ignore <span class="op">=</span> {<span class="st">&quot;state&quot;</span>, <span class="st">&quot;vendor_cluster&quot;</span>}</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>    num_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.select_dtypes(include<span class="op">=</span><span class="st">&quot;number&quot;</span>) <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> ignore]</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>    cat_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.columns <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> num_cols <span class="kw">and</span> c <span class="kw">not</span> <span class="kw">in</span> ignore]</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 1. Effectâ€‘size audit -----------------------------------------------</span></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;â–¶ Effectâ€‘size audit vs. state&quot;</span>)</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>    audit_vs_state(df, num_cols, cat_cols, out <span class="op">/</span> <span class="st">&quot;audit_vs_state.csv&quot;</span>)</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 2. Pilot clustering -------------------------------------------------</span></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;â–¶ Pilot clustering on 1Â </span><span class="sc">% s</span><span class="st">ample&quot;</span>)</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>    pilot_cluster(df, num_cols, cat_cols,</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>                  k_range<span class="op">=</span><span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">13</span>),</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>                  out_csv<span class="op">=</span>out <span class="op">/</span> <span class="st">&quot;pilot_metrics.csv&quot;</span>)</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 3. Collapse vendor clusters ----------------------------------------</span></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;â–¶ Collapsing vendor clusters â†’ </span><span class="sc">%d</span><span class="st"> groups&quot;</span>, args.target_clusters)</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> collapse_vendor(df, num_cols, args.target_clusters)</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>    json.dump(mapping, <span class="bu">open</span>(out <span class="op">/</span> <span class="st">&quot;vendor23_to_k.json&quot;</span>, <span class="st">&quot;w&quot;</span>))</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 4. Fresh clustering -------------------------------------------------</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;â–¶ Fresh MiniBatchÂ Kâ€‘means (</span><span class="sc">%d</span><span class="st"> comps â†’ k=</span><span class="sc">%d</span><span class="st">)&quot;</span>,</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>             args.n_components, args.target_clusters)</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>    pipe, km <span class="op">=</span> fresh_clustering(df, num_cols, cat_cols,</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>                                args.n_components,</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>                                args.target_clusters,</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>                                out <span class="op">/</span> <span class="st">&quot;customers_with_clusters.parquet&quot;</span>)</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 5. Lift tables ------------------------------------------------------</span></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;â–¶ Computing lift tables for description&quot;</span>)</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>    lift_tables(df, <span class="st">&quot;new_cluster&quot;</span>, num_cols, cat_cols, out <span class="op">/</span> <span class="st">&quot;lifts&quot;</span>)</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;âœ… Done.  Results in </span><span class="sc">%s</span><span class="st">&quot;</span>, out)</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a><span class="co"># â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> argparse.ArgumentParser()</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--input&quot;</span>, required<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">help</span><span class="op">=</span><span class="st">&quot;Parquet file with raw data (1.5Â M rows)&quot;</span>)</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--output_dir&quot;</span>, default<span class="op">=</span><span class="st">&quot;results&quot;</span>)</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--n_components&quot;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">help</span><span class="op">=</span><span class="st">&quot;SVD dimensions before clustering&quot;</span>)</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--target_clusters&quot;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">help</span><span class="op">=</span><span class="st">&quot;Desired number of final clusters&quot;</span>)</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> p.parse_args()</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>    main(args)</span></code></pre></div>
<hr />
<h3 id="why-this-script-hits-the-sweetspot">Why this script hits the
sweetâ€‘spot</h3>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 84%" />
</colgroup>
<thead>
<tr>
<th>Concern</th>
<th>How the script addresses it</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Memory / speed</strong></td>
<td>MiniBatchÂ Kâ€‘means, sparse encoders, TruncatedÂ SVD; heavy
calculations run on 1Â % or 50Â k samples only.</td>
</tr>
<tr>
<td><strong>Vendor label reuse</strong></td>
<td><code>collapse_vendor()</code> gives you a defensible 6â€‘group
rollâ€‘up without reâ€‘clustering 1.5Â M rows.</td>
</tr>
<tr>
<td><strong>Fresh segmentation</strong></td>
<td><code>fresh_clustering()</code> creates an allâ€‘new solution in the
same run, so you can A/B them.</td>
</tr>
<tr>
<td><strong>Business story</strong></td>
<td><code>lift_tables()</code> autoâ€‘exports CSVs showing which features
are over/underâ€‘represented in each clusterâ€”perfect for slideâ€‘ware.</td>
</tr>
<tr>
<td><strong>Reproducibility</strong></td>
<td>One command, deterministic RNG seeds, all outputs timestamped.</td>
</tr>
</tbody>
</table>
<p>Tweak component counts, cluster counts, or swap in an autoâ€‘encoder by
replacing <code>make_reducer()</code> with your Keras modelâ€”everything
else stays the same.</p>
<p>Run:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> seg_pipeline.py <span class="at">--input</span> customers.parquet <span class="at">--output_dir</span> seg_results</span></code></pre></div>
<p>and inspect the logs plus the files under
<code>seg_results/</code>.</p>
<p>Let me know if youâ€™d like any part (e.g., Spark adaptation,
autoâ€‘encoder example, richer KPI lift charts) fleshed out further!</p>
<h1 id="section-5">5</h1>
<h3 id="can-zip-codes-be-used-for-clustering">Can ZIP codes be used for
clustering?</h3>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 35%" />
<col style="width: 24%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>Option</th>
<th>How youâ€™d engineer it</th>
<th>When it helps</th>
<th>Watchâ€‘outs</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Raw 5â€‘digit ZIP as a categorical</strong></td>
<td>Oneâ€‘hot (â‰ˆâ€¯42â€¯k columns) or target / frequency encode</td>
<td>Almost neverâ€”the code itself has no ordinal meaning and explodes
dimensionality</td>
<td>Curse of dimensionality; distance metrics treat two adjacent ZIPs as
â€œmaximally differentâ€</td>
</tr>
<tr>
<td><strong>Lat /â€¯lon centroid</strong></td>
<td>Map each ZIP to its latitude &amp; longitude, standardize, feed as
two numeric features</td>
<td>When geographic proximity is the signal you care about (e.g.,
service distance, weather)</td>
<td><em>Kâ€‘means</em>â€™ Euclidean distance on lat/lon is only approximate;
consider Haversine or convert to UTM</td>
</tr>
<tr>
<td><strong>Derived geodemographics</strong></td>
<td>Join external tables: rural/urban flag, county poverty rate, median
HH income, population density, Nielsen PRIZM cluster, etc.</td>
<td>When you suspect socioeconomic context drives behavior</td>
<td>Adds real signal without exploding feature space</td>
</tr>
<tr>
<td><strong>Spatial embeddings / distance matrices</strong></td>
<td>Preâ€‘compute adjacency graph or distanceâ€‘toâ€‘nearestâ€‘N ZIPs and use
graphâ€‘based clustering (HDBSCAN, Leiden)</td>
<td>When true â€œneighborhoodsâ€ matter more than city/state lines</td>
<td>Heavier engineering; harder to explain to nonâ€‘technical
stakeholders</td>
</tr>
</tbody>
</table>
<p><strong>What Iâ€™d do in your project</strong></p>
<ol type="1">
<li><strong>Add lat &amp; lon</strong> for every record (two numeric
columns).</li>
<li>Optional: pull in <strong>one or two contextual scores</strong>
(median income, rural/urban).</li>
<li>Skip the raw ZIP code column during clustering; keep it only for
postâ€‘hoc descriptions like â€œClusterâ€¯3 skews toward Denver suburbs.â€</li>
</ol>
<hr />
<h3 id="what-about-an-occupation-variable-with-40-categories">What about
an occupation variable withâ€¯&gt;â€¯40 categories?</h3>
<p>Fortyâ€‘odd categories is manageable, but think through these
angles:</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 31%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr>
<th>Technique</th>
<th>Pros</th>
<th>Cons / mitigations</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Oneâ€‘hot encode all 40</strong></td>
<td>Simple, transparent; 40 extra columns is tiny vs.Â 150 base
features</td>
<td>Treats occupations as independent; can dilute distances if many
lowâ€‘freq jobs â†’ consider frequencyâ€‘based pruning (â€œOtherâ€ bucket for
categories &lt;â€¯1â€¯% of sample).</td>
</tr>
<tr>
<td><strong>Group into major SOC codes</strong> (e.g., 10â€‘15
sectors)</td>
<td>Easier to interpret (â€œHealthcareâ€, â€œConstructionâ€)</td>
<td>You lose granularity; do this only if stakeholders want higherâ€‘level
personas.</td>
</tr>
<tr>
<td><strong>Ordinal encode by median income or job
prestige</strong></td>
<td>Injects business meaning; keeps it 1D</td>
<td>Assumes monotonic effect that may not exist; fine if income is the
real driver and you already have salary bands.</td>
</tr>
<tr>
<td><strong>Entity embeddings</strong> (learned in a shallow neural net
before clustering)</td>
<td>Captures similarity between occupations (e.g., â€œnurseâ€ closer to
â€œphysician assistantâ€ than to â€œtruck driverâ€)</td>
<td>Extra engineering; you need TensorFlow/PyTorch loop and to freeze
the embedding before clustering.</td>
</tr>
<tr>
<td><strong>Weightâ€‘ofâ€‘Evidence / target encoding</strong> (if you have a
KPI)</td>
<td>Squeezes information into a single numeric with predictive
power</td>
<td>Risk of leakage unless you use outâ€‘ofâ€‘fold encodings; ties the
clustering to that KPI.</td>
</tr>
</tbody>
</table>
<p><strong>Practical pick</strong><br />
<em>With 1.5â€¯M rows,</em> oneâ€‘hot encoding 40 categories usually works
fine and keeps the pipeline simple. Drop the few occupations with
&lt;â€¯1â€¯% frequency into an â€œOtherâ€ bucket to avoid tiny dummy columns.
If interpretability is paramount, map the 40 codes to 10â€“15 SOC
superâ€‘groups <strong>and</strong> keep the original fineâ€‘grained codes
for postâ€‘hoc profiling.</p>
<hr />
<h3
id="recommended-minimal-additions-to-the-pipeline-you-already-have">Recommended
minimal additions to the pipeline you already have</h3>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After loading df â€¦</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ---- Zipâ€‘code engineering -------------------------------------------</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>zip_meta <span class="op">=</span> pd.read_csv(<span class="st">&quot;zip_lat_lon_income.csv&quot;</span>)  <span class="co"># ZIP, lat, lon, median_income</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.merge(zip_meta, on<span class="op">=</span><span class="st">&quot;zip&quot;</span>, how<span class="op">=</span><span class="st">&quot;left&quot;</span>)     <span class="co"># adds &#39;lat&#39;, &#39;lon&#39;, &#39;zip_income&#39;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>num_cols.extend([<span class="st">&#39;lat&#39;</span>, <span class="st">&#39;lon&#39;</span>, <span class="st">&#39;zip_income&#39;</span>])     <span class="co"># use in clustering</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ---- Occupation oneâ€‘hots ---------------------------------------------</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>occ_freq <span class="op">=</span> df[<span class="st">&#39;occupation&#39;</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>rare <span class="op">=</span> occ_freq[occ_freq <span class="op">&lt;</span> <span class="fl">0.01</span>].index</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;occupation_mod&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;occupation&#39;</span>].where(<span class="op">~</span>df[<span class="st">&#39;occupation&#39;</span>].isin(rare), <span class="st">&#39;Other&#39;</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>cat_cols.append(<span class="st">&#39;occupation_mod&#39;</span>)                 <span class="co"># treated by OneHotEncoder</span></span></code></pre></div>
<p>This keeps the clustering engine lightweight, injects meaningful
geography and occupation signal, and still allows you to say in the
final deck:</p>
<blockquote>
<p>â€œClusterÂ 4: younger professionals, Denverâ€‘metro ZIPs, tech &amp;
healthcare occupations, median incomeÂ $78â€¯k.â€</p>
</blockquote>
<p>Let me know if youâ€™d like code for a geospatial distance kernel or
embeddingâ€‘based occupation encoding!</p>
    
</body>
</html>