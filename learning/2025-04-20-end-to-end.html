<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1 id="section">1</h1>
<p>Below is a practical playbook you can adapt. I’ll start with the “one
big model vs. two state‑specific models” question, then lay out a
workflow for feature reduction, picking the number of clusters, and the
“describe‑after‑you‑cluster” stage.</p>
<hr />
<h2 id="should-tennessee-and-colorado-be-clustered-together">1  Should
Tennessee and Colorado be clustered together?</h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 36%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr>
<th>What to check</th>
<th>How to check it (quick tests)</th>
<th>What the result tells you</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>How separable are the two states?</strong></td>
<td>• Train any fast classifier (e.g., gradient‑boosted trees) to
predict <em>state</em> from <em>all 150 features</em>.<br>• Look at
cross‑validated AUC / accuracy.</td>
<td><strong>AUC ≈ 1</strong> → The states already live in different
parts of feature‑space; joint clustering will mostly recreate “state” as
the first split.<br><strong>AUC ≈ 0.6–0.7</strong> → States overlap a
lot; joint clustering is more defensible.</td>
</tr>
<tr>
<td><strong>Variance captured by state vs. everything else</strong></td>
<td>• Fit a one‑way ANOVA or, for mixed types, compute η² / Cramér’s V
for <em>each</em> feature against <em>state</em>.<br>• Summarize the
distribution of those effect sizes.</td>
<td>If many features have large η² or V (&gt; 0.14 ≈ “large”), state
differences dominate the data – consider splitting.</td>
</tr>
<tr>
<td><strong>Cluster stability across states</strong> (only takes
minutes)</td>
<td>• Run a pilot clustering on a 1% stratified sample <em>ignoring
state</em>.<br>• Cross‑tab clusters × state.<br>• Compute Adjusted Rand
vs. state.</td>
<td>If clusters almost map 1‑for‑1 to TN vs. CO, you’ve learned that
“state” is the natural first‑level segmentation, so run separate models
or take a hierarchical approach (see below).</td>
</tr>
</tbody>
</table>
<p><strong>Rules of thumb</strong></p>
<ul>
<li><p><strong>If “state” explains ≫ 30 % of total variance (or you can
predict state with ≳ 90 % accuracy), do <em>not</em> force one joint
model.</strong><br />
You’ll waste the 5 remaining clusters on rediscovering the
obvious.</p></li>
<li><p><strong>If state explains only a modest slice of variance, or you
<em>want</em> cross‑state personas (e.g., marketing campaigns that apply
in both markets), keep one model but still make sure your post‑hoc
cluster descriptions include the state breakdown.</strong></p></li>
<li><p><strong>Compromise option:</strong> hierarchical or multi‑level
clustering</p>
<ol type="1">
<li>Split by state, find <em>within‑state</em> clusters (say 3
each).</li>
<li>Pool the cluster centroids and re‑cluster them (3 + 3 → ≤ 6 total
personas).<br />
This often yields interpretable personas that retain local nuance.</li>
</ol></li>
</ul>
<hr />
<h2 id="feature-reduction-before-clustering">2  Feature reduction before
clustering</h2>
<p>You’re right: 150 dimensions + 6 clusters is asking for “snowball
clusters” that mix unrelated people. A proven two‑step recipe:</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 56%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th>Step</th>
<th>What to do</th>
<th>Tips</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2.1  Structural filtering (get from
150 → ≈ 70)</strong></td>
<td>• Remove near‑zero variance columns.<br>• Collapse one‑hot groups
with rare levels into “other”.<br>• Drop highly collinear pairs
(│ρ│ &gt; 0.9).</td>
<td>Keep a log of everything you drop – stakeholders always ask.</td>
</tr>
<tr>
<td><strong>2.2  Noise &amp; redundancy filtering
(≈ 70 → 20–40)</strong></td>
<td>• Unsupervised: use variance threshold, sparse PCA, or autoencoder
bottleneck.<br>• Semi‑supervised: calculate mutual information with a
high‑level business target (e.g., spend, churn) and keep top‑k features
<em>if</em> that aligns with goals.</td>
<td>You need enough features to <em>build</em> clusters (20–40 works
well), but you can—and often should—describe them later with the full
150.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="choosing-6-clusters-that-are-still-meaningful">3  Choosing
<strong>≤ 6</strong> clusters that are still meaningful</h2>
<ol type="1">
<li><strong>Start from the data, not the mandate.</strong><br />
Compute internal metrics (silhouette, Davies–Bouldin, gap statistic) for
<em>k</em> = 2–12 on a 1 % sample with MiniBatch K‑means or GMM.</li>
<li><strong>Find the “elbow” / local optimum.</strong><br />
If the best <em>k</em> is 9 and your cap is 6, show the trade‑off graph
to decision‑makers. Often you’ll get approval for 7–8 once they see the
loss in cohesion.</li>
<li><strong>If you must stay at 6:</strong>
<ul>
<li>Run <em>k</em> = 6, then compute <em>within‑cluster</em> silhouettes
and entropy.</li>
<li>Split any low‑cohesion cluster post‑hoc into “sub‑segments” for
internal use while reporting the mandated 6 externally.</li>
</ul></li>
</ol>
<hr />
<h2 id="describing-profiling-the-clusters-stage-2">4  Describing /
profiling the clusters (Stage 2)</h2>
<ul>
<li>After locking the cluster labels, <strong>freeze the clustering
feature set</strong> and bring back <em>all</em> 150 variables for
description.</li>
<li>Popular, scalable descriptive tools:
<ul>
<li><strong>Target vs. overall mean / lift charts</strong> for numeric
features.</li>
<li><strong>Cramér’s V heatmap</strong> for categorical enrichment.</li>
<li><strong>One‑vs‑rest SHAP values</strong> from a tree model trained
to predict each cluster label—gives intuitive “what drives membership”
stories.</li>
</ul></li>
<li>Provide <strong>state mix (%) per cluster</strong> even if you ran
state‑specific models; execs love to see whether a segment is
“TN‑skewed” or “truly national.”</li>
</ul>
<hr />
<h2 id="putting-it-all-together-a-minimal-pipeline">5  Putting it all
together – a minimal pipeline</h2>
<pre class="text"><code>1. Stratified sample (1 %) → EDA + checks in §1.
2. Decide: joint vs. split vs. hierarchical.
3. Full data → preprocessing (§2) → MiniBatch K‑means / GMM grid (k=2–12).
4. Pick k with metrics (§3) and business cap.
5. Fit final model on 100 % data → assign ids → store cluster_id.
6. Descriptive notebook (§4) → tables, graphs, narrative deck.
7. Validate over time (monthly refresh): stability index, new data drift.</code></pre>
<hr />
<h3 id="next-steps-for-you">Next steps for you</h3>
<ul>
<li><strong>Run the separability quick‑checks</strong> first; they
usually settle the “one vs. two models” debate fast.</li>
<li>If you’d like example code snippets (scikit‑learn / PySpark) or a
template for the descriptive notebook, let me know and I can draft
them.</li>
</ul>
<p>Good luck—once you pin down how much “state” matters, the rest of the
design decisions fall into place.</p>
<h1 id="section-1">2</h1>
<p>Absolutely—you <em>can</em> salvage value from those 23
“off‑the‑shelf” clusters, but you’ll want to treat them as raw material
rather than a finished product. Here are four practical ways teams fold
an inherited label set into a new, smaller segmentation, plus some quick
tests to decide whether collapsing to ≈ 6 groups is defensible.</p>
<hr />
<h2 id="audit-the-23cluster-set-first">1. Audit the 23‑cluster set
first</h2>
<p>Before you collapse anything, check three things:</p>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 26%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr>
<th>Check</th>
<th>How (fast)</th>
<th>Why it matters</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Internal quality</strong></td>
<td>Silhouette / Davies–Bouldin on the original 150 features.</td>
<td>If the 23 clusters themselves are shaky, merging will only make them
worse.</td>
</tr>
<tr>
<td><strong>Business relevance</strong></td>
<td>Cross‑tab the 23 labels with KPIs (spend tiers, churn).</td>
<td>You may discover only 10 of the 23 drive any outcome; those are the
candidates to keep distinct.</td>
</tr>
<tr>
<td><strong>State skew</strong></td>
<td><code>% Tennessee</code> vs. <code>% Colorado</code> per
cluster.</td>
<td>If some clusters are 95 % TN, others 95 % CO, you’re really looking
at a hidden “state × subcluster” design; that changes how you
merge.</td>
</tr>
</tbody>
</table>
<p>If quality is poor or heavily state‑biased, start fresh. Otherwise
pick one of the merge strategies below.</p>
<hr />
<h2 id="strategies-for-collapsing-23-6">2. Strategies for collapsing
23 → ~6</h2>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 50%" />
<col style="width: 17%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>Strategy</th>
<th>Steps</th>
<th>Good when…</th>
<th>Caveats</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hierarchical merging of centroids</strong></td>
<td>1  Compute 23 centroids in original feature
space.<br>2  Agglomerative clustering (Ward or complete) on those 23
points.<br>3  Cut the dendrogram at <em>k = 6</em>.</td>
<td>You trust the original geometry and just want a higher‑level
roll‑up.</td>
<td>If some original clusters are huge and others tiny, centroids can be
misleading—weight by cluster size.</td>
</tr>
<tr>
<td><strong>Outcome‑driven merge (supervised)</strong></td>
<td>1  Pick a business target.<br>2  Train a tree/GBM to predict the
target using <em>only</em> the 23‑cluster label.<br>3  Prune the tree so
it produces ≤ 6 leaves; each leaf is a merged group.</td>
<td>Management insists the segments predict a specific KPI.</td>
<td>You will ignore variance <em>not</em> related to that KPI.</td>
</tr>
<tr>
<td><strong>Graph‑based co‑membership</strong></td>
<td>1  Compute misclassification probabilities: for each pair (i,j), how
often do individuals switch between clusters across multiple re‑runs or
perturbations?<br>2  Build a similarity graph, then Louvain / Leiden to
detect ≈ 6 communities.</td>
<td>You can rerun the vendor’s algorithm or approximate it to get
co‑membership stats.</td>
<td>Requires multiple runs; heavy for 1.5 M rows unless you
subsample.</td>
</tr>
<tr>
<td><strong>Use labels as a <em>feature</em> rather than a
cluster</strong></td>
<td>One‑hot the 23‑label column, feed it (with other features) into your
own clustering; let the algorithm decide whether to honor or ignore
it.</td>
<td>You’re not sure yet whether to trust or toss the vendor labels.</td>
<td>The label may dominate unless you down‑weight it.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="how-to-test-whether-the-6group-merge-works">3. How to test
whether the 6‑group merge “works”</h2>
<ol type="1">
<li><strong>Stability</strong> – Jaccard similarity of group membership
when you re‑sample or re‑run the merge logic.</li>
<li><strong>Cohesion vs. separation</strong> – Compare silhouette of
6‑group solution to that of the full 23. A drop of ≤ 10 % is usually
acceptable.</li>
<li><strong>Business lift</strong> – Re‑check KPI differences. If the 6
groups still split KPIs cleanly (ANOVA <em>p</em> &lt; 0.05 or ≥ 10 %
lift), you’ve kept most of the signal.</li>
<li><strong>Interpretability</strong> – Can you write a two‑sentence
description of each merged group your stakeholders understand? If not,
reconsider the merge.</li>
</ol>
<hr />
<h2 id="practical-workflow-you-can-run-in-a-day">4. Practical workflow
you can run in a day</h2>
<pre class="text"><code>A.  Pull 100 k stratified rows  ➜ quick diagnostics (§1).
B.  Pick a merge strategy (start with centroid‑hierarchical).
C.  Implement on full 1.5 M:
    – compute centroids            (PySpark or pandas groupby‑mean)
    – scipy.cluster.hierarchy.linkage(…, method=&quot;ward&quot;)
    – scipy.cluster.hierarchy.fcluster(…, t=6)
D.  Assign new_group_id to every record via a lookup.
E.  Validate: silhouettes, KPI lift, state mix.
F.  If unhappy, try supervised merge or keep &gt; 6 groups and
    negotiate with stakeholders (show the diagnostic plots).</code></pre>
<hr />
<h2 id="when-to-discard-the-vendor-labels-altogether">5. When to
<em>discard</em> the vendor labels altogether</h2>
<ul>
<li>They were built on a radically different feature set than
yours.</li>
<li>Most clusters are tiny (≤ 1 % of population) or amorphous
(silhouette &lt; 0).</li>
<li>They embed a business logic you no longer care about (e.g., “catalog
buyers” in a mostly‑digital world).</li>
</ul>
<p>In those cases, treat the 23‑label column as historical metadata
only—use it for back‑testing and storytelling but not for the new
segmentation.</p>
<hr />
<h3 id="where-this-leaves-you">Where this leaves you</h3>
<ol type="1">
<li><strong>Run the quick audit</strong> in §1 to gauge label quality
and state bias.</li>
<li>If quality ≥ “good enough,” prototype the centroid merge; it’s the
fastest path to a 6‑group roll‑up you can show tomorrow.</li>
<li>Keep the diagnostic plots handy—if stakeholders push back, you’ll
have evidence for why 6 groups is (or isn’t) defensible.</li>
</ol>
<p>Let me know if you’d like code snippets for the hierarchical merge or
for the KPI‑driven tree pruning, and I can paste them.</p>
<h1 id="section-2">3</h1>
<p>You’re absolutely right that <em>vanilla</em> hierarchical
agglomerative clustering (HAC) on the <strong>raw 1.5 million
rows</strong> is a non‑starter:</p>
<ul>
<li><strong>Time complexity</strong> ≈ O(n²) distance computations →
(1.5 M)² ≈ 2.3 × 10¹² distances.</li>
<li><strong>Memory</strong> The condensed distance matrix alone would be
~17 TB (at 8 bytes / float).</li>
</ul>
<p>So, when I spoke about “hierarchical merging,” I meant
<em>hierarchical‑on‑prototypes</em>, <strong>not</strong> “run HAC on
every record.” Here are the scalable variations that <em>are</em>
feasible:</p>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 39%" />
<col style="width: 15%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr>
<th>What actually runs</th>
<th>Data size HAC sees</th>
<th>Typical use‑case</th>
<th>Feasible on 1.5 M?</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Centroid / prototype merge</strong> (what I suggested for
the 23 vendor clusters)</td>
<td>23 centroids → HAC on 23 × 150 matrix</td>
<td>Collapsing an existing label set</td>
<td>✓ instant</td>
</tr>
<tr>
<td><strong>Sample‑then‑assign</strong></td>
<td>HAC on a stratified sample (e.g., 50 k) ➜ train a classifier (k‑NN,
tree) to label the remaining 1.45 M</td>
<td>Need full dendrogram for interpretability</td>
<td>✓ (sample)</td>
</tr>
<tr>
<td><strong>Two‑stage “mini‑Batch K‑means → HAC on K centroids”</strong>
(aka “K‑means‑tree” or <em>make_blobs → Ward</em>)</td>
<td>Choose K ≈ 1 000; HAC on 1 000 centroids</td>
<td>Want both global &amp; local structure</td>
<td>✓ (K tunable)</td>
</tr>
<tr>
<td><strong>Scalable special‑purpose algorithms</strong> (BIRCH, CURE,
HDBSCAN*, OPTICS)</td>
<td>Works in streaming / chunks</td>
<td>Density‑based or balanced trees</td>
<td>✓ (Spark / Dask)</td>
</tr>
</tbody>
</table>
<h3 id="where-the-23cluster-collapse-fits">Where the 23‑cluster collapse
fits</h3>
<ul>
<li><strong>Step 1</strong> You already have 23 disjoint groups—treat
each as a <em>prototype</em>.</li>
<li><strong>Step 2</strong> Compute their 23 × 150 centroid matrix.</li>
<li><strong>Step 3</strong> Run Ward’s linkage (or complete) on
<em>that</em> matrix.<br />
<em>Memory &amp; CPU are negligible because n = 23.</em></li>
</ul>
<p>Cutting the dendrogram at <em>k = 6</em> rolls the 23 originals into
6 super‑clusters without ever touching all 1.5 M rows in a pairwise
fashion.</p>
<hr />
<h3 id="if-you-want-a-fresh-hierarchical-model-on-everyone">If you want
a <em>fresh</em> hierarchical model on everyone</h3>
<ol type="1">
<li><p><strong>Mini‑batch K‑means first
(K ≈ 1 000–5 000).</strong><br />
<em>Each person</em> is still assigned a cluster quickly (O(n)), and you
get K centroids.</p></li>
<li><p><strong>HAC on the K centroids only.</strong><br />
Memory ≈ (K²)/2 → with K = 2 000 that’s ~16 MB, trivial.</p></li>
<li><p><strong>Label propagation.</strong><br />
Every original record inherits the HAC label of its K‑means
centroid.</p></li>
</ol>
<p>This “K‑means‑tree” (also called bisecting K‑means in Spark) gives
you a genuine hierarchical segmentation while staying linear in n.</p>
<hr />
<h3 id="quick-sanity-checklist">Quick sanity checklist</h3>
<table>
<colgroup>
<col style="width: 48%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr>
<th>Question</th>
<th>Rule of thumb</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Are you clustering &gt; 100 k rows?</strong></td>
<td>Don’t run HAC directly—prototype first.</td>
</tr>
<tr>
<td><strong>Have pre‑existing clusters or centroids?</strong></td>
<td>Perfect: run HAC on those centroids.</td>
</tr>
<tr>
<td><strong>Need interpretable dendrogram on full data?</strong></td>
<td>Sample 2–5 % max; label the rest post‑hoc.</td>
</tr>
<tr>
<td><strong>Need single‑pass, streaming, or Spark‑native?</strong></td>
<td>Use BIRCH or bisecting K‑means (built‑in to PySpark).</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="next-action-items-for-your-project">Next action items for your
project</h3>
<ol type="1">
<li><p><strong>If you plan to use the vendor’s 23 labels as a starting
point:</strong><br />
go ahead with the centroid‑merge strategy—it’s instantaneous.</p></li>
<li><p><strong>If you want a fresh hierarchy from
scratch:</strong><br />
prototype the “mini‑batch K = 2 000 → HAC” workflow on a 5 % subset,
measure silhouette / Davies–Bouldin at different cut levels, and check
that the business KPIs still separate clearly.</p></li>
<li><p><strong>Need code samples or Spark snippets?</strong><br />
Let me know the environment (pandas vs. PySpark) and I can send a
ready‑to‑run example.</p></li>
</ol>
<p>That should clarify why hierarchical methods can still be on the
table—even with 1.5 million rows—as long as you don’t apply vanilla HAC
to the entire raw dataset.</p>
<h1 id="section-3">4</h1>
<p>Below is a <strong>single, end‑to‑end script</strong> (Python ≥ 3.9)
that strings together every step you listed.<br />
It’s written for a <em>pandas + scikit‑learn</em> workflow and should
run as‑is once you plug in your own data‑loading block.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">Segmentation utilities for 1.5 M‑row customer file</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">=================================================</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">Assumptions</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">-----------</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">* Raw table ==&gt; pandas DataFrame `df`</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - `state`          : &#39;TN&#39; / &#39;CO&#39;</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - `vendor_cluster` : integer 0‒22 (23 pre‑built labels)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - 150 feature columns, mixed numeric + categorical</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">* Keep the script modular: every “step” is wrapped in a function.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.  Imports &amp; helpers</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans, AgglomerativeClustering</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> SparsePCA</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f_oneway, chi2_contingency</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> linkage, fcluster</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sp</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.  Effect‑size audit: η² (numeric) &amp; Cramér’s V (categorical) vs. state</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eta_squared(a, b):</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;One‑way ANOVA η² for numeric feature `a` against binary factor `b`.&quot;&quot;&quot;</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> [a[b <span class="op">==</span> g] <span class="cf">for</span> g <span class="kw">in</span> np.unique(b)]</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    ss_between <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(g) <span class="op">*</span> (g.mean() <span class="op">-</span> a.mean()) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> g <span class="kw">in</span> groups)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    ss_total   <span class="op">=</span> ((a <span class="op">-</span> a.mean()) <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ss_between <span class="op">/</span> ss_total</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cramers_v(cat, state):</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Cramér’s V for two categoricals.&quot;</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    confusion <span class="op">=</span> pd.crosstab(cat, state).values</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    chi2 <span class="op">=</span> chi2_contingency(confusion, correction<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> confusion.<span class="bu">sum</span>()</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    r, k <span class="op">=</span> confusion.shape</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sqrt(chi2 <span class="op">/</span> (n <span class="op">*</span> (<span class="bu">min</span>(r, k) <span class="op">-</span> <span class="dv">1</span>)))</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> effect_size_vs_state(df, numeric_cols, cat_cols):</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> []</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> numeric_cols:</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        res.append({<span class="st">&#39;feature&#39;</span>: col,</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;type&#39;</span>: <span class="st">&#39;numeric&#39;</span>,</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;eta2&#39;</span>: eta_squared(df[col].values, df[<span class="st">&#39;state&#39;</span>].values)})</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> cat_cols:</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        res.append({<span class="st">&#39;feature&#39;</span>: col,</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;type&#39;</span>: <span class="st">&#39;categorical&#39;</span>,</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;cramersV&#39;</span>: cramers_v(df[col].values, df[<span class="st">&#39;state&#39;</span>].values)})</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(res)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 2.  Pilot clustering on 1 % stratified sample (ignore state)</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pilot_clustering(df, num_cols, cat_cols, sample_frac<span class="op">=</span><span class="fl">0.01</span>, k_range<span class="op">=</span><span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">13</span>)):</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    samp <span class="op">=</span> df.groupby(<span class="st">&#39;state&#39;</span>, group_keys<span class="op">=</span><span class="va">False</span>).<span class="bu">apply</span>(</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span>sample_frac, random_state<span class="op">=</span><span class="dv">42</span>))</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># basic preprocessing</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;num&#39;</span>, StandardScaler(), num_cols),</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;cat&#39;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pre.fit_transform(samp)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>    best_k, best_score <span class="op">=</span> <span class="va">None</span>, <span class="op">-</span>np.inf</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>, batch_size<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> km.fit_predict(X)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        sil <span class="op">=</span> silhouette_score(X, labels, sample_size<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;k=</span><span class="sc">{</span>k<span class="sc">:2d}</span><span class="ss">, silhouette=</span><span class="sc">{</span>sil<span class="sc">:0.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sil <span class="op">&gt;</span> best_score:</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>            best_k, best_score <span class="op">=</span> k, sil</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_k, best_score</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a><span class="co"># 3.  Unsupervised dimensionality reduction: variance‑filter → sparse PCA</span></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a><span class="co">#     (Autoencoder hook left as </span><span class="al">TODO</span><span class="co"> if you prefer DL)</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> unsupervised_reducer(n_components<span class="op">=</span><span class="dv">30</span>, vt_thresh<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Pipeline([</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;vt&#39;</span>, VarianceThreshold(threshold<span class="op">=</span>vt_thresh)),  <span class="co"># drop near‑zero‑var cols</span></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;spca&#39;</span>, SparsePCA(n_components<span class="op">=</span>n_components, random_state<span class="op">=</span><span class="dv">42</span>)),</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;sc&#39;</span>, StandardScaler())  <span class="co"># optional: scale the components</span></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a><span class="co"># 4.  Vendor 23‑cluster roll‑up to 6 via HAC on centroids</span></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collapse_vendor_clusters(df, feature_cols):</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> df.groupby(<span class="st">&#39;vendor_cluster&#39;</span>)[feature_cols].mean().values  <span class="co"># 23×150</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> linkage(centroids, method<span class="op">=</span><span class="st">&#39;ward&#39;</span>)          <span class="co"># hierarchical on 23 points</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    labels_6 <span class="op">=</span> fcluster(Z, t<span class="op">=</span><span class="dv">6</span>, criterion<span class="op">=</span><span class="st">&#39;maxclust&#39;</span>)  <span class="co"># array(size=23)</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">enumerate</span>(labels_6, start<span class="op">=</span><span class="dv">0</span>))   <span class="co"># {0:2, 1:5, …}</span></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&#39;vendor_cluster_6&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;vendor_cluster&#39;</span>].<span class="bu">map</span>(mapping)</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df, mapping</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="co"># 5.  HAC on 50 k sample  →  train classifier to label remaining 1.45 M</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stratified_HAC_then_classifier(df, num_cols, cat_cols,</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>                                   sample_n<span class="op">=</span><span class="dv">50_000</span>,</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>                                   hac_linkage<span class="op">=</span><span class="st">&#39;ward&#39;</span>,</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>                                   classifier<span class="op">=</span><span class="st">&#39;rf&#39;</span>,</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>                                   n_clusters<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.1  Draw stratified sample</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>    samp <span class="op">=</span> df.groupby(<span class="st">&#39;state&#39;</span>, group_keys<span class="op">=</span><span class="va">False</span>).<span class="bu">apply</span>(</span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> x: x.sample(n<span class="op">=</span><span class="bu">min</span>(sample_n <span class="op">//</span> <span class="dv">2</span>, <span class="bu">len</span>(x)), random_state<span class="op">=</span><span class="dv">7</span>))</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>    rest <span class="op">=</span> df.drop(samp.index)</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.2  Preprocess (same transformer for both)</span></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;num&#39;</span>, StandardScaler(), num_cols),</span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&#39;cat&#39;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>    ], sparse_threshold<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>    X_samp <span class="op">=</span> pre.fit_transform(samp)</span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.3  HAC on sample</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>    hac <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span>n_clusters, linkage<span class="op">=</span>hac_linkage)</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>    samp_labels <span class="op">=</span> hac.fit_predict(X_samp.toarray() <span class="cf">if</span> sp.issparse(X_samp) <span class="cf">else</span> X_samp)</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>    samp[<span class="st">&#39;hac_label&#39;</span>] <span class="op">=</span> samp_labels</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.4  Train classifier</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> (RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">400</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>           <span class="cf">if</span> classifier <span class="op">==</span> <span class="st">&#39;rf&#39;</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>           <span class="cf">else</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">25</span>))</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>    clf.fit(X_samp, samp_labels)</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.5  Predict labels for the rest</span></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>    X_rest <span class="op">=</span> pre.transform(rest)</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>    rest[<span class="st">&#39;hac_label&#39;</span>] <span class="op">=</span> clf.predict(X_rest)</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5.6  Re‑assemble</span></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.concat([samp, rest]).sort_index()</span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a><span class="co"># 6.  ── Put it all together</span></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------------------</span></span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑A  Load your 1.5 M‑row DataFrame here  ────────────────────────────────</span></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df = pd.read_parquet(&quot;customers.parquet&quot;)  # &lt;‑‑ EXAMPLE</span></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">SystemExit</span>(<span class="st">&quot;🔴 Load `df` before running the pipeline.&quot;</span>)</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tag numeric vs categorical columns</span></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>    excluded <span class="op">=</span> {<span class="st">&#39;state&#39;</span>, <span class="st">&#39;vendor_cluster&#39;</span>}</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>    num_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.select_dtypes(include<span class="op">=</span><span class="st">&#39;number&#39;</span>) <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> excluded]</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>    cat_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.columns <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> num_cols <span class="kw">and</span> c <span class="kw">not</span> <span class="kw">in</span> excluded]</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑B  Effect‑size audit</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>    audit <span class="op">=</span> effect_size_vs_state(df, num_cols, cat_cols)</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>    audit.to_csv(<span class="st">&quot;effect_size_vs_state.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑C  Pilot clustering on 1 % sample</span></span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>    best_k, _ <span class="op">=</span> pilot_clustering(df, num_cols, cat_cols)</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑D  Dim‑reduced features (example: 30 sparse‑PCA comps)</span></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>    reducer <span class="op">=</span> unsupervised_reducer(n_components<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>    <span class="co"># reducer.fit_transform(...)  ← plug into your real pipeline</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑E  Collapse vendor clusters 23→6</span></span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>    df, mapping23to6 <span class="op">=</span> collapse_vendor_clusters(df, num_cols)</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑F  HAC on 50 k sample → classifier for rest</span></span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> stratified_HAC_then_classifier(df, num_cols, cat_cols)</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6‑G  Save or push `df` with new label columns wherever you need</span></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>    df.to_parquet(<span class="st">&quot;customers_with_segments.parquet&quot;</span>)</span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;✅ Pipeline completed.&quot;</span>)</span></code></pre></div>
<h3 id="how-to-adapt">How to adapt:</h3>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr>
<th>Section</th>
<th>What you might change</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data load block</strong></td>
<td>Replace the placeholder at the bottom with your actual file
read.</td>
</tr>
<tr>
<td><strong>Feature lists</strong></td>
<td>If you have text or date columns, preprocess (TF‑IDF, age) first,
then add to <code>num_cols</code> or <code>cat_cols</code>.</td>
</tr>
<tr>
<td><strong>Reducer</strong></td>
<td>Swap <code>SparsePCA</code> for an auto‑encoder: build your Keras
model, then change the <code>unsupervised_reducer()</code> step.</td>
</tr>
<tr>
<td><strong>Classifier</strong></td>
<td>Use <code>classifier='knn'</code> if you prefer k‑NN over Random
Forest.</td>
</tr>
<tr>
<td><strong>Cluster counts</strong></td>
<td>Tune <code>n_clusters</code> in both pilot and HAC sections to
whatever ceiling management sets.</td>
</tr>
</tbody>
</table>
<p>Run the script end‑to‑end once; you’ll get:</p>
<ul>
<li><p><code>effect_size_vs_state.csv</code> – a table of η² or
Cramér’s V per feature.</p></li>
<li><p><code>customers_with_segments.parquet</code> – full 1.5 M rows,
now with:</p>
<ul>
<li><code>vendor_cluster_6</code> (collapsed vendor labels)</li>
<li><code>hac_label</code> (fresh 50 k‑sample HAC propagated to all
records)</li>
</ul></li>
</ul>
<p>Feel free to drop any section you don’t need, or ping me if you’d
like the auto‑encoder block fleshed out.</p>
<h1 id="section-4">4</h1>
<p>Below is the <strong>“from‑scratch” pipeline I would stand up if I
owned your project</strong>.<br />
It keeps everything in a single, reproducible Python file (call it
<code>seg_pipeline.py</code>).<br />
Key design goals:</p>
<ul>
<li><strong>Scales to 1.5 M rows on a single beefy workstation</strong>
(32‑64 GB RAM) by
<ul>
<li>sampling for expensive diagnostics,</li>
<li>using MiniBatch algorithms,</li>
<li>working in sparse format when possible.</li>
</ul></li>
<li><strong>All‑in‑one</strong> – run it once and you get:
<ul>
<li>an effect‑size audit (<code>audit_vs_state.csv</code>),</li>
<li>a pilot‑clustering report (<code>pilot_metrics.csv</code>),</li>
<li>collapsed vendor labels (23 → 6),</li>
<li>a brand‑new 6‑cluster solution on the full data,</li>
<li>post‑hoc “lift” tables for cluster description,</li>
<li>a single Parquet with every person’s final labels.</li>
</ul></li>
<li><strong>Modular</strong> – each stage is a function; you can comment
out any step.</li>
</ul>
<hr />
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">seg_pipeline.py  ·  End‑to‑end customer segmentation</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">====================================================</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">Usage</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">-----</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">$ python seg_pipeline.py --input customers.parquet </span><span class="ch">\</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">                         --output_dir results </span><span class="ch">\</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">                         --n_components 30 </span><span class="ch">\</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">                         --target_clusters 6</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 0. Imports &amp; CLI</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> argparse, os, logging, json, math</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans, AgglomerativeClustering</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score, davies_bouldin_score</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> linkage, fcluster</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Helpers  ·  Effect sizes</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eta_squared(x, groups):</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> [x[groups <span class="op">==</span> g] <span class="cf">for</span> g <span class="kw">in</span> np.unique(groups)]</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    ss_between <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(g) <span class="op">*</span> (g.mean() <span class="op">-</span> x.mean()) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> g <span class="kw">in</span> groups)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    ss_total <span class="op">=</span> ((x <span class="op">-</span> x.mean()) <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ss_between <span class="op">/</span> ss_total <span class="cf">if</span> ss_total <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cramers_v(cat, state):</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    tab <span class="op">=</span> pd.crosstab(cat, state).values</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    chi2 <span class="op">=</span> chi2_contingency(tab, correction<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> tab.<span class="bu">sum</span>()</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    r, k <span class="op">=</span> tab.shape</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> math.sqrt(chi2 <span class="op">/</span> (n <span class="op">*</span> (<span class="bu">min</span>(r, k) <span class="op">-</span> <span class="dv">1</span>))) <span class="cf">if</span> n <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> audit_vs_state(df, numeric_cols, cat_cols, out_csv):</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    rows <span class="op">=</span> []</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> numeric_cols:</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        rows.append({<span class="st">&quot;feature&quot;</span>: col,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;type&quot;</span>: <span class="st">&quot;num&quot;</span>,</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;eta2&quot;</span>: eta_squared(df[col].values, df[<span class="st">&quot;state&quot;</span>].values)})</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> cat_cols:</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        rows.append({<span class="st">&quot;feature&quot;</span>: col,</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;type&quot;</span>: <span class="st">&quot;cat&quot;</span>,</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;cramersV&quot;</span>: cramers_v(df[col].values, df[<span class="st">&quot;state&quot;</span>].values)})</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(rows).to_csv(out_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Pilot clustering on 1 % stratified sample</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pilot_cluster(df, num_cols, cat_cols, k_range, out_csv):</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    samp <span class="op">=</span> df.groupby(<span class="st">&quot;state&quot;</span>, group_keys<span class="op">=</span><span class="va">False</span>)<span class="op">\</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>             .<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span><span class="dv">42</span>))</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;num&quot;</span>, StandardScaler(), num_cols),</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;cat&quot;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&quot;ignore&quot;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    ], n_jobs<span class="op">=-</span><span class="dv">1</span>, sparse_threshold<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pre.fit_transform(samp)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    records <span class="op">=</span> []</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        lbl <span class="op">=</span> km.fit_predict(X)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        sil <span class="op">=</span> silhouette_score(X, lbl, sample_size<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        db  <span class="op">=</span> davies_bouldin_score(X, lbl)</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        records.append({<span class="st">&quot;k&quot;</span>: k, <span class="st">&quot;silhouette&quot;</span>: sil, <span class="st">&quot;davies_bouldin&quot;</span>: db})</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(records).to_csv(out_csv, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Dimensionality reduction block</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_reducer(vt_thresh, n_components):</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Pipeline([</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;vt&quot;</span>, VarianceThreshold(threshold<span class="op">=</span>vt_thresh)),</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;svd&quot;</span>, TruncatedSVD(n_components<span class="op">=</span>n_components, random_state<span class="op">=</span><span class="dv">2</span>)),</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;sc&quot;</span>,  StandardScaler())</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Collapse vendor clusters 23 → k via HAC on centroids</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collapse_vendor(df, feature_cols, target_k):</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> df.groupby(<span class="st">&quot;vendor_cluster&quot;</span>)[feature_cols].mean().values</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> linkage(centroids, method<span class="op">=</span><span class="st">&quot;ward&quot;</span>)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>    map23 <span class="op">=</span> fcluster(Z, t<span class="op">=</span>target_k, criterion<span class="op">=</span><span class="st">&quot;maxclust&quot;</span>)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> {old: new <span class="cf">for</span> old, new <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="bu">len</span>(map23)), map23)}</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&quot;vendor_k</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(target_k)] <span class="op">=</span> df[<span class="st">&quot;vendor_cluster&quot;</span>].<span class="bu">map</span>(mapping)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mapping</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Fresh clustering on full data (dim‑reduced)</span></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fresh_clustering(df, num_cols, cat_cols,</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>                     n_components, target_k, out_parquet):</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> ColumnTransformer([</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;num&quot;</span>, StandardScaler(), num_cols),</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;cat&quot;</span>, OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&quot;ignore&quot;</span>, sparse<span class="op">=</span><span class="va">True</span>), cat_cols)</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    ], n_jobs<span class="op">=-</span><span class="dv">1</span>, sparse_threshold<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>    reducer <span class="op">=</span> make_reducer(<span class="fl">0.0</span>, n_components)</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>    pipe <span class="op">=</span> Pipeline([(<span class="st">&quot;pre&quot;</span>, pre), (<span class="st">&quot;red&quot;</span>, reducer)])</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    X_red <span class="op">=</span> pipe.fit_transform(df)      <span class="co"># sparse → dense SVD(…)</span></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>target_k,</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>                         random_state<span class="op">=</span><span class="dv">3</span>, batch_size<span class="op">=</span><span class="dv">4096</span>)</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">&quot;new_cluster&quot;</span>] <span class="op">=</span> km.fit_predict(X_red)</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>    df.to_parquet(out_parquet)</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pipe, km</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Post‑hoc lifts for description</span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lift_tables(df, cluster_col, numeric_cols, cat_cols, out_dir):</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>    grp <span class="op">=</span> df.groupby(cluster_col)</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>    base <span class="op">=</span> df.shape[<span class="dv">0</span>]</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>    Path(out_dir).mkdir(exist_ok<span class="op">=</span><span class="va">True</span>, parents<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numeric mean lift</span></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>    num_tbl <span class="op">=</span> pd.concat({</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>        c: (grp[c].mean() <span class="op">/</span> df[c].mean()) <span class="cf">for</span> c <span class="kw">in</span> numeric_cols</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>    }, axis<span class="op">=</span><span class="dv">1</span>).T</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>    num_tbl.to_csv(Path(out_dir) <span class="op">/</span> <span class="st">&quot;lift_numeric.csv&quot;</span>)</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># categorical over‑/under‑rep (% share)</span></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> cat_cols:</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>        tab <span class="op">=</span> pd.crosstab(df[cluster_col], df[c], normalize<span class="op">=</span><span class="st">&quot;index&quot;</span>)</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>        tab.to_csv(Path(out_dir) <span class="op">/</span> <span class="ss">f&quot;lift_</span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">.csv&quot;</span>)</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Main</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(args):</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> Path(args.output_dir)</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>    out.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>    logging.basicConfig(</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>        level<span class="op">=</span>logging.INFO,</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>        <span class="bu">format</span><span class="op">=</span><span class="st">&quot;</span><span class="sc">%(asctime)s</span><span class="st"> | </span><span class="sc">%(levelname)s</span><span class="st"> | </span><span class="sc">%(message)s</span><span class="st">&quot;</span>,</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>        handlers<span class="op">=</span>[logging.FileHandler(out <span class="op">/</span> <span class="st">&quot;run.log&quot;</span>),</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>                  logging.StreamHandler()])</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>    log <span class="op">=</span> logging.getLogger()</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;▶ Loading data …&quot;</span>)</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_parquet(args.<span class="bu">input</span>)</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- column buckets ------------------------------------------------------</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>    ignore <span class="op">=</span> {<span class="st">&quot;state&quot;</span>, <span class="st">&quot;vendor_cluster&quot;</span>}</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>    num_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.select_dtypes(include<span class="op">=</span><span class="st">&quot;number&quot;</span>) <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> ignore]</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>    cat_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> df.columns <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> num_cols <span class="kw">and</span> c <span class="kw">not</span> <span class="kw">in</span> ignore]</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 1. Effect‑size audit -----------------------------------------------</span></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;▶ Effect‑size audit vs. state&quot;</span>)</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>    audit_vs_state(df, num_cols, cat_cols, out <span class="op">/</span> <span class="st">&quot;audit_vs_state.csv&quot;</span>)</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 2. Pilot clustering -------------------------------------------------</span></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;▶ Pilot clustering on 1 </span><span class="sc">% s</span><span class="st">ample&quot;</span>)</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>    pilot_cluster(df, num_cols, cat_cols,</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>                  k_range<span class="op">=</span><span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">13</span>),</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>                  out_csv<span class="op">=</span>out <span class="op">/</span> <span class="st">&quot;pilot_metrics.csv&quot;</span>)</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 3. Collapse vendor clusters ----------------------------------------</span></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;▶ Collapsing vendor clusters → </span><span class="sc">%d</span><span class="st"> groups&quot;</span>, args.target_clusters)</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> collapse_vendor(df, num_cols, args.target_clusters)</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>    json.dump(mapping, <span class="bu">open</span>(out <span class="op">/</span> <span class="st">&quot;vendor23_to_k.json&quot;</span>, <span class="st">&quot;w&quot;</span>))</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 4. Fresh clustering -------------------------------------------------</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;▶ Fresh MiniBatch K‑means (</span><span class="sc">%d</span><span class="st"> comps → k=</span><span class="sc">%d</span><span class="st">)&quot;</span>,</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>             args.n_components, args.target_clusters)</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>    pipe, km <span class="op">=</span> fresh_clustering(df, num_cols, cat_cols,</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>                                args.n_components,</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>                                args.target_clusters,</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>                                out <span class="op">/</span> <span class="st">&quot;customers_with_clusters.parquet&quot;</span>)</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---- 5. Lift tables ------------------------------------------------------</span></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;▶ Computing lift tables for description&quot;</span>)</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>    lift_tables(df, <span class="st">&quot;new_cluster&quot;</span>, num_cols, cat_cols, out <span class="op">/</span> <span class="st">&quot;lifts&quot;</span>)</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>    log.info(<span class="st">&quot;✅ Done.  Results in </span><span class="sc">%s</span><span class="st">&quot;</span>, out)</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a><span class="co"># ──────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> argparse.ArgumentParser()</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--input&quot;</span>, required<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">help</span><span class="op">=</span><span class="st">&quot;Parquet file with raw data (1.5 M rows)&quot;</span>)</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--output_dir&quot;</span>, default<span class="op">=</span><span class="st">&quot;results&quot;</span>)</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--n_components&quot;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">help</span><span class="op">=</span><span class="st">&quot;SVD dimensions before clustering&quot;</span>)</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>    p.add_argument(<span class="st">&quot;--target_clusters&quot;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, default<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">help</span><span class="op">=</span><span class="st">&quot;Desired number of final clusters&quot;</span>)</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> p.parse_args()</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>    main(args)</span></code></pre></div>
<hr />
<h3 id="why-this-script-hits-the-sweetspot">Why this script hits the
sweet‑spot</h3>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 84%" />
</colgroup>
<thead>
<tr>
<th>Concern</th>
<th>How the script addresses it</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Memory / speed</strong></td>
<td>MiniBatch K‑means, sparse encoders, Truncated SVD; heavy
calculations run on 1 % or 50 k samples only.</td>
</tr>
<tr>
<td><strong>Vendor label reuse</strong></td>
<td><code>collapse_vendor()</code> gives you a defensible 6‑group
roll‑up without re‑clustering 1.5 M rows.</td>
</tr>
<tr>
<td><strong>Fresh segmentation</strong></td>
<td><code>fresh_clustering()</code> creates an all‑new solution in the
same run, so you can A/B them.</td>
</tr>
<tr>
<td><strong>Business story</strong></td>
<td><code>lift_tables()</code> auto‑exports CSVs showing which features
are over/under‑represented in each cluster—perfect for slide‑ware.</td>
</tr>
<tr>
<td><strong>Reproducibility</strong></td>
<td>One command, deterministic RNG seeds, all outputs timestamped.</td>
</tr>
</tbody>
</table>
<p>Tweak component counts, cluster counts, or swap in an auto‑encoder by
replacing <code>make_reducer()</code> with your Keras model—everything
else stays the same.</p>
<p>Run:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> seg_pipeline.py <span class="at">--input</span> customers.parquet <span class="at">--output_dir</span> seg_results</span></code></pre></div>
<p>and inspect the logs plus the files under
<code>seg_results/</code>.</p>
<p>Let me know if you’d like any part (e.g., Spark adaptation,
auto‑encoder example, richer KPI lift charts) fleshed out further!</p>
<h1 id="section-5">5</h1>
<h3 id="can-zip-codes-be-used-for-clustering">Can ZIP codes be used for
clustering?</h3>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 35%" />
<col style="width: 24%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>Option</th>
<th>How you’d engineer it</th>
<th>When it helps</th>
<th>Watch‑outs</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Raw 5‑digit ZIP as a categorical</strong></td>
<td>One‑hot (≈ 42 k columns) or target / frequency encode</td>
<td>Almost never—the code itself has no ordinal meaning and explodes
dimensionality</td>
<td>Curse of dimensionality; distance metrics treat two adjacent ZIPs as
“maximally different”</td>
</tr>
<tr>
<td><strong>Lat / lon centroid</strong></td>
<td>Map each ZIP to its latitude &amp; longitude, standardize, feed as
two numeric features</td>
<td>When geographic proximity is the signal you care about (e.g.,
service distance, weather)</td>
<td><em>K‑means</em>’ Euclidean distance on lat/lon is only approximate;
consider Haversine or convert to UTM</td>
</tr>
<tr>
<td><strong>Derived geodemographics</strong></td>
<td>Join external tables: rural/urban flag, county poverty rate, median
HH income, population density, Nielsen PRIZM cluster, etc.</td>
<td>When you suspect socioeconomic context drives behavior</td>
<td>Adds real signal without exploding feature space</td>
</tr>
<tr>
<td><strong>Spatial embeddings / distance matrices</strong></td>
<td>Pre‑compute adjacency graph or distance‑to‑nearest‑N ZIPs and use
graph‑based clustering (HDBSCAN, Leiden)</td>
<td>When true “neighborhoods” matter more than city/state lines</td>
<td>Heavier engineering; harder to explain to non‑technical
stakeholders</td>
</tr>
</tbody>
</table>
<p><strong>What I’d do in your project</strong></p>
<ol type="1">
<li><strong>Add lat &amp; lon</strong> for every record (two numeric
columns).</li>
<li>Optional: pull in <strong>one or two contextual scores</strong>
(median income, rural/urban).</li>
<li>Skip the raw ZIP code column during clustering; keep it only for
post‑hoc descriptions like “Cluster 3 skews toward Denver suburbs.”</li>
</ol>
<hr />
<h3 id="what-about-an-occupation-variable-with-40-categories">What about
an occupation variable with &gt; 40 categories?</h3>
<p>Forty‑odd categories is manageable, but think through these
angles:</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 31%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr>
<th>Technique</th>
<th>Pros</th>
<th>Cons / mitigations</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>One‑hot encode all 40</strong></td>
<td>Simple, transparent; 40 extra columns is tiny vs. 150 base
features</td>
<td>Treats occupations as independent; can dilute distances if many
low‑freq jobs → consider frequency‑based pruning (“Other” bucket for
categories &lt; 1 % of sample).</td>
</tr>
<tr>
<td><strong>Group into major SOC codes</strong> (e.g., 10‑15
sectors)</td>
<td>Easier to interpret (“Healthcare”, “Construction”)</td>
<td>You lose granularity; do this only if stakeholders want higher‑level
personas.</td>
</tr>
<tr>
<td><strong>Ordinal encode by median income or job
prestige</strong></td>
<td>Injects business meaning; keeps it 1D</td>
<td>Assumes monotonic effect that may not exist; fine if income is the
real driver and you already have salary bands.</td>
</tr>
<tr>
<td><strong>Entity embeddings</strong> (learned in a shallow neural net
before clustering)</td>
<td>Captures similarity between occupations (e.g., “nurse” closer to
“physician assistant” than to “truck driver”)</td>
<td>Extra engineering; you need TensorFlow/PyTorch loop and to freeze
the embedding before clustering.</td>
</tr>
<tr>
<td><strong>Weight‑of‑Evidence / target encoding</strong> (if you have a
KPI)</td>
<td>Squeezes information into a single numeric with predictive
power</td>
<td>Risk of leakage unless you use out‑of‑fold encodings; ties the
clustering to that KPI.</td>
</tr>
</tbody>
</table>
<p><strong>Practical pick</strong><br />
<em>With 1.5 M rows,</em> one‑hot encoding 40 categories usually works
fine and keeps the pipeline simple. Drop the few occupations with
&lt; 1 % frequency into an “Other” bucket to avoid tiny dummy columns.
If interpretability is paramount, map the 40 codes to 10–15 SOC
super‑groups <strong>and</strong> keep the original fine‑grained codes
for post‑hoc profiling.</p>
<hr />
<h3
id="recommended-minimal-additions-to-the-pipeline-you-already-have">Recommended
minimal additions to the pipeline you already have</h3>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After loading df …</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ---- Zip‑code engineering -------------------------------------------</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>zip_meta <span class="op">=</span> pd.read_csv(<span class="st">&quot;zip_lat_lon_income.csv&quot;</span>)  <span class="co"># ZIP, lat, lon, median_income</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.merge(zip_meta, on<span class="op">=</span><span class="st">&quot;zip&quot;</span>, how<span class="op">=</span><span class="st">&quot;left&quot;</span>)     <span class="co"># adds &#39;lat&#39;, &#39;lon&#39;, &#39;zip_income&#39;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>num_cols.extend([<span class="st">&#39;lat&#39;</span>, <span class="st">&#39;lon&#39;</span>, <span class="st">&#39;zip_income&#39;</span>])     <span class="co"># use in clustering</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ---- Occupation one‑hots ---------------------------------------------</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>occ_freq <span class="op">=</span> df[<span class="st">&#39;occupation&#39;</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>rare <span class="op">=</span> occ_freq[occ_freq <span class="op">&lt;</span> <span class="fl">0.01</span>].index</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;occupation_mod&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;occupation&#39;</span>].where(<span class="op">~</span>df[<span class="st">&#39;occupation&#39;</span>].isin(rare), <span class="st">&#39;Other&#39;</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>cat_cols.append(<span class="st">&#39;occupation_mod&#39;</span>)                 <span class="co"># treated by OneHotEncoder</span></span></code></pre></div>
<p>This keeps the clustering engine lightweight, injects meaningful
geography and occupation signal, and still allows you to say in the
final deck:</p>
<blockquote>
<p>“Cluster 4: younger professionals, Denver‑metro ZIPs, tech &amp;
healthcare occupations, median income $78 k.”</p>
</blockquote>
<p>Let me know if you’d like code for a geospatial distance kernel or
embedding‑based occupation encoding!</p>
    
</body>
</html>