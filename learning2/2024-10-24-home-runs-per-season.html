<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><p>Sure! Here’s a step-by-step tutorial to implement a
<strong>hierarchical (multilevel) logistic regression</strong> or a
<strong>generalized linear mixed model (GLMM)</strong> in Python using a
common library like <code>statsmodels</code> or
<code>scikit-learn</code> for the logistic regression part, and
<code>lme4</code> in R or <code>MixedLM</code> from
<code>statsmodels</code> for the GLMM.</p>
<h3
id="tutorial-predicting-home-runs-using-hierarchical-logistic-regression-glmm">Tutorial:
Predicting Home Runs Using Hierarchical Logistic Regression (GLMM)</h3>
<p><strong>Goal</strong>: We want to predict whether a baseball player
will hit more than 30 home runs next season using previous seasons’ home
run data and player characteristics, accounting for <em>within-player
variability</em>.</p>
<h4 id="prerequisites">Prerequisites</h4>
<p>Make sure you have the following Python libraries installed:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pandas numpy matplotlib seaborn statsmodels sklearn</span></code></pre></div>
<h3 id="step-1-import-libraries">Step 1: Import Libraries</h3>
<p>Start by importing the necessary Python libraries:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.regression.mixed_linear_model <span class="im">import</span> MixedLM</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span></code></pre></div>
<h3 id="step-2-data-preparation">Step 2: Data Preparation</h3>
<p>Let’s assume you have a dataset with the following columns:</p>
<ul>
<li><code>player_id</code>: Player’s unique ID</li>
<li><code>season</code>: The season (year)</li>
<li><code>home_runs</code>: Number of home runs in that season</li>
<li><code>age</code>: Player’s age during the season</li>
<li><code>at_bats</code>: Number of at-bats</li>
<li><code>will_hit_30_plus_hr</code>: Binary target variable
(<code>1</code> if the player hit more than 30 home runs, <code>0</code>
otherwise)</li>
</ul>
<p>First, load your dataset:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load your dataset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;baseball_player_data.csv&#39;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the binary target variable</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;will_hit_30_plus_hr&#39;</span>] <span class="op">=</span> (df[<span class="st">&#39;home_runs&#39;</span>] <span class="op">&gt;</span> <span class="dv">30</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View the data</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span></code></pre></div>
<h3 id="step-3-exploratory-data-analysis-eda">Step 3: Exploratory Data
Analysis (EDA)</h3>
<p>Let’s do a quick EDA to understand the data:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic statistics</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.describe())</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution of home runs</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>sns.histplot(df[<span class="st">&#39;home_runs&#39;</span>], bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Home Run Distribution&#39;</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the proportion of players hitting more than 30 HRs per season</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>season_hr_proportion <span class="op">=</span> df.groupby(<span class="st">&#39;season&#39;</span>)[<span class="st">&#39;will_hit_30_plus_hr&#39;</span>].mean()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.plot(season_hr_proportion)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Proportion of Players with &gt;30 Home Runs Over Time&#39;</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<h3 id="step-4-splitting-the-data">Step 4: Splitting the Data</h3>
<p>Now, we’ll split the data into training and test sets:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into training and test sets</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>train, test <span class="op">=</span> train_test_split(df, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Training size: </span><span class="sc">{</span><span class="bu">len</span>(train)<span class="sc">}</span><span class="ss">, Test size: </span><span class="sc">{</span><span class="bu">len</span>(test)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 id="step-5-logistic-regression-without-hierarchical-structure">Step
5: Logistic Regression (Without Hierarchical Structure)</h3>
<p>First, let’s fit a standard <strong>logistic regression</strong>
(without any player-specific effects) using
<code>statsmodels</code>.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple logistic regression model</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>logit_model <span class="op">=</span> logit(<span class="st">&#39;will_hit_30_plus_hr ~ age + at_bats + home_runs&#39;</span>, data<span class="op">=</span>train)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> logit_model.fit()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary of the model</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result.summary())</span></code></pre></div>
<h3 id="step-6-introducing-hierarchical-structure-using-glmm">Step 6:
Introducing Hierarchical Structure Using GLMM</h3>
<p>Next, we will introduce <strong>player-specific random
effects</strong> by using a <strong>Generalized Linear Mixed Model
(GLMM)</strong> with <code>MixedLM</code> from <code>statsmodels</code>.
This will account for each player having a different baseline
probability of hitting more than 30 home runs.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mixed effects model - including random intercepts for players</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>md <span class="op">=</span> MixedLM.from_formula(<span class="st">&#39;will_hit_30_plus_hr ~ age + at_bats + home_runs&#39;</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                          groups<span class="op">=</span><span class="st">&#39;player_id&#39;</span>, data<span class="op">=</span>train)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>mixed_lm_result <span class="op">=</span> md.fit()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary of the model</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mixed_lm_result.summary())</span></code></pre></div>
<h3 id="step-7-model-interpretation">Step 7: Model Interpretation</h3>
<p>After fitting the GLMM model, the <code>summary()</code> method will
give you a table that includes the coefficients for both the fixed
effects (e.g., age, at_bats, home_runs) and the random effects for each
player.</p>
<ul>
<li><strong>Fixed effects</strong> represent the population-level trends
(e.g., for every additional home run in previous seasons, the likelihood
of hitting 30+ home runs increases).</li>
<li><strong>Random effects</strong> are player-specific intercepts,
accounting for how each player’s baseline performance differs from the
overall population trend.</li>
</ul>
<h3 id="step-8-predicting-on-test-data">Step 8: Predicting on Test
Data</h3>
<p>Now, let’s predict on the test data and evaluate the model’s
performance:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;predicted_prob&#39;</span>] <span class="op">=</span> mixed_lm_result.predict(test)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert probabilities to binary outcomes using a threshold of 0.5</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;predicted_class&#39;</span>] <span class="op">=</span> (test[<span class="st">&#39;predicted_prob&#39;</span>] <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(test[<span class="st">&#39;will_hit_30_plus_hr&#39;</span>], test[<span class="st">&#39;predicted_class&#39;</span>]))</span></code></pre></div>
<h3 id="step-9-interpretation-of-results">Step 9: Interpretation of
Results</h3>
<p>The classification report will give you the precision, recall, and
F1-score for predicting whether a player will hit more than 30 home runs
in the next season.</p>
<ul>
<li><strong>Precision</strong>: Out of all the players predicted to hit
30+ home runs, how many actually did?</li>
<li><strong>Recall</strong>: Out of all the players who hit 30+ home
runs, how many were correctly identified?</li>
<li><strong>F1-score</strong>: The harmonic mean of precision and
recall.</li>
</ul>
<h3 id="step-10-conclusion">Step 10: Conclusion</h3>
<ul>
<li>If you find that the random effects (player-specific baselines)
significantly improve the model’s performance, this is a sign that
<strong>within-player variation</strong> is important in your
model.</li>
<li>You can also experiment with more features, such as player-specific
historical trends, injuries, or league-wide changes, to further improve
predictive power.</li>
</ul>
<h3 id="next-steps">Next Steps</h3>
<ul>
<li><strong>Hyperparameter Tuning</strong>: You can try cross-validation
and hyperparameter tuning for better model performance.</li>
<li><strong>Alternative Models</strong>: You can explore Bayesian models
(e.g., using <code>PyMC3</code> or <code>Stan</code>), which offer more
flexibility in handling hierarchical structures.</li>
</ul>
<hr />
<p>By following this tutorial, you should now have a working
hierarchical model that accounts for player-specific variability in
predicting whether a player will hit more than 30 home runs in the next
season!</p>
    
</body>
</html>