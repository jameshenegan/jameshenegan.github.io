<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1
id="machine-learning-design-patterns-chapter-7-responsible-ai">Machine
Learning Design Patterns, Chapter 7: Responsible AI</h1>
<h2 id="introduction">Introduction</h2>
<p>The previous focus has been on patterns that assist data and
engineering teams in preparing, building, training, and scaling machine
learning (ML) models for production. These patterns primarily benefit
those involved in the development process. However, once a model is
deployed, its influence extends beyond the development teams to a wider
range of stakeholders, both internal and external to the organization.
These stakeholders include business executives, model end users, and
regulatory bodies.</p>
<p>The groups of stakeholders can be categorized as follows:</p>
<ol type="1">
<li><strong>Model builders</strong>: Data scientists and researchers
responsible for developing ML models.</li>
<li><strong>ML engineers</strong>: Teams involved in deploying these
models.</li>
<li><strong>Business decision-makers</strong>: Evaluate whether the
model aligns with business goals and customer applications.</li>
<li><strong>End users</strong>: Individuals or groups who interact with
model outputs, such as customers, employees, or professionals relying on
predictions for decisions.</li>
<li><strong>Regulatory bodies</strong>: Those needing insight into how
models make decisions to ensure compliance with rules and standards,
such as auditors or governance teams.</li>
</ol>
<p>In this chapter, we explore patterns that address the broader impact
of models, focusing on their effect on individuals and groups outside
the development team. Key patterns discussed include:</p>
<ul>
<li><strong>Heuristic Benchmark</strong>: Offers a way to contextualize
model performance for end users and decision-makers.</li>
<li><strong>Explainable Predictions</strong>: Helps increase trust in ML
systems by explaining how a model makes decisions.</li>
<li><strong>Fairness Lens</strong>: Ensures models operate fairly across
different user groups and scenarios.</li>
</ul>
<p>Together, these patterns contribute to the broader practice of
Responsible AI, which focuses on building fairness, interpretability,
privacy, and security into AI systems. This approach involves
considering diverse user perspectives, understanding limitations in data
and models, and continuously monitoring systems post-deployment. While
the chapter highlights specific Responsible AI patterns, many other
patterns discussed previously also align with these goals, helping to
create AI systems that are ethical, transparent, and secure.</p>
<h2 id="design-pattern-28-heuristic-benchmark">Design Pattern 28:
Heuristic Benchmark</h2>
<p>The <strong>Heuristic Benchmark design pattern</strong> provides a
practical way for machine learning (ML) teams to contextualize model
performance by comparing it to a simple, understandable benchmark. This
pattern is particularly valuable when presenting models to business
stakeholders, who may find raw performance metrics difficult to
interpret.</p>
<h3 id="the-problem">The Problem</h3>
<p>When presenting model performance, such as mean absolute error (MAE)
or mean average precision (MAP), stakeholders often ask whether the
performance is “good” or “bad.” For example, in a bicycle rental
agency’s case, if the ML model predicts rental duration with an MAE of
1,200 seconds, stakeholders may not understand whether this performance
is acceptable. Simply presenting the technical metrics doesn’t provide
the business context decision-makers need.</p>
<h3 id="the-solution">The Solution</h3>
<p>To make these metrics relatable, we compare the model’s performance
against a simple, intuitive <strong>heuristic benchmark</strong>. This
is especially important when building the first ML model for a task,
where no prior model exists for comparison. A heuristic benchmark should
be easy to understand and compute, such as a rule of thumb, a constant,
or basic statistical measures like the mean, median, or mode. It should
avoid the complexity of ML algorithms like linear regression, which
could confuse non-technical stakeholders.</p>
<p>The key advantage of using a heuristic benchmark is that it provides
a reference point, helping stakeholders quickly grasp whether the
model’s performance represents an improvement or not.</p>
<h3 id="when-not-to-use-heuristic-benchmarks">When Not to Use Heuristic
Benchmarks</h3>
<p>If an operational practice already exists, such as a manual method or
an older system for solving the problem, it’s better to compare the new
model against that practice instead of creating a heuristic benchmark.
The existing method serves as a real-world reference point, which might
be more relevant for stakeholders.</p>
<h3 id="examples-of-heuristic-benchmarks">Examples of Heuristic
Benchmarks</h3>
<p>Table 7-1 illustrates heuristic benchmarks across different
scenarios:</p>
<ul>
<li><strong>Regression Problems</strong>: For predicting rental
durations, you might use the median rental time, such as predicting that
all rentals will take 2,120 seconds, which is the median time from
historical data.</li>
<li><strong>Binary Classification</strong>: In scenarios like predicting
whether a question on Stack Overflow will get edited, the overall
fraction of positive outcomes (e.g., 0.36 of all accepted answers get
edited) could serve as the heuristic.</li>
<li><strong>Time Series Prediction</strong>: When predicting weekly
sales, you might use the previous week’s sales figure or a trend based
on last year’s sales from the same week.</li>
</ul>
<h3 id="trade-offs-and-alternatives">Trade-Offs and Alternatives</h3>
<p>Heuristic benchmarks not only help explain model performance but can
also assist during the model development phase by identifying
performance issues. For example, in the bicycle rental model, creating a
lookup table based on station location and peak hours could help detect
biases, such as the model over-focusing on busy stations and neglecting
less popular ones.</p>
<p>In cases where human experts are involved, like diagnosing diseases
from medical images, the benchmark could be based on expert performance.
By comparing the model’s predictions against a panel of experts, you can
assess the model’s effectiveness and communicate how it performs
relative to human decision-makers.</p>
<h3 id="utility-value">Utility Value</h3>
<p>Beyond simply comparing performance metrics, it’s crucial to explain
the <strong>utility value</strong> of the improvement the ML model
offers. For instance, if a model’s MAE is 30 seconds lower or its MAP is
1% higher, it’s important to communicate the real-world impact of this
improvement. This could mean financial savings, fewer abandoned orders,
or earlier disease detection. By calculating the utility value,
stakeholders can better understand whether the benefits of the model
justify the costs of deploying it.</p>
<p>In the bicycle rental scenario, this could mean measuring how the
model improves bicycle availability or profitability, helping
stakeholders decide whether the ML model is worth implementing.</p>
<h3 id="conclusion">Conclusion</h3>
<p>The Heuristic Benchmark design pattern enables ML teams to bridge the
gap between technical metrics and business relevance by providing an
intuitive, easily understandable comparison point. This helps
stakeholders make informed decisions about deploying models by
illustrating both performance and the tangible benefits to the
business.</p>
<h2 id="design-pattern-29-explainable-predictions">Design Pattern 29:
Explainable Predictions</h2>
<p>The <strong>Explainable Predictions design pattern</strong> addresses
the need for users and stakeholders to understand how machine learning
(ML) models arrive at their predictions. While models such as decision
trees are inherently interpretable, more complex models, like deep
neural networks, often lack transparency. This pattern aims to provide
insights into the factors influencing model behavior to build trust in
ML systems.</p>
<h3 id="the-problem-1">The Problem</h3>
<p>Common performance metrics, such as accuracy or mean squared error,
can show how well a model is performing but do not explain <em>why</em>
it makes specific predictions. This lack of transparency can cause users
to hesitate when trusting model outputs. For example, a model predicting
diabetic retinopathy (DR) severity may achieve high accuracy, but
without knowing whether the model is focusing on the correct image
regions (e.g., the retina rather than irrelevant parts like doctor
annotations), medical professionals might be reluctant to rely on it for
diagnoses.</p>
<p>This lack of interpretability becomes even more critical in sensitive
domains such as finance or healthcare, where users want to know the
reasoning behind model predictions, such as credit scores or medical
decisions. Without understanding the contributing factors behind
predictions, biases can emerge, leading to a loss of trust in the
model.</p>
<p>Additionally, regulatory stakeholders often need transparency to
justify the use of a model, especially in sectors that require audits.
Data scientists and ML engineers also benefit from understanding feature
importance to improve model performance and debug unexpected
behaviors.</p>
<h3 id="the-solution-1">The Solution</h3>
<p>The solution to this problem is to implement <strong>explainability
methods</strong> that provide insights into how models make predictions.
These methods, known as feature attributions, help assign importance to
each feature, explaining its contribution to the final prediction.
Feature attributions can be applied at two levels:</p>
<ol type="1">
<li><strong>Instance-level</strong>: Explains individual predictions,
such as why a specific loan was approved or denied.</li>
<li><strong>Global-level</strong>: Summarizes overall model behavior,
like identifying the most important features across all
predictions.</li>
</ol>
<p>For simpler models like decision trees or linear regression,
interpretability is easier because their structure directly reveals the
relationships between input features and outputs. However, for complex
models like neural networks, <strong>post hoc explainability</strong>
methods are used to approximate these relationships without depending on
the model’s internal weights.</p>
<p>Two common explainability methods include:</p>
<ul>
<li><strong>Sampled Shapley</strong>: Based on Shapley values from game
theory, it calculates how adding or removing features affects
predictions.</li>
<li><strong>Integrated Gradients (IG)</strong>: Calculates gradients
between a baseline (like a median value) and a specific input to
determine feature contributions.</li>
</ul>
<h3 id="feature-attributions-in-practice">Feature Attributions in
Practice</h3>
<p>To make feature attributions useful, a <strong>baseline</strong> is
needed as a reference point for comparison. For instance, in a model
predicting heart disease, the baseline might be the median health
statistics of a typical patient. Attribution values are then calculated
relative to this baseline, showing how features like cholesterol or
blood pressure influence the prediction.</p>
<p>For example, a bike rental model might predict a rental duration of
10.71 minutes, with a baseline prediction of 13.6 minutes. The feature
attribution values help explain the difference, revealing which factors
(e.g., distance traveled) contributed to the lower predicted
duration.</p>
<h3 id="tools-and-techniques">Tools and Techniques</h3>
<p>Several tools make implementing explainability easier, such as the
open-source <strong>SHAP</strong> library, which provides an API to
calculate feature attributions for models trained on image, text, or
tabular data. SHAP’s visualizations, like force plots and summary plots,
help identify the most important features for specific predictions and
provide a global view of feature importance across multiple
examples.</p>
<p>In production environments, cloud-based services like <strong>Google
Cloud’s Explainable AI</strong> allow users to deploy models with
integrated explainability features. These services offer different
methods, such as Integrated Gradients or XRAI (an advanced version of IG
for image models), to visualize feature importance for models deployed
in the cloud.</p>
<h3 id="trade-offs-and-alternatives-1">Trade-Offs and Alternatives</h3>
<p>While explainability improves trust and model transparency, it is not
foolproof. Explanations are limited by the quality of the training data
and the chosen baseline. For example, if the training data is biased or
unrepresentative, the explanations will reflect these biases.
Additionally, explanations may only reveal relationships within the
context of the data and model used, and should not be considered
definitive outside that scope.</p>
<p>Other methods like <strong>counterfactual analysis</strong> and
<strong>example-based explanations</strong> offer alternative ways to
interpret predictions. Counterfactual analysis identifies similar data
points that led to different outcomes, providing insights into feature
influence. Example-based explanations show comparisons with similar
examples from the training set, helping understand how training data
impacts predictions.</p>
<h3 id="limitations-of-explanations">Limitations of Explanations</h3>
<p>Explanations are valuable tools but should not be viewed as perfect
representations of model behavior. They must be used in conjunction with
other model evaluation criteria, such as accuracy, error rates, and
fairness assessments. The explanations are only as good as the
underlying data and model quality, and they should be considered as one
piece of a broader approach to model evaluation and improvement.</p>
<h3 id="conclusion-1">Conclusion</h3>
<p>The Explainable Predictions design pattern offers a way to increase
transparency and trust in machine learning models. By using feature
attribution methods, stakeholders can better understand why models make
specific decisions. This not only improves confidence in model outputs
but also helps identify biases, improve performance, and ensure
regulatory compliance. However, these methods should be used in
conjunction with broader evaluation techniques to ensure a well-rounded
understanding of model behavior.</p>
<h2 id="design-pattern-30-fairness-lens">Design Pattern 30: Fairness
Lens</h2>
<p>The <strong>Fairness Lens design pattern</strong> focuses on
identifying and mitigating bias in machine learning (ML) systems through
preprocessing and postprocessing techniques. This pattern is essential
to ensure that models make fair and equitable predictions for different
user groups and scenarios, acknowledging that fairness in ML is an
evolving area with no universal solution. By evaluating the entire ML
pipeline—from data collection to model deployment—through a fairness
perspective, developers can build models that have less risk of
introducing harmful biases.</p>
<h3 id="the-problem-2">The Problem</h3>
<p>Although it might seem that ML models, as machines, should be free
from bias, the reality is that they learn from datasets that are
inherently created by humans. Since human data can reflect societal
biases, these biases often seep into ML models. Some biases are natural
and reflective of the data (e.g., rare events like financial fraud are
naturally underrepresented), but <strong>problematic bias</strong>
occurs when the data or models treat certain groups unfairly.</p>
<p>For example, consider a model trained to recognize types of shoes. If
the training data disproportionately contains images of sandals, the
model may perform well on sandals but fail on heels or sneakers, leading
to unfair results for users whose shoes don’t fit the dominant pattern.
This <strong>data distribution bias</strong> happens when the dataset
doesn’t represent all groups equally, leading to skewed model
behavior.</p>
<p>Other forms of bias include:</p>
<ul>
<li><strong>Data representation bias</strong>: This occurs when certain
groups are misrepresented in the data, such as predominantly negative
reviews for a certain type of restaurant in a sentiment analysis
model.</li>
<li><strong>Proxy bias</strong>: When sensitive characteristics (like
race or gender) are reflected in other features (such as zip code or
income), bias remains even if explicit features are removed.</li>
<li><strong>Experimenter bias</strong>: Bias introduced by data
labelers, especially in subjective tasks, like sentiment labeling.</li>
</ul>
<p>Bias can also be introduced during model training through the
objective function. For instance, optimizing for overall accuracy may
ignore the fact that a model might perform poorly on smaller, minority
groups within the data.</p>
<h3 id="the-solution-2">The Solution</h3>
<p>To address bias, the <strong>Fairness Lens</strong> pattern proposes
both <strong>pre-training</strong> and <strong>post-training</strong>
techniques to ensure fairness across data and models.</p>
<ol type="1">
<li><p><strong>Pre-training bias identification</strong>: Before
training the model, analyzing the dataset to identify problematic bias
is crucial. This involves looking at who will be affected by the model
and whether certain groups are underrepresented. Table 7-3 in the
pattern provides questions to help identify bias, such as whether data
contains balanced examples across demographic groups.</p>
<p>Tools like the <strong>What-If Tool</strong> can help visualize
dataset splits and identify potential biases. For instance, using this
tool with a US mortgage application dataset can reveal if certain loan
agencies are disproportionately represented. Addressing these biases
early can reduce their impact later.</p></li>
<li><p><strong>Post-training bias mitigation</strong>: After training,
further analysis helps identify if the model itself introduces or
amplifies bias. The <strong>What-If Tool</strong> allows users to
evaluate how a trained model performs across different slices of the
data, such as demographic groups. For example, it could show that a
model predicts loan approvals more accurately for one agency compared to
others, suggesting representation bias in the training data.</p>
<p><strong>Classification thresholds</strong> can also be adjusted to
reduce bias. For example, instead of approving loans only when the model
is 98% confident, lowering the threshold to 60% might create more
equitable outcomes between different groups, though the threshold
depends on the use case.</p></li>
</ol>
<h3 id="trade-offs-and-alternatives-2">Trade-offs and Alternatives</h3>
<p>While the <strong>What-If Tool</strong> is powerful, other
alternatives and methods exist for achieving fairness in ML models:</p>
<ul>
<li><strong>Fairness Indicators</strong>: A set of open-source tools
like TensorFlow Data Validation (TFDV) and TensorFlow Model Analysis
(TFMA) help automate fairness analysis as part of ML pipelines.</li>
<li><strong>Allow/Disallow Lists</strong>: In situations where inherent
bias can’t be fully removed, rules can be hardcoded into models to
prevent them from returning biased predictions. For example, Google’s
Vision API stopped using gendered labels like “man” or “woman” because
such labels could reinforce stereotypes.</li>
<li><strong>Data Augmentation</strong>: Techniques like
<strong>ablation</strong>, which remove identity terms (e.g., replacing
“Mint chip is the best flavor” with “___ is the best flavor”) can
prevent models from using irrelevant terms to make biased
predictions.</li>
</ul>
<h3 id="post-training-techniques">Post-Training Techniques</h3>
<p>Once a model is trained, it’s essential to analyze its fairness. The
<strong>What-If Tool</strong> can slice the data by different
demographics or features to identify patterns of bias in predictions. If
bias is identified, actions like adjusting the classification threshold
or retraining the model with different data splits can help. The
ultimate goal is to make decisions based on fairness metrics like
<strong>demographic parity</strong> or <strong>equality of
opportunity</strong>.</p>
<h3 id="limitations-and-other-considerations">Limitations and Other
Considerations</h3>
<p>While these fairness techniques help mitigate bias, there’s no
one-size-fits-all solution. Bias and fairness must be continuously
evaluated throughout the ML pipeline. The pattern emphasizes that
fairness is an ongoing process, not just a one-time check. Moreover,
while fairness tools aim to improve equity, they can only work if models
are used in the context for which they were designed.</p>
<p>Another useful tool to ensure transparency is the <strong>Model
Cards</strong> framework. Model Cards provide a standardized way to
document a model’s intended use, limitations, and fairness metrics,
encouraging accountability and transparency in deployment.</p>
<h3 id="fairness-vs.-explainability">Fairness vs. Explainability</h3>
<p>Fairness and explainability often overlap but are distinct concepts.
<strong>Fairness</strong> focuses on identifying and reducing bias in
models, while <strong>explainability</strong> helps understand why a
model made a specific prediction. Explainability tools can help detect
fairness issues, but they also have broader applications, such as
explaining why a model flagged certain transactions as fraudulent.</p>
<h3 id="conclusion-2">Conclusion</h3>
<p>The Fairness Lens design pattern is an essential component of
building responsible AI. By ensuring that models are fair and equitable
for all user groups, data scientists can create more ethical and
effective systems. This pattern emphasizes a continuous process of
fairness evaluation, requiring careful attention to data collection,
model training, and post-training analysis to mitigate bias throughout
the ML lifecycle.</p>
    
</body>
</html>