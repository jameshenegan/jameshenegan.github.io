<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h1
id="comprehensive-tutorial-using-pipelines-in-scikit-learn"><strong>Comprehensive
Tutorial: Using Pipelines in Scikit-learn</strong></h1>
<p>In this tutorial, we’ll cover:</p>
<ol type="1">
<li>Introduction to Pipelines</li>
<li>Importance of proper data preprocessing</li>
<li>Building a simple pipeline with <code>StandardScaler</code> and an
estimator</li>
<li>Avoiding data leakage in preprocessing</li>
<li>Performing cross-validation with pipelines</li>
<li>Hyperparameter tuning using <code>GridSearchCV</code> in a
pipeline</li>
</ol>
<hr />
<h3 id="introduction-to-pipelines"><strong>1. Introduction to
Pipelines</strong></h3>
<p>A pipeline in scikit-learn allows us to chain multiple steps in a
machine learning workflow. This typically includes preprocessing steps
like scaling or feature selection and the final model, making it easier
to manage the entire workflow.</p>
<p>Benefits of using a pipeline:</p>
<ul>
<li><strong>Streamlined workflow</strong>: All steps are organized and
executed sequentially.</li>
<li><strong>Prevents data leakage</strong>: Ensures that the same
transformations are applied consistently during training and
testing.</li>
<li><strong>Cross-validation and hyperparameter tuning</strong>: Makes
it easy to apply techniques like cross-validation or grid search without
risk of data leakage.</li>
</ul>
<hr />
<h3 id="importance-of-proper-data-preprocessing"><strong>2. Importance
of Proper Data Preprocessing</strong></h3>
<p>Many machine learning algorithms perform better when features are
standardized (i.e., rescaled so that they have a mean of 0 and a
standard deviation of 1). Scikit-learn’s <code>StandardScaler</code>
computes the mean and standard deviation of the features during training
and uses them to transform both the training and test data.</p>
<p>It’s crucial to ensure that the test set is transformed using
statistics (mean, std) computed <strong>only from the training
data</strong>. If the test set were to use its own statistics for
scaling, it would lead to data leakage, inflating the model’s
performance inappropriately.</p>
<hr />
<h3 id="building-a-simple-pipeline"><strong>3. Building a Simple
Pipeline</strong></h3>
<p>Let’s start by building a pipeline that includes:</p>
<ul>
<li>A <strong>scaling step</strong> using
<code>StandardScaler</code></li>
<li>A <strong>classification model</strong> using Support Vector
Classifier (SVC)</li>
</ul>
<p>We’ll use the Iris dataset, a popular dataset for classification
tasks.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Iris dataset</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test sets</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pipeline with StandardScaler and SVC</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;scaler&#39;</span>, StandardScaler()),  <span class="co"># Step 1: Standardize the features</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;svc&#39;</span>, SVC())                 <span class="co"># Step 2: SVM classifier</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the pipeline on the training data</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>pipeline.fit(X_train, y_train)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the pipeline on test data</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> pipeline.score(X_test, y_test)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Test Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 id="explanation"><strong>Explanation</strong>:</h3>
<ol type="1">
<li><strong>StandardScaler</strong> computes the mean and standard
deviation using the <strong>training data only</strong>.</li>
<li>The <strong>SVC</strong> classifier is trained on the scaled
training data.</li>
<li>When evaluating on the test data, the <strong>same</strong> mean and
standard deviation from the training data are used to scale the test
set.</li>
</ol>
<hr />
<h3 id="avoiding-data-leakage"><strong>4. Avoiding Data
Leakage</strong></h3>
<p>To avoid <strong>data leakage</strong>:</p>
<ul>
<li>The scaling step (or any preprocessing step) should only use
statistics (e.g., mean and standard deviation) computed from the
training set. This ensures the test set remains unseen during
training.</li>
<li>Scikit-learn’s <code>Pipeline</code> handles this automatically by
“fitting” the scaler only on the training data when you call
<code>fit</code>.</li>
</ul>
<h4 id="what-happens-behind-the-scenes"><strong>What happens behind the
scenes</strong>:</h4>
<ul>
<li>When you call <code>pipeline.fit(X_train, y_train)</code>, the
<code>StandardScaler</code> first computes the mean and standard
deviation using <code>X_train</code> and transforms the training
data.</li>
<li>When you later call <code>pipeline.score(X_test, y_test)</code>, the
<code>StandardScaler</code> uses the previously computed mean and
standard deviation from <code>X_train</code> to transform
<code>X_test</code>—without recomputing on <code>X_test</code>.</li>
</ul>
<hr />
<h3 id="cross-validation-with-pipelines"><strong>5. Cross-Validation
with Pipelines</strong></h3>
<p>Cross-validation is a technique that helps to evaluate model
performance by dividing the dataset into multiple folds and training the
model on different subsets of the data. When you use cross-validation
with a pipeline, the scaler is fitted inside each fold, ensuring no
leakage of test information into the training data.</p>
<p>Let’s perform cross-validation using
<code>cross_val_score</code>:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cross-validation</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(pipeline, X, y, cv<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Cross-validation scores: </span><span class="sc">{</span>cv_scores<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean CV accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(cv_scores)<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 id="explanation-1"><strong>Explanation</strong>:</h3>
<ul>
<li>During cross-validation, the data is split into 5 folds.</li>
<li>For each fold, the scaler is fitted only on the training portion and
is applied to both training and validation data.</li>
<li>This ensures that the validation data remains completely unseen
during training.</li>
</ul>
<hr />
<h3 id="hyperparameter-tuning-with-gridsearchcv"><strong>6.
Hyperparameter Tuning with <code>GridSearchCV</code></strong></h3>
<p>The pipeline can also be integrated with <code>GridSearchCV</code> to
search for the best hyperparameters of both the model and the
preprocessing steps. For example, we can tune the regularization
parameter <code>C</code> of the SVC, as well as the <code>gamma</code>
parameter for the kernel.</p>
<p>Here’s how to perform hyperparameter tuning within a pipeline:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a parameter grid for GridSearch</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;svc__C&#39;</span>: [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>],        <span class="co"># Regularization parameter for SVC</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;svc__gamma&#39;</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>],  <span class="co"># Kernel coefficient for SVC</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;svc__kernel&#39;</span>: [<span class="st">&#39;rbf&#39;</span>, <span class="st">&#39;linear&#39;</span>]  <span class="co"># Test both RBF and linear kernels</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GridSearchCV object</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(pipeline, param_grid, cv<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit GridSearchCV to the entire dataset (it will automatically perform cross-validation)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X, y)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best parameters and the best score</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best Parameters: </span><span class="sc">{</span>grid_search<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best CV Score: </span><span class="sc">{</span>grid_search<span class="sc">.</span>best_score_<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 id="explanation-2"><strong>Explanation</strong>:</h3>
<ul>
<li><strong><code>param_grid</code></strong> specifies a grid of
hyperparameters to search over.</li>
<li><strong><code>GridSearchCV</code></strong> automatically fits the
pipeline for each combination of hyperparameters using
cross-validation.</li>
<li>It tunes both the SVC model and the pipeline step
(<code>svc__C</code>, <code>svc__gamma</code>).</li>
</ul>
<p>By tuning the model within a pipeline, we ensure that hyperparameter
tuning is applied correctly to both the preprocessing steps and the
model, avoiding data leakage.</p>
<hr />
<h3 id="summary"><strong>Summary</strong></h3>
<p>In this tutorial, we explored how to use scikit-learn’s
<strong>Pipelines</strong> to streamline machine learning workflows. We
covered:</p>
<ol type="1">
<li>Building a basic pipeline with scaling and modeling.</li>
<li>Ensuring that data preprocessing (like scaling) is done correctly
without leaking information from the test set.</li>
<li>Using <strong>cross-validation</strong> to evaluate model
performance with a pipeline.</li>
<li><strong>Hyperparameter tuning</strong> with
<code>GridSearchCV</code> inside a pipeline to optimize both the model
and preprocessing steps.</li>
</ol>
<p>This approach is highly recommended for production machine learning
systems, as it ensures consistency, avoids data leakage, and simplifies
the overall workflow.</p>
    
</body>
</html>