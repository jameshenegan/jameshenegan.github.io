<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>

<style>
body {
  font-family: "Avenir Next", Helvetica, Arial, sans-serif;
  padding: 1em;
  margin: auto;
  max-width: 42em;
  background: #fefefe;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  font-weight: bold;
}

h1 {
  color: #000000;
  font-size: 28pt;
}

h2 {
  border-bottom: 1px solid #cccccc;
  color: #000000;
  font-size: 24px;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777777;
  background-color: inherit;
  font-size: 14px;
}

hr {
  height: 0.2em;
  border: 0;
  color: #cccccc;
  background-color: #cccccc;
}

p,
blockquote,
ul,
ol,
dl,
li,
table,
pre {
  margin: 15px 0;
}

img {
  max-width: 100%;
}

table {
  border-collapse: collapse;
  width: 100%;
}

table,
th,
td {
  border: 1px solid #eaeaea;

  border-radius: 3px;
  padding: 5px;
}

tr:nth-child(even) {
  background-color: #f8f8f8;
}

a,
a:visited {
  color: #4183c4;
  background-color: inherit;
  text-decoration: none;
}

#message {
  border-radius: 6px;
  border: 1px solid #ccc;
  display: block;
  width: 100%;
  height: 60px;
  margin: 6px 0px;
}

button,
#ws {
  font-size: 10pt;
  padding: 4px 6px;
  border-radius: 5px;
  border: 1px solid #bbb;
  background-color: #eee;
}

code,
pre,
#ws,
#message {
  font-family: Monaco, monospace;
  font-size: 10pt;
  border-radius: 3px;
  background-color: #f8f8f8;
  color: inherit;
}

code {
  border: 1px solid #eaeaea;
  margin: 0 2px;
  padding: 0 5px;
}

pre {
  border: 1px solid #cccccc;
  overflow: auto;
  padding: 4px 8px;
}

pre > code {
  border: 0;
  margin: 0;
  padding: 0;
}

#ws {
  background-color: #f8f8f8;
}

.send {
  color: #77bb77;
}
.server {
  color: #7799bb;
}
.error {
  color: #aa0000;
}
</style>


     </head>
  <body><h3
id="harmonic-mean-a-detailed-tutorial-with-f1-score-example">Harmonic
Mean: A Detailed Tutorial with F1 Score Example</h3>
<p>The <strong>harmonic mean</strong> is a type of average, often used
when dealing with rates or ratios. Unlike the arithmetic mean (which is
just a sum of values divided by the count), the harmonic mean tends to
give more weight to smaller values, making it more suitable when
averaging rates. It’s especially useful when we’re working with
situations where low values have a large impact.</p>
<h3 id="formula-for-the-harmonic-mean">Formula for the Harmonic
Mean</h3>
<p>The harmonic mean for a set of numbers is calculated as:</p>
<p>[ = Number of values (Sum of the reciprocals of the values) ]</p>
<p>For example, if you have two values, <code>a</code> and
<code>b</code>, the harmonic mean is:</p>
<p>[ Harmonic Mean = 2ab (a + b) ]</p>
<p>This is the form used in calculating the <strong>F1 score</strong> in
machine learning.</p>
<h3 id="when-to-use-the-harmonic-mean">When to Use the Harmonic
Mean</h3>
<p>The harmonic mean is useful when you’re dealing with rates and want
to avoid the influence of outliers or when you want to give equal
importance to all data points. It’s especially relevant in:</p>
<ul>
<li><strong>Speed or rates</strong> (such as averaging speeds over a
distance)</li>
<li><strong>Evaluation metrics</strong> in machine learning like the
<strong>F1 Score</strong>, where both precision and recall matter.</li>
</ul>
<hr />
<h2 id="f1-score-example">F1 Score Example</h2>
<h3 id="what-is-f1-score">What is F1 Score?</h3>
<p>In binary classification tasks (where we classify something into one
of two groups, like spam or not spam), the <strong>F1 score</strong>
balances two important metrics: <strong>precision</strong> and
<strong>recall</strong>. It’s particularly useful when you have
imbalanced data, where one class might be much larger or smaller than
the other.</p>
<ul>
<li><p><strong>Precision</strong> is the proportion of correctly
predicted positive results out of all predicted positives. Essentially,
it answers: <em>Of all the things we predicted to be positive, how many
were actually correct?</em></p>
<p>[ Precision = True Positives (True Positives + False Positives)
]</p></li>
<li><p><strong>Recall</strong> is the proportion of actual positive
results that were correctly identified. It answers: <em>Out of all the
actual positive cases, how many did we predict correctly?</em></p>
<p>[ Recall = True Positives (True Positives + False Negatives)
]</p></li>
</ul>
<p>The <strong>F1 Score</strong> combines these two measures into a
single metric by taking the harmonic mean of precision and recall:</p>
<p>[ F1 Score = 2 (Precision Recall) (Precision + Recall) ]</p>
<h3 id="why-use-harmonic-mean-for-f1-score">Why Use Harmonic Mean for F1
Score?</h3>
<p>The harmonic mean is a better choice than the arithmetic mean for
combining precision and recall because it gives more weight to lower
values. If either precision or recall is very low, the F1 score will
also be low. This makes the F1 score more reflective of the model’s
performance when there is an imbalance between precision and recall.</p>
<p>For example, if you have high precision but very low recall, the F1
score will not allow the high precision to overly inflate the overall
score—it will still be low, emphasizing the importance of both precision
and recall.</p>
<hr />
<h3 id="f1-score-calculation-example">F1 Score Calculation Example</h3>
<p>Let’s take an example of a confusion matrix from a binary
classification model:</p>
<table>
<thead>
<tr>
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actual Positive</strong></td>
<td>40</td>
<td>10</td>
</tr>
<tr>
<td><strong>Actual Negative</strong></td>
<td>5</td>
<td>45</td>
</tr>
</tbody>
</table>
<p>From this confusion matrix:</p>
<ul>
<li><strong>True Positives (TP)</strong> = 40</li>
<li><strong>False Positives (FP)</strong> = 5</li>
<li><strong>False Negatives (FN)</strong> = 10</li>
<li><strong>True Negatives (TN)</strong> = 45</li>
</ul>
<h4 id="step-1-calculate-precision">Step 1: Calculate Precision</h4>
<p>Precision is the number of true positives divided by the total number
of predicted positives:</p>
<p>[ Precision = 40 (40 + 5) = 40 = 0.8889 ]</p>
<h4 id="step-2-calculate-recall">Step 2: Calculate Recall</h4>
<p>Recall is the number of true positives divided by the total number of
actual positives:</p>
<p>[ Recall = 40 (40 + 10) = 40 = 0.8 ]</p>
<h4 id="step-3-calculate-f1-score">Step 3: Calculate F1 Score</h4>
<p>Now, to find the F1 score, we take the harmonic mean of precision and
recall:</p>
<p>[ F1 Score = 2 (0.8889 ) (0.8889 + 0.8) ]</p>
<p>This gives us:</p>
<p>[ F1 Score = 2 = 0.841 ]</p>
<p>So, the F1 Score is approximately <strong>0.841</strong>.</p>
<hr />
<h3 id="why-the-harmonic-mean-is-used-in-the-f1-score">Why the Harmonic
Mean is Used in the F1 Score</h3>
<p>The harmonic mean ensures that both precision and recall contribute
equally to the F1 score. If either precision or recall is low, the F1
score will reflect that by being closer to the lower value. This
prevents situations where a model with very high precision but poor
recall (or vice versa) looks like it’s performing well based on just one
metric.</p>
<p>For example, consider a case where precision is 0.99 (very high), but
recall is only 0.1 (very low). If we took the arithmetic mean, we would
get a misleading score of 0.545. However, the harmonic mean would give a
much lower and more appropriate score closer to the weaker metric,
emphasizing the balance between precision and recall.</p>
<hr />
<h3 id="summary">Summary</h3>
<ul>
<li>The <strong>harmonic mean</strong> is useful for averaging rates or
ratios, especially when small values need to be given more
importance.</li>
<li>It’s calculated as the number of values divided by the sum of their
reciprocals.</li>
<li>The <strong>F1 score</strong> in machine learning is the harmonic
mean of <strong>precision</strong> and <strong>recall</strong>.</li>
<li>It is a valuable metric when you want to balance precision and
recall, especially in cases where there is an imbalance between the
two.</li>
</ul>
<p>The harmonic mean ensures that a model that performs poorly on either
precision or recall will not receive an artificially high score, making
it a robust measure for evaluating classification models.</p>
    
</body>
</html>